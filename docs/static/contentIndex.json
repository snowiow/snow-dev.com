{"Impressum":{"slug":"Impressum","filePath":"Impressum.md","title":"Impressum","links":[],"tags":["explorerexclude"],"content":"Angaben gemäß § 5 TMG\nMarcel Patzwahl\nKontakt:\nTelefon: 0170-2809354\nE-Mail: marcel (dot) patzwahl (at) posteo (dot) de"},"index":{"slug":"index","filePath":"index.md","title":"snow garden","links":["tags/AWS","tags/emacs","tags/obisidian","tags/linux","posts","blog.snow-dev.com"],"tags":[],"content":"\nHi, my name is Marcel and I’m a Platform Engineer. My current interests are FinOps, AWS, Emacs, Obsidian and all things Linux. Probably you will find something about these topics here.\nI want to experiment with a digital garden instead of a classic blog, because I began to use Obsidian for my private notes and I really like how different notes build interconnections over time. So I was looking for something similar on my public space. I ended up using  Quartz. I will probably also write in more detail how I exactly set it up at some point and will link it here.\nThis page will contain notes about things, little bits and longer posts. You can also find the posts from my old blog here as well."},"posts/AWS/ECS-CD-with-AWS-CodePipeline-in-Terraform":{"slug":"posts/AWS/ECS-CD-with-AWS-CodePipeline-in-Terraform","filePath":"posts/AWS/ECS CD with AWS CodePipeline in Terraform.md","title":"ECS CD with AWS CodePipeline in Terraform","links":["posts/AWS/VPC-Peering-with-MongoDB-Atlas-and-AWS-in-Terraform"],"tags":["terraform","AWS"],"content":"Last week I came along a problem regarding the deployment of an ECS Service.\nI wanted to use the newly\nannounced Blue/Green-Deployment\npowered by CodeDeploy, because for the time being I only needed one Fargate\ninstance to run. The classic ECS Deployment destroyed one instance and started\na new one in it’s place. With one instance this would mean, we would have some\ndowntime during the start of the new container.\nWith Blue/Green Deployment a completely new group of instances will be started.\nIf everything went fine, the load balancer forwards the traffic to the new\ngroup and the old gets shut down afterwards. Even with only one running\ninstance, a second gets started before the first will be destroyed and if the\nsecond is working correctly, the traffic switches over and the old will be\ndestroyed afterwards. The traffic switch is a minimal downtime, which isn’t\nrecognizable in normal operation.\nIn this post we will create a complete Continuous Delivery Pipeline with AWS\nCodePipeline, which consists of three stages. In the first stage (called\nsource) the pipeline listens to master commits on the GitHub Repository of\ninterest. The second stage (called build) builds a new docker image of the\nDockerfile in the GitHub Repository and pushes the new image to AWS ECR. The\nlast state (called deploy) does a blue/green deployment to ECS with the new\nimage.\nThe docker image is a simple HTML website with a message so we can see that the\ncorrect container is deployed.\nThe complete codebase can be found in this GitHub\nRepository.\nWe start by creating the infrastructure for the ECS Service itself.\n\nAs you can see we will build up a VPC with three subnets where the ECS\nServices can be spawned in. The ECS Services have access to the ECR to pull\nimages. In front of our network is the load balancer, which redirects the\ntraffic to the services.\nBefore we go into the infrastructure, we create a main.tf and create some\nvariables, we need later and create the AWS provider.\nvariable &quot;github_token&quot; {\n  description = &quot;The GitHub Token to be used for the CodePipeline&quot;\n  type        = &quot;string&quot;\n}\n \nvariable &quot;account_id&quot; {\n  description = &quot;id of the active account&quot;\n  type        = &quot;string&quot;\n}\n \nvariable &quot;region&quot; {\n  description = &quot;region to deploy to&quot;\n  type        = &quot;string&quot;\n}\n \nprovider &quot;aws&quot; {\n  region  = &quot;${var.region}&quot;\n  version = &quot;2.7&quot;\n}\nPart 1: The VPC\nWe start with what is the basis of every AWS tutorial: The VPC.\nlocals {\n  subnets = {\n    &quot;${var.region}a&quot; = &quot;172.16.0.0/21&quot;\n    &quot;${var.region}b&quot; = &quot;172.16.8.0/21&quot;\n    &quot;${var.region}c&quot; = &quot;172.16.16.0/21&quot;\n  }\n}\n \nresource &quot;aws_vpc&quot; &quot;this&quot; {\n  cidr_block = &quot;172.16.0.0/16&quot;\n \n  enable_dns_support   = true\n  enable_dns_hostnames = true\n \n  tags = {\n    Name = &quot;example-vpc&quot;\n  }\n}\n \nresource &quot;aws_internet_gateway&quot; &quot;this&quot; {\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  tags = {\n    Name = &quot;example-internet-gateway&quot;\n  }\n}\n \nresource &quot;aws_subnet&quot; &quot;this&quot; {\n  count      = &quot;${length(local.subnets)}&quot;\n  cidr_block = &quot;${element(values(local.subnets), count.index)}&quot;\n  vpc_id     = &quot;${aws_vpc.this.id}&quot;\n \n  map_public_ip_on_launch = true\n  availability_zone       = &quot;${element(keys(local.subnets), count.index)}&quot;\n \n  tags = {\n    Name = &quot;${element(keys(local.subnets), count.index)}&quot;\n  }\n}\n \nresource &quot;aws_route_table&quot; &quot;this&quot; {\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  tags = {\n    Name = &quot;example-route-table-public&quot;\n  }\n}\n \nresource &quot;aws_route&quot; &quot;this&quot; {\n  route_table_id         = &quot;${aws_route_table.this.id}&quot;\n  destination_cidr_block = &quot;0.0.0.0/0&quot;\n  gateway_id             = &quot;${aws_internet_gateway.this.id}&quot;\n}\n \nresource &quot;aws_route_table_association&quot; &quot;this&quot; {\n  count          = &quot;${length(local.subnets)}&quot;\n  route_table_id = &quot;${aws_route_table.this.id}&quot;\n  subnet_id      = &quot;${element(aws_subnet.this.*.id, count.index)}&quot;\n}\nHere we create a VPC with three subnets in the three AZs of the defined region.\nThe region is a variable which is set in a terraform.tfvars file (at least for\nme.  You can choose other sources for input variables as\nwell). We\ncreate a routing table with route associations to the three subnets and a route\nto an internet gateway. I won’t go into detail about the VPC. For more infos\nyou can read VPC Peering with MongoDB Atlas and AWS in Terraform in which\nI talk a bit more about VPCs. The difference this time is, that there are three\nsubnets, which split up the whole CIDR space of the VPC. We are making three\nsubnets, because we want to spawn instances randomly in different availability\nzones. If we increase the count of the running tasks, those will be split\nevenly over the different AZs. In the rare case of unavailability of one AZ,\nthe app would still be available, because it’s also running in the other AZs.\nPart 2: The Load Balancer\nNow we need a load balancer, which is the key part of our Green/Blue\nDeployment. The Load Balancer has two target groups. We need the two target\ngroups for the green/blue deployment later. That’s why we call one target group\ngreen and the other blue. The blue group will stay empty for the time being and\nwe spawn everything in the green one.\n\ndata &quot;aws_subnet_ids&quot; &quot;this&quot; {\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n}\n \nresource &quot;aws_security_group&quot; &quot;this&quot; {\n  name   = &quot;allow-http&quot;\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  ingress {\n    from_port   = 80\n    protocol    = &quot;tcp&quot;\n    to_port     = 80\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n \n  egress {\n    from_port   = 0\n    protocol    = &quot;-1&quot;\n    to_port     = 0\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n \n  tags {\n    Name = &quot;allow-http-sg&quot;\n  }\n}\n \nresource &quot;aws_lb&quot; &quot;this&quot; {\n  name               = &quot;example-lb&quot;\n  internal           = false\n  load_balancer_type = &quot;application&quot;\n  security_groups    = [&quot;${aws_security_group.this.id}&quot;]\n  subnets            = [&quot;${data.aws_subnet_ids.this.ids}&quot;]\n \n  tags {\n    Name = &quot;example&quot;\n  }\n}\n \nlocals {\n  target_groups = [\n    &quot;green&quot;,\n    &quot;blue&quot;,\n  ]\n}\n \nresource &quot;aws_lb_target_group&quot; &quot;this&quot; {\n  count = &quot;${length(local.target_groups)}&quot;\n \n  name = &quot;example-tg-${element(local.target_groups, count.index)}&quot;\n \n  port        = 80\n  protocol    = &quot;HTTP&quot;\n  vpc_id      = &quot;${aws_vpc.this.id}&quot;\n  target_type = &quot;ip&quot;\n \n  health_check {\n    path = &quot;/&quot;\n    port = 80\n  }\n}\n \nresource &quot;aws_lb_listener&quot; &quot;this&quot; {\n  load_balancer_arn = &quot;${aws_lb.this.arn}&quot;\n  port              = &quot;80&quot;\n  protocol          = &quot;HTTP&quot;\n \n  default_action {\n    type             = &quot;forward&quot;\n    target_group_arn = &quot;${aws_lb_target_group.this.*.arn[0]}&quot;\n  }\n}\n \nresource &quot;aws_lb_listener_rule&quot; &quot;this&quot; {\n  listener_arn = &quot;${aws_lb_listener.this.arn}&quot;\n \n  &quot;action&quot; {\n    type             = &quot;forward&quot;\n    target_group_arn = &quot;${aws_lb_target_group.this.*.arn[0]}&quot;\n  }\n \n  &quot;condition&quot; {\n    field  = &quot;path-pattern&quot;\n    values = [&quot;/*&quot;]\n  }\n}\nBeside of the before mentioned resources, we need a listener, which will listen\non port 80 for this example and a security group. The security group allows\nincoming traffic via port 80 to the load balancer.\nOne more thing to point out is the health check on the target groups. Those are\nimportant for the blue/green deployment later, because only if the health check\nsucceeds, the group switch will be made. By default the Load Balancer checks\nthe health of the targets every 30 seconds. After 3 consecutive successful\nchecks, the target is healthy and real traffic is forwarded to the target. If a\nservice fails to answer with a 200 status code within 6 seconds three\nconsecutive times, then the target is marked unhealthy and real traffic isn’t\nforwarded to this target.\nPart 3: The ECS Service\nNow we create the ECS Service. AWS has created some concepts on top of the\ndocker container itself. The smallest unit on top of the container in AWS is\nthe task definition. In the task definition, we define what information is\npassed to one or more containers when they are being run. Basically everything\nthat can be passed as an argument to docker run and some more options can be\nset in the task definition. For example port mappings or environment variables\netc..\nTask definitions are normally written in JSON, but there is a module by\ncloudposse\nwhich allows you to write the task definition in terraform. This works similar\nto the\naws_iam_policy_document.\nThe advantage of writing those definitions in terraform is to get some more\nvalidation, before the resources get applied. Dump mistakes like missing\nmandatory attributes can be catched early this way.\nHere is the basic overview what we are trying to achieve in this part:\n\nTo use the cloudposses module and create a container definition document we\nneed to import the module:\nlocals {\n  container_name = &quot;green-blue-ecs-example&quot;\n}\n \ndata &quot;aws_ecr_repository&quot; &quot;this&quot; {\n  name = &quot;snowiow/${local.container_name}&quot;\n}\n \nresource &quot;aws_cloudwatch_log_group&quot; &quot;this&quot; {\n  name = &quot;example-app&quot;\n}\n \nmodule &quot;container_definition&quot; {\n  source  = &quot;cloudposse/ecs-container-definition/aws&quot;\n  version = &quot;0.13.0&quot;\n \n  container_name  = &quot;${local.container_name}&quot;\n  container_image = &quot;${data.aws_ecr_repository.this.repository_url}:latest&quot;\n \n  port_mappings = [\n    {\n      containerPort = 80\n    },\n  ]\n \n  log_options = {\n    awslogs-region        = &quot;${var.region}&quot;\n    awslogs-group         = &quot;example-app&quot;\n    awslogs-stream-prefix = &quot;ecs-service&quot;\n  }\n}\nWe also define the ECR repository as a data source where will pull the images\nfrom. The URL of the latest ECR image will be referenced as the\ncontainer_image in the container definition. We also create a cloudwatch log\ngroup in which the ECS Service can write it’s logs. Therefore we also define\nthe according log options in the container definition.\nBecause we are just hosting a simple HTML website, there isn’t much more going\non in the container definition.\nNow we need two roles. One which is used by the task, which is able to run the\ntask definition and the actual rights of the container during runtime. Those\nare the so called task role and execution role.\nresource &quot;aws_ecs_cluster&quot; &quot;this&quot; {\n  name = &quot;example-cluster&quot;\n}\n \ndata &quot;aws_iam_policy_document&quot; &quot;assume_by_ecs&quot; {\n  statement {\n    sid     = &quot;AllowAssumeByEcsTasks&quot;\n    effect  = &quot;Allow&quot;\n    actions = [&quot;sts:AssumeRole&quot;]\n \n    principals {\n      type        = &quot;Service&quot;\n      identifiers = [&quot;ecs-tasks.amazonaws.com&quot;]\n    }\n  }\n}\n \ndata &quot;aws_iam_policy_document&quot; &quot;execution_role&quot; {\n  statement {\n    sid    = &quot;AllowECRPull&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;ecr:GetDownloadUrlForLayer&quot;,\n      &quot;ecr:BatchGetImage&quot;,\n      &quot;ecr:BatchCheckLayerAvailability&quot;,\n    ]\n \n    resources = [&quot;${data.aws_ecr_repository.this.arn}&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowECRAuth&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [&quot;ecr:GetAuthorizationToken&quot;]\n \n    resources = [&quot;*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowLogging&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;logs:CreateLogStream&quot;,\n      &quot;logs:PutLogEvents&quot;,\n    ]\n \n    resources = [&quot;*&quot;]\n  }\n}\n \ndata &quot;aws_iam_policy_document&quot; &quot;task_role&quot; {\n  statement {\n    sid    = &quot;AllowDescribeCluster&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [&quot;ecs:DescribeClusters&quot;]\n \n    resources = [&quot;${aws_ecs_cluster.this.arn}&quot;]\n  }\n}\n \nresource &quot;aws_iam_role&quot; &quot;execution_role&quot; {\n  name               = &quot;ecs-example-execution-role&quot;\n  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_ecs.json}&quot;\n}\n \nresource &quot;aws_iam_role_policy&quot; &quot;execution_role&quot; {\n  role   = &quot;${aws_iam_role.execution_role.name}&quot;\n  policy = &quot;${data.aws_iam_policy_document.execution_role.json}&quot;\n}\n \nresource &quot;aws_iam_role&quot; &quot;task_role&quot; {\n  name               = &quot;ecs-example-task-role&quot;\n  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_ecs.json}&quot;\n}\n \nresource &quot;aws_iam_role_policy&quot; &quot;task_role&quot; {\n  role   = &quot;${aws_iam_role.task_role.name}&quot;\n  policy = &quot;${data.aws_iam_policy_document.task_role.json}&quot;\n}\nFirst of all we create the ECS Cluster, because we want to reference it as the\nonly resource for the task role policy document later.\nAfterwards we need an assume role policy document, which can be assumed by ECS\nTasks. This document will be attached to both the task role and execution role.\nSo both can be assumed by our task, which will be created next.\nNow we define the permissions of the execution role and task role. It is\nimportant that we are able to download images from the ECR during the task\nexecution, as well as writing logs, so we pass the needed permissions to the\ndocument and restrict the ECR download actions to the ECR Repository of the\nexample app. The task role needs permissions to describe the cluster it can be\nrun in.\nLastly we create the execution role and task role and append the policy\ndocuments to those roles.\nNow we have everything in place to create the task definition and service:\nresource &quot;aws_ecs_task_definition&quot; &quot;this&quot; {\n  family                   = &quot;green-blue-ecs-example&quot;\n  container_definitions    = &quot;${module.container_definition.json}&quot;\n  execution_role_arn       = &quot;${aws_iam_role.execution_role.arn}&quot;\n  task_role_arn            = &quot;${aws_iam_role.task_role.arn}&quot;\n  network_mode             = &quot;awsvpc&quot;\n  cpu                      = &quot;0.25 vcpu&quot;\n  memory                   = &quot;0.5 gb&quot;\n  requires_compatibilities = [&quot;FARGATE&quot;]\n}\n \nresource &quot;aws_security_group&quot; &quot;ecs&quot; {\n  name   = &quot;allow-ecs-traffic&quot;\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  ingress {\n    from_port   = 80\n    protocol    = &quot;tcp&quot;\n    to_port     = 443\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n \n  egress {\n    from_port   = 0\n    protocol    = &quot;-1&quot;\n    to_port     = 0\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n}\n \nresource &quot;aws_ecs_service&quot; &quot;this&quot; {\n  name            = &quot;example-service&quot;\n  task_definition = &quot;${aws_ecs_task_definition.this.id}&quot;\n  cluster         = &quot;${aws_ecs_cluster.this.arn}&quot;\n \n  load_balancer {\n    target_group_arn = &quot;${aws_lb_target_group.this.0.arn}&quot;\n    container_name   = &quot;${local.container_name}&quot;\n    container_port   = 80\n  }\n \n  launch_type   = &quot;FARGATE&quot;\n  desired_count = 3\n \n  network_configuration {\n    subnets         = [&quot;${aws_subnet.this.*.id}&quot;]\n    security_groups = [&quot;${aws_security_group.ecs.id}&quot;]\n \n    assign_public_ip = true\n  }\n \n  depends_on = [&quot;aws_lb_listener.this&quot;]\n}\nFor the task definition we don’t set anything special, except that we force the\nawsvpc network mode to spawn it in our earlier created VPC. In the service we\ndefine a load balancer block. This way any task started by this service will be\nspawned in the target group of our load balancer. Here we start to fill the\ngreen group initially. We also have to set the subnets, where the service\nspawns the tasks in. The service has a desired task count of three. AWS\nbalances these three tasks in all of our subnets evenly, so we will have one\ntask running in every subnet.\nAs the security group we use one with allowed ingress from port 80 to 443. The\nnormal traffic from the load balancer comes from port 80, but we also need to\nbe able to download the docker image from the ECR, which will be done via\nHTTPS. We also need to assign a public IP to the task to be able to download\nthe image.\nIt is important to note, that we need to explicitly name a dependency to the\naws_lb_listener. Because we apply anything simultaneously, the service would\notherwise just wait for the target group to be finished. But then the creation\nof the ECS Service would give us an error, because the created target group\nisn’t linked to a load balancer. That’s why we need to explicitly wait for the\nlistener to be created before we start to create the ECS Service.\nAt this point we have a fully working infrastructure to host our dockerized\napplication, which can be scaled by modifying the cpu and memory attributes\nand a new terraform apply. We can also add an Autoscaling Policy to scale our\ntask count automatically when the traffic increases.\nPart 4: The Deployment\nNow that the application is up and running, we need some sort of automatic\nbuilding and deploying. Therefore we use the AWS Codepipeline, which will\nconsist of three steps:\n\nSource: Trigger the pipeline through a master commit in the GitHub\nrepository of the application\nBuild: Build a new container and push it to the ECR\nDeploy: Do a Blue/Green Deployment in our ECS Service with the latest\ncontainer version\n\nWe start by creating the pipeline with it’s first stage. Therefore we need a new\nIAM Role for the pipeline and a S3 Bucket, where the interim results of the\npipeline are saved and downloaded by the next step of the pipeline. The S3\nBucket is very straight forward:\nresource &quot;aws_s3_bucket&quot; &quot;codepipeline&quot; {\n  bucket = &quot;example-app-codepipeline&quot;\n}\nand the iam role:\ndata &quot;aws_iam_policy_document&quot; &quot;assume_by_pipeline&quot; {\n  statement {\n    sid     = &quot;AllowAssumeByPipeline&quot;\n    effect  = &quot;Allow&quot;\n    actions = [&quot;sts:AssumeRole&quot;]\n \n    principals {\n      type        = &quot;Service&quot;\n      identifiers = [&quot;codepipeline.amazonaws.com&quot;]\n    }\n  }\n}\n \nresource &quot;aws_iam_role&quot; &quot;pipeline&quot; {\n  name               = &quot;pipeline-example-role&quot;\n  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_pipeline.json}&quot;\n}\n \ndata &quot;aws_iam_policy_document&quot; &quot;pipeline&quot; {\n  statement {\n    sid    = &quot;AllowS3&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;s3:GetObject&quot;,\n      &quot;s3:ListBucket&quot;,\n      &quot;s3:PutObject&quot;,\n    ]\n \n    resources = [\n      &quot;${aws_s3_bucket.this.arn}&quot;,\n      &quot;${aws_s3_bucket.this.arn}/*&quot;,\n    ]\n  }\n}\n \nresource &quot;aws_iam_role_policy&quot; &quot;pipeline&quot; {\n  role   = &quot;${aws_iam_role.pipeline.name}&quot;\n  policy = &quot;${data.aws_iam_policy_document.pipeline.json}&quot;\n}\nFor the moment we only give the pipeline the permissions to list, download and\nupload to the bucket, we created before.\nNow we can create the pipeline.\nresource &quot;aws_codepipeline&quot; &quot;this&quot; {\n  name     = &quot;example-pipeline&quot;\n  role_arn = &quot;${aws_iam_role.pipeline.arn}&quot;\n \n  artifact_store {\n    location = &quot;${aws_s3_bucket.this.bucket}&quot;\n    type     = &quot;S3&quot;\n  }\n \n  stage {\n    name = &quot;Source&quot;\n \n    action {\n      name             = &quot;Source&quot;\n      category         = &quot;Source&quot;\n      owner            = &quot;ThirdParty&quot;\n      provider         = &quot;GitHub&quot;\n      version          = &quot;1&quot;\n      output_artifacts = [&quot;source&quot;]\n \n      configuration = {\n        OAuthToken = &quot;${var.github_token}&quot;\n        Owner      = &quot;snowiow&quot;\n        Repo       = &quot;green-blue-ecs-example&quot;\n        Branch     = &quot;master&quot;\n      }\n    }\n  }\n}\nWe define GitHub as the source and my repository’s master branch as the start\ntrigger. You will need an OAuthToken as well. This way the pipeline will be\nnotified when a new commit happened. The other way would be to let the pipeline\npull information from GitHub every now and then. But with this option you will\nalways have some time between the commit and the start of the pipeline. To\ngenerate a GitHub Token with the right scopes, you can follow this guide from\nAWS.\nIf you copied the token you can use it for example as a variable in terraform.\nAnother way is to set the GITHUB_TOKEN environment variable. This way you can\nleave the OAuthToken attribute unset.\nNow we will add the build stage. First of all, the CodeBuild needs it’s own set\nof permissions, so we do the usual stuff to create an IAM role for the\nCodeBuild:\ndata &quot;aws_iam_policy_document&quot; &quot;assume_by_codebuild&quot; {\n  statement {\n    sid     = &quot;AllowAssumeByCodebuild&quot;\n    effect  = &quot;Allow&quot;\n    actions = [&quot;sts:AssumeRole&quot;]\n \n    principals {\n      type        = &quot;Service&quot;\n      identifiers = [&quot;codebuild.amazonaws.com&quot;]\n    }\n  }\n}\n \nresource &quot;aws_iam_role&quot; &quot;codebuild&quot; {\n  name               = &quot;codebuild-example-role&quot;\n  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_codebuild.json}&quot;\n}\n \ndata &quot;aws_iam_policy_document&quot; &quot;codebuild&quot; {\n  statement {\n    sid    = &quot;AllowS3&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;s3:GetObject&quot;,\n      &quot;s3:ListBucket&quot;,\n      &quot;s3:PutObject&quot;,\n    ]\n \n    resources = [\n      &quot;${aws_s3_bucket.this.arn}&quot;,\n      &quot;${aws_s3_bucket.this.arn}/*&quot;,\n    ]\n  }\n \n  statement {\n    sid    = &quot;AllowECRAuth&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [&quot;ecr:GetAuthorizationToken&quot;]\n \n    resources = [&quot;*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowECRUpload&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;ecr:InitiateLayerUpload&quot;,\n      &quot;ecr:UploadLayerPart&quot;,\n      &quot;ecr:CompleteLayerUpload&quot;,\n      &quot;ecr:BatchCheckLayerAvailability&quot;,\n      &quot;ecr:PutImage&quot;,\n    ]\n \n    resources = [&quot;${data.aws_ecr_repository.this.arn}&quot;]\n  }\n \n  statement {\n    sid       = &quot;AllowECSDescribeTaskDefinition&quot;\n    effect    = &quot;Allow&quot;\n    actions   = [&quot;ecs:DescribeTaskDefinition&quot;]\n    resources = [&quot;*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowLogging&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;logs:CreateLogGroup&quot;,\n      &quot;logs:CreateLogStream&quot;,\n      &quot;logs:PutLogEvents&quot;,\n    ]\n \n    resources = [&quot;*&quot;]\n  }\n}\n \nresource &quot;aws_iam_role_policy&quot; &quot;codebuild&quot; {\n  role   = &quot;${aws_iam_role.codebuild.name}&quot;\n  policy = &quot;${data.aws_iam_policy_document.codebuild.json}&quot;\n}\nBecause our CodeBuild should be able to upload Images to the ECR, we give the\naccording permissions here. The CodeBuild also needs permissions to access the\nS3 Bucket, to download the artifact from the Source (GitHub). Otherwise the\nCodebuild wouldn’t be able to access the downloaded source code of GitHub and\ntherefeore couldn’t create the Docker Image.\nNow we are able to create the Codebuild project like this:\nresource &quot;aws_codebuild_project&quot; &quot;this&quot; {\n  name         = &quot;example-codebuild&quot;\n  description  = &quot;Codebuild for the ECS Green/Blue Example app&quot;\n  service_role = &quot;${aws_iam_role.codebuild.arn}&quot;\n \n  artifacts {\n    type = &quot;CODEPIPELINE&quot;\n  }\n \n  environment {\n    compute_type    = &quot;BUILD_GENERAL1_SMALL&quot;\n    image           = &quot;aws/codebuild/docker:18.09.0&quot;\n    type            = &quot;LINUX_CONTAINER&quot;\n    privileged_mode = true\n \n    environment_variable {\n      name = &quot;REPOSITORY_URI&quot;\n      value = &quot;${data.aws_ecr_repository.this.repository_url}&quot;\n    }\n  }\n  source {\n    type = &quot;CODEPIPELINE&quot;\n  }\n}\nNothing special here. We append the IAM role, which we created earlier to the\nCodeBuild Project. The Environment settings are the information for which kind\nof machine we will execute the build. As the source we define CODEPIPELINE.\nBecause the artifacts from the source stage is already provided there. That’s\nwhy we need to set CODEPIPELINE as the artifacts type as well.\nThe Codebuild will be appended to the CodePipeline as an additional stage:\nstage {\n  name = &quot;Build&quot;\n \n  action {\n    name             = &quot;Build&quot;\n    category         = &quot;Build&quot;\n    owner            = &quot;AWS&quot;\n    provider         = &quot;CodeBuild&quot;\n    version          = &quot;1&quot;\n    input_artifacts  = [&quot;source&quot;]\n    output_artifacts = [&quot;build&quot;]\n \n    configuration {\n      ProjectName = &quot;{aws_codebuild_project.this.name}&quot;\n    }\n  }\n}\nHere you can see how we connect the artifact inputs and outputs of the\ndifferent stages. In the source stage we created an output_artifact, which is\nused as an input_artifact in the build stage. The artifact names are\nsubdirectories of the S3 Bucket we defined earlier. In these directories the\nartifact is saved as a zip file with a specific hash identifying the build, it\nbelongs to.\nBecause the CodeBuild will be triggered from the CodePipeline, we need to give\nit the corresponding permissions. Therefore we add the following statement to\nthe CodePipeline Permissions:\nstatement {\n  sid    = &quot;AllowCodeBuild&quot;\n  effect = &quot;Allow&quot;\n \n  actions = [\n    &quot;codebuild:BatchGetBuilds&quot;,\n    &quot;codebuild:StartBuild&quot;,\n  ]\n \n  resources = [&quot;${aws_codebuild_project.this.arn}&quot;]\n}\nNow we have everything in place to run builds. But we haven’t yet defined what\nthe build should actually do. CodeBuild works similar to other CI/CD Services\nlike Travis and CircleCI. We need to place a file in the root directory, which\ndescribes the build steps. In CodeBuild this file is called buildspec.yml an\nis written in YAML. My buildspec.yml looks like this:\nversion: 0.2\n \nphases:\n  pre_build:\n    commands:\n      - echo Logging in to Amazon ECR...\n      - $(aws ecr get-login --region eu-central-1 --no-include-email)\n      - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)\n  build:\n    commands:\n      - echo Build started on `date`\n      - echo Building the Docker image...\n      - docker build -t $REPOSITORY_URI:latest .\n      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG\n  post_build:\n    commands:\n      - echo Build completed on `date`\n      - echo Pushing the Docker images...\n      - docker push $REPOSITORY_URI:latest\n      - docker push $REPOSITORY_URI:$IMAGE_TAG\nThe build consists of three phases. In the pre_build the build container is\nlogging into the ECR and I set some environment variables, which I use during\nthe build. The build phase builds the docker image from the Dockerfile of the\nroot directory. In the post_build the image is pushed to the ECR.\nThis file will be placed in the project root. Because the source stage of the\npipeline downloads the whole repository from github and set it as an input\nartifact for the build, CodeBuild looks automatically for a buildspec.yml\nthere.\nNow we conclude with the CodeDeploy. Like the buildspec.yml for the\nCodeBuild, there is a appspec.yaml for CodeDeploy (Yes, here the suffix is\nyaml, not yml by default). This file will be placed in the directory root as\nwell and holds some environment variables, which will be set during the build:\nversion: 0.0\nResources:\n  - TargetService:\n      Type: AWS::ECS::Service\n      Properties:\n        TaskDefinition: &quot;$TASK_DEFINITION&quot;\n        LoadBalancerInfo:\n          ContainerName: &quot;$CONTAINER_NAME&quot;\n          ContainerPort: &quot;80&quot;\n        PlatformVersion: &quot;LATEST&quot;\n        NetworkConfiguration:\n          AwsvpcConfiguration:\n            Subnets: [&quot;$SUBNET_1&quot;,&quot;$SUBNET_2&quot;,&quot;$SUBNET_3&quot;]\n            SecurityGroups: [&quot;$SECURITY_GROUP&quot;]\n            AssignPublicIp: &quot;ENABLED&quot;\nThis file is saved as appspec_template.yaml in the directory root. In the\nbuildspec.yml we add an install phase at the beginning, which installs jq:\ninstall:\n  commands:\n    - apt-get update\n    - apt install jq\nWe need jq to get the current task definition and pass it to codedeploy. This\nis the second file which is needed by CodeDeploy next to the appspec.yml.\nIn the post build section we save the most current task definition as a file,\nwhich will be passed as an artifact to CodeDeploy:\n- aws ecs describe-task-definition --task-definition green-blue-ecs-example | \\\n  jq &#039;.taskDefinition&#039; &gt; taskdef.json\nWith jq we extract the actual task definition and write it to a file.\nNow we substitute every environment variable in the appspec_template.yaml with\nenvsubst and write it to the actual appspec.yml:\n- envsubst &lt; appspec_template.yaml &gt; appspec.yaml\nTo actually make the substitution work, we need to set the needed environment\nvariables. We set them in terraform in the aws_codebuild_project\nresource. So the final resource looks like this:\nresource &quot;aws_codebuild_project&quot; &quot;this&quot; {\n  name         = &quot;example-codebuild&quot;\n  description  = &quot;Codebuild for the ECS Green/Blue Example app&quot;\n  service_role = &quot;${aws_iam_role.codebuild.arn}&quot;\n \n  artifacts {\n    type = &quot;CODEPIPELINE&quot;\n  }\n \n  environment {\n    compute_type    = &quot;BUILD_GENERAL1_SMALL&quot;\n    image           = &quot;aws/codebuild/docker:18.09.0&quot;\n    type            = &quot;LINUX_CONTAINER&quot;\n    privileged_mode = true\n \n    environment_variable {\n      name  = &quot;REPOSITORY_URI&quot;\n      value = &quot;${data.aws_ecr_repository.this.repository_url}&quot;\n    }\n \n    environment_variable {\n      name  = &quot;TASK_DEFINITION&quot;\n      value = &quot;arn:aws:ecs:${var.region}:${var.account_id}:task-definition/${aws_ecs_task_definition.this.family}&quot;\n    }\n \n    environment_variable {\n      name  = &quot;CONTAINER_NAME&quot;\n      value = &quot;${local.container_name}&quot;\n    }\n \n    environment_variable {\n      name  = &quot;SUBNET_1&quot;\n      value = &quot;${aws_subnet.this.*.id[0]}&quot;\n    }\n \n    environment_variable {\n      name  = &quot;SUBNET_2&quot;\n      value = &quot;${aws_subnet.this.*.id[1]}&quot;\n    }\n \n    environment_variable {\n      name  = &quot;SUBNET_3&quot;\n      value = &quot;${aws_subnet.this.*.id[2]}&quot;\n    }\n \n    environment_variable {\n      name  = &quot;SECURITY_GROUP&quot;\n      value = &quot;${aws_security_group.ecs.id}&quot;\n    }\n  }\n \n  source {\n    type = &quot;CODEPIPELINE&quot;\n  }\n}\nIt is important to note, that we give the task definition arn wihtout it’s\nrevision. This is how we always force, that the most current task definition\nrevision is used.\nTo pass these two files to CodeDeploy, we set them as artifacts of the\nCodeBuild in the buildspec.yml:\nartifacts:\n  files:\n    - appspec.yaml\n    - taskdef.json\nBecause we already set the output artifacts path in the aws_codepipeline\nresource, these files will be saved in the build directory in S3, which will be\npicked up by CodeDeploy afterwards.\nNow we need the IAM Policy for CodeDeploy:\ndata &quot;aws_iam_policy_document&quot; &quot;assume_by_codedeploy&quot; {\n  statement {\n    sid     = &quot;&quot;\n    effect  = &quot;Allow&quot;\n    actions = [&quot;sts:AssumeRole&quot;]\n \n    principals {\n      type        = &quot;Service&quot;\n      identifiers = [&quot;codedeploy.amazonaws.com&quot;]\n    }\n  }\n}\n \nresource &quot;aws_iam_role&quot; &quot;codedeploy&quot; {\n  name               = &quot;codedeploy&quot;\n  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_codedeploy.json}&quot;\n}\n \ndata &quot;aws_iam_policy_document&quot; &quot;codedeploy&quot; {\n  statement {\n    sid    = &quot;AllowLoadBalancingAndECSModifications&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;ecs:CreateTaskSet&quot;,\n      &quot;ecs:DeleteTaskSet&quot;,\n      &quot;ecs:DescribeServices&quot;,\n      &quot;ecs:UpdateServicePrimaryTaskSet&quot;,\n      &quot;elasticloadbalancing:DescribeListeners&quot;,\n      &quot;elasticloadbalancing:DescribeRules&quot;,\n      &quot;elasticloadbalancing:DescribeTargetGroups&quot;,\n      &quot;elasticloadbalancing:ModifyListener&quot;,\n      &quot;elasticloadbalancing:ModifyRule&quot;,\n    ]\n \n    resources = [&quot;*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowS3&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [&quot;s3:GetObject&quot;]\n \n    resources = [&quot;${aws_s3_bucket.this.arn}/*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowPassRole&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [&quot;iam:PassRole&quot;]\n \n    resources = [\n      &quot;${aws_iam_role.execution_role.arn}&quot;,\n      &quot;${aws_iam_role.task_role.arn}&quot;,\n    ]\n  }\n}\n \nresource &quot;aws_iam_role_policy&quot; &quot;codedeploy&quot; {\n  role   = &quot;${aws_iam_role.codedeploy.name}&quot;\n  policy = &quot;${data.aws_iam_policy_document.codedeploy.json}&quot;\n}\nEverything is analogous to the CodeBuild and the pipeline policies. CodeDeploy\nneeds permissions to modify the ECS task sets and the load balancer. But\nCodeDeploy just needs to get the items from S3 and doesn’t write anything back.\nNow we can create the CodeDeploy App and the Deployment Group:\nresource &quot;aws_codedeploy_deployment_group&quot; &quot;this&quot; {\n  app_name               = &quot;${aws_codedeploy_app.this.name}&quot;\n  deployment_group_name  = &quot;example-deploy-group&quot;\n  deployment_config_name = &quot;CodeDeployDefault.ECSAllAtOnce&quot;\n  service_role_arn       = &quot;${aws_iam_role.codedeploy.arn}&quot;\n \n  blue_green_deployment_config {\n    deployment_ready_option {\n      action_on_timeout = &quot;CONTINUE_DEPLOYMENT&quot;\n    }\n \n    terminate_blue_instances_on_deployment_success {\n      action = &quot;TERMINATE&quot;\n    }\n  }\n \n  ecs_service {\n    cluster_name = &quot;${aws_ecs_cluster.this.name}&quot;\n    service_name = &quot;${aws_ecs_service.this.name}&quot;\n  }\n \n  deployment_style {\n    deployment_option = &quot;WITH_TRAFFIC_CONTROL&quot;\n    deployment_type   = &quot;BLUE_GREEN&quot;\n  }\n \n  load_balancer_info {\n    target_group_pair_info {\n      prod_traffic_route {\n        listener_arns = [&quot;${aws_lb_listener.this.arn}&quot;]\n      }\n \n      target_group {\n        name = &quot;${aws_lb_target_group.this.*.name[0]}&quot;\n      }\n \n      target_group {\n        name = &quot;${aws_lb_target_group.this.*.name[1]}&quot;\n      }\n    }\n  }\n}\nHere we define the deployment group to be a blue/green deployment. We define\nour ECS Service to be the Service where to deploy in. We also give it the load\nblancer listener and the target groups. With these infos AWS is now able to\ncreate a fully working blue/green deployment.\nThe last step is to add the deployment to the pipeline as a new stage:\nstage {\n  name = &quot;Deploy&quot;\n \n  action {\n    name            = &quot;Deploy&quot;\n    category        = &quot;Deploy&quot;\n    owner           = &quot;AWS&quot;\n    provider        = &quot;CodeDeployToECS&quot;\n    version         = &quot;1&quot;\n    input_artifacts = [&quot;build&quot;]\n \n    configuration {\n      ApplicationName                = &quot;${aws_codedeploy_app.this.name}&quot;\n      DeploymentGroupName            = &quot;${aws_codedeploy_deployment_group.this.deployment_group_name}&quot;\n      TaskDefinitionTemplateArtifact = &quot;build&quot;\n      AppSpecTemplateArtifact        = &quot;build&quot;\n    }\n  }\n}\nAnd we need to extend the IAM permissions, so the CodePipeline is able to to\ntrigger CodeDeploy:\nstatement {\n  sid    = &quot;AllowCodeDeploy&quot;\n  effect = &quot;Allow&quot;\n \n  actions = [\n    &quot;codedeploy:CreateDeployment&quot;,\n    &quot;codedeploy:GetApplication&quot;,\n    &quot;codedeploy:GetApplicationRevision&quot;,\n    &quot;codedeploy:GetDeployment&quot;,\n    &quot;codedeploy:GetDeploymentConfig&quot;,\n    &quot;codedeploy:RegisterApplicationRevision&quot;,\n  ]\n \n  resources = [&quot;*&quot;]\n \n  statement {\n    sid    = &quot;AllowECS&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [&quot;ecs:*&quot;]\n \n    resources = [&quot;*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowPassRole&quot;\n    effect = &quot;Allow&quot;\n \n    resources = [&quot;*&quot;]\n \n    actions = [&quot;iam:PassRole&quot;]\n \n    condition {\n      test     = &quot;StringLike&quot;\n      values   = [&quot;ecs-tasks.amazonaws.com&quot;]\n      variable = &quot;iam:PassedToService&quot;\n    }\n  }\n}\nTo make an ECS Service deployable by CodeDeploy, we add the following block to\nthe aws_ecs_service resource:\ndeployment_controller {\n  type = &quot;CODE_DEPLOY&quot;\n}\nWith everything applied, a first AWS CodePipeline run through is triggered\nautomatically.  The progress of the pipeline can be followed in the AWS Web\nConsole:\n\nCodeDeploy also has a very good overview of the state of the Blue/Green\nDeployment, where you can see how much traffic to which target group is\nforwarded at the moment:\n\nFurthermore you can append validation lambdas to different states of the\ndeployment to check automatically, if the newly started version of your\napplication is working correctly. For example you can program a lambda which\nchecks your newly created application routes for a 500 status code. When the\nlambda discovers one, it returns a validation failure and the deployment is\nrolled back automatically. You get an overview of the lifecycle events further\ndown the page.\n\nI hope this post was helpful. It is very extensive and some parts, where it\ncould be shortened. Especially the explicit IAM permissions take up a big part.\nBut I don’t want to encourage you to use the predefined policies, because they\noften allow more than needed. If you are writing your own policies you\nget much more control and can fine grain your permissions for every AWS\nservice you instantiate."},"posts/AWS/How-to-migrate-from-Serverless-to-CDK":{"slug":"posts/AWS/How-to-migrate-from-Serverless-to-CDK","filePath":"posts/AWS/How to migrate from Serverless to CDK.md","title":"How to migrate from Serverless to CDK","links":[],"tags":["Serverless","CDK","AWS"],"content":"The Serverless Application Framework was the go to solution to deploy serverless\napplication stacks, like AWS Lambda for quite a while. It offers a very easy to\nuse YAML DSL to deploy common serverless patterns, like a Lambda function behind\nan API Gateway. However if serverless applications become more complex and\nconsist of more than the default resources, you were back writing at CloudFormation\nagain.\nI faced a scenario like this recently. The same Lambdas needed to be deployed\nbehind two different API Gateways. One is hosted at the edge and the other one\nwas deployed regionally. Realizing this with the Serverless Framework isn’t\npossible, because it only supports one API Gateway. The other one has to be\nconfigured and referenced manually in the CloudFormation part of your\nserverless.yml. This wasn’t the most comfortable way forward. Instead we\ndecided to go with CDK from here.\nHowever it wasn’t possible to tear down the whole stack and rewrite the\ninfrastructure in CDK, because we had resources holding important data (DynamoDB\nand Cognito Identity Pool). We didn’t want to have a downtime either, where we\nwould dump the data back into the newly provisioned resources. So we needed to\nmigrate over the existing stack and continue working on that via CDK. This\nwasn’t possible until of October 2020, when the CDK team announced the new\ncloudformation-include module.\nIn this post I want to go over a simple Serverless example and how we are able\nto migrate this project with the help of this new CDK module.\nThe Serverless example application\nAs a starting point we take the Rest API with\nDynamoDB\nexample from the Serverless website. You can install it on your machine like\nthis to go along:\nserverless install -u github.com/serverless/examples/tree/master/aws-node-rest-api-with-dynamodb -n  aws-node-rest-api-with-dynamodb\nWe slightly cut down the example to have a reason to do a migration. Namely we\nremove the iamRoleStatements and environment variable from the\nserverless.yml. This will be the part we want to add to the CDK Code as a\nshowcase how to add new infrastructure components to the existing stack. Now the\nserverless.yml should look like this:\nservice: aws-node-rest-api-with-dynamodb\n \nframeworkVersion: &quot;&gt;=1.1.0&quot;\n \nprovider:\n  name: aws\n  runtime: nodejs10.x\nfunctions:\n  create:\n    handler: todos/create.create\n    events:\n      - http:\n          path: todos\n          method: post\n          cors: true\n \n  list:\n    handler: todos/list.list\n    events:\n      - http:\n          path: todos\n          method: get\n          cors: true\n \n  get:\n    handler: todos/get.get\n    events:\n      - http:\n          path: todos/{id}\n          method: get\n          cors: true\n \n  update:\n    handler: todos/update.update\n    events:\n      - http:\n          path: todos/{id}\n          method: put\n          cors: true\n \n  delete:\n    handler: todos/delete.delete\n    events:\n      - http:\n          path: todos/{id}\n          method: delete\n          cors: true\n \nresources:\n  Resources:\n    TodosDynamoDbTable:\n      Type: &#039;AWS::DynamoDB::Table&#039;\n      Properties:\n        TableName: Todos\n        BillingMode: PAY_PER_REQUEST\n        AttributeDefinitions:\n          -\n            AttributeName: id\n            AttributeType: S\n        KeySchema:\n          -\n            AttributeName: id\n            KeyType: HASH\nLet’s install the dependencies and deploy the application with:\nnpm install &amp;&amp; serverless deploy\nWe get an API Gateway with five Lambda Proxies, which will be triggered when you\ndo a request to the respective endpoint with the right method. Also a DynamoDB\nwill be deployed, but the Lambdas don’t have access so far and don’t know the\nname of the DynamoDB, because we currently don’t pass it via an environment\nvariable.\nWe can now try to create a todo. The output of serverless should have an output\nlike this at the end:\nServerless: Stack update finished...\nService Information\nservice: aws-node-rest-api-with-dynamodb\nstage: dev\nregion: eu-central-1\nstack: aws-node-rest-api-with-dynamodb-dev\nresources: 35\napi keys:\n  None\nendpoints:\n  POST - mhau9nhq75.execute-api.eu-central-1.amazonaws.com/dev/todos\n  GET - mhau9nhq75.execute-api.eu-central-1.amazonaws.com/dev/todos\n  GET - mhau9nhq75.execute-api.eu-central-1.amazonaws.com/dev/todos/{id}\n  PUT - mhau9nhq75.execute-api.eu-central-1.amazonaws.com/dev/todos/{id}\n  DELETE - mhau9nhq75.execute-api.eu-central-1.amazonaws.com/dev/todos/{id}\nfunctions:\n  create: aws-node-rest-api-with-dynamodb-dev-create\n  list: aws-node-rest-api-with-dynamodb-dev-list\n  get: aws-node-rest-api-with-dynamodb-dev-get\n  update: aws-node-rest-api-with-dynamodb-dev-update\n  delete: aws-node-rest-api-with-dynamodb-dev-delete\nlayers:\n  None\n\nLet’s try to create a todo by calling the todos endpoint via POST and giving a\ntodo:\n$ curl -XPOST mhau9nhq75.execute-api.eu-central-1.amazonaws.com/dev/todos --data &#039;{&quot;text&quot;: &quot;Migrate to CDK&quot;}&#039;\nCouldn&#039;t create the todo item.\nAs expected we got an error, that we can’t create a todo item. If we go into the\nCloudWatch Logs for the create Lambda, we see an error message like this:\nMissingRequiredParameter: Missing required key &#039;TableName&#039; in params at ParamValidator.\n\nIntroducing CDK into an existing Serverless project\nWe are now at the point, where we want to add new stuff to our infrastructure,\nbut don’t want to do it via CloudFormation. Instead we want to move the existing\nstack to CDK, but don’t want to recreate everything from scratch and also keep\nthe DynamoDB with all its data as is (Yes, we waited around two years between\nchapter one and two).\nTo keep the new CDK code separated, we initiate the CDK project in a new subdirectory:\n$ mkdir cdk-infra &amp;&amp; cd cdk-infra &amp;&amp; cdk init app --language=typescript\nWe also move everything specific to the Lambda Code in another subdirectory.\n$ cd .. &amp;&amp; mkdir code;\nmv todos package* node_modules/ code;\nLet CDK use the existing CloudFormation stack\nNow we want to import the existing stack into CDK. For this we use the new\ncloudformation-include module. We can install it in the cdk-infra\nsubdirectory via npm:\nnpm install @aws-cdk/cloudformation-include\nThe module has a CfnInclude class, which works the same like the one from the\naws-core module, but afterwards we can do a lot more, as you will see. Importing a\nCloudFormation Template is similar to the old CfnInclude class, where we need\nto pass a path to the CloudFormation Template, which represents the current\nstack.\nSince the serverless.yml is not an actual CloudFormation Template, we need to\nretrieve the finished, rendered version from for example the AWS\nUI. Navigating to the existing stack, which is called\naws-node-rest-api-with-dynamodb-dev, shows us a Template tab in which we\nfind the rendered CloudFormation template:\n\nWe copy/paste the template to the file cdk-infra/resources/template.json.\nUnder lib/cdk-infra-stack we edit the current code to create the CfnInclude\nclass and use the saved CloudFormation Template:\nimport * as cdk from &#039;@aws-cdk/core&#039;;\nimport { CfnInclude } from &#039;@aws-cdk/cloudformation-include&#039;;\n \nexport class CdkInfraStack extends cdk.Stack {\n  constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n \n    const cfnInclude = new CfnInclude(this, &#039;Template&#039;, {\n      templateFile: &#039;resources/template.json&#039;,\n    });\n  }\n}\nThe last thing we need to do is to name the stack, which CDK wants to create, to the\none which Serverless already deployed. For this we go to bin/cdk-infra.ts and edit\nthe stack name:\n#!/usr/bin/env node\nimport &#039;source-map-support/register&#039;;\nimport * as cdk from &#039;@aws-cdk/core&#039;;\nimport { CdkInfraStack } from &#039;../lib/cdk-infra-stack&#039;;\n \nconst app = new cdk.App();\nnew CdkInfraStack(app, &#039;aws-node-rest-api-with-dynamodb-dev&#039;);\nWe are now able to execute all cdk commands from the cdk-infra\nsubdirectory. Let’s try if we can retrieve a diff between the deployed stack and\nour code. When we try to get the difference between our CDK code and the\ndeployed stack via:\ncdk diff\nwe get an error, which contains this message:\nError: Resolution error: Supplied properties not correct for &quot;CfnRestApiProps&quot;\n  policy: &quot;&quot; should be an &#039;object&#039;.\n\nThis is an inconsistency between CloudFormation and the CDK CloudFormation Reader. CDK always\nexpects an object for the policy key, but the rendered CloudFormation Template\nhas an empty string. To fix this we can remove it from the saved Template by\nsearching for the term ,&quot;Policy&quot;:&quot;&quot; and remove it.\nNow cdk diff should work and will give us the following output:\nStack aws-node-simple-http-endpoint-dev\nConditions\n[+] Condition CDKMetadata/Condition CDKMetadataAvailable: {&quot;Fn::Or&quot;:[{&quot;Fn::Or&quot;:[{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ap-east-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ap-northeast-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ap-northeast-2&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ap-south-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ap-southeast-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ap-southeast-2&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;ca-central-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;cn-north-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;cn-northwest-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;eu-central-1&quot;]}]},{&quot;Fn::Or&quot;:[{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;eu-north-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;eu-west-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;eu-west-2&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;eu-west-3&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;me-south-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;sa-east-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;us-east-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;us-east-2&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;us-west-1&quot;]},{&quot;Fn::Equals&quot;:[{&quot;Ref&quot;:&quot;AWS::Region&quot;},&quot;us-west-2&quot;]}]}]}\n\nResources\n[~] AWS::ApiGateway::RestApi Template/ApiGatewayRestApi ApiGatewayRestApi \n └─ [-] Policy\n     └─ \n\nCDK always adds its Metadata to a stack and it wants to remove the empty\npolicy. Beside of these differences the CDK project would preserve anything in\nthe stack as it was before. We are now able to remove the serverless.yml and\ncontinue building our infrastructure in CDK. But before we continue, let’s\ndeploy the changes:\ncdk deploy\nUse existing Resources in CDK\nNow that we have imported the existing stack into our CDK project, we are able\nto use all resources, which are deployed as part of our stack. This is now\npossible thanks to the new cloudformation_include module.\nTo show you what I mean by this, let’s give our Lambda Functions the Environment\nVariable with the DynamoDB name. First we need the DynamoDB and Lambda CDK packages. So\nlet’s install these inside of the cdk-infra directory as well.\nnpm install @aws-cdk/aws-dynamodb @aws-cdk/aws-lambda\n\nNow we can assign the DynamoDB table name as an environment variable to the\nlambda functions. Fist we retrieve the DynamoDB from CfnIncludes getResource method:\nconst dynamoDB = cfnInclude.getResource(&#039;TodosDynamoDbTable&#039;) as CfnTable;\ngetResource takes the logical ID of a resource as the parameter. The\nlogical id can be found again in the AWS UI under the Resources tab for example.\n\nThese resources will be imported as a generic type CfnResource and can be\ncasted to the respective Cfn sub type. In this case we cast it to\nCfnTable.\nBecause you can emit the TableName it’s a conditional type and we extract the\ntable name next:\nif (!dynamoDB.tableName) {\n  throw new Error(&quot;DynamoDB has no name&quot;);\n}\nconst dynamodbTable: string = dynamoDB.tableName;\nIf there would be no table name, we would throw an error. Now we can apply the\nsame principle to get the CfnFunctions. For this we again need the logical\nids. We save them in a readonly field of our class:\nreadonly lambdaLogicalNames = [\n  &#039;CreateLambdaFunction&#039;,\n  &#039;DeleteLambdaFunction&#039;,\n  &#039;GetLambdaFunction&#039;,\n  &#039;UpdateLambdaFunction&#039;,\n  &#039;ListLambdaFunction&#039;,\n];\nNow we can iterate over this array, get the CfnFunctions from these logical\nids and iterate over these to set the environment variable DYNAMODB_NAME:\nconst cfnFunctions = this.lambdaLogicalNames.map(\n  (logicalName) =&gt; cfnInclude.getResource(logicalName) as CfnFunction\n);\n \ncfnFunctions.forEach((f) =&gt; f.environment = {\n  variables: {\n    &#039;DYNAMODB_TABLE: dynamodbTable,\n  }\n});\nCalling cdk diff again would give us the following changes:\nStack aws-node-rest-api-with-dynamodb-dev\nResources\n[~] AWS::Lambda::Function Template/CreateLambdaFunction CreateLambdaFunction \n └─ [+] Environment\n     └─ {&quot;Variables&quot;:{&quot;DYNAMODB_TABLE&quot;:&quot;Todos&quot;}}\n[~] AWS::Lambda::Function Template/ListLambdaFunction ListLambdaFunction \n └─ [+] Environment\n     └─ {&quot;Variables&quot;:{&quot;DYNAMODB_TABLE&quot;:&quot;Todos&quot;}}\n[~] AWS::Lambda::Function Template/GetLambdaFunction GetLambdaFunction \n └─ [+] Environment\n     └─ {&quot;Variables&quot;:{&quot;DYNAMODB_TABLE&quot;:&quot;Todos&quot;}}\n[~] AWS::Lambda::Function Template/UpdateLambdaFunction UpdateLambdaFunction \n └─ [+] Environment\n     └─ {&quot;Variables&quot;:{&quot;DYNAMODB_TABLE&quot;:&quot;Todos&quot;}}\n[~] AWS::Lambda::Function Template/DeleteLambdaFunction DeleteLambdaFunction \n └─ [+] Environment\n     └─ {&quot;Variables&quot;:{&quot;DYNAMODB_TABLE&quot;:&quot;Todos&quot;}}\nExactly what we want, let’s deploy it! Here is the complete code of the\nlib/cdk-infra-stack.ts class as of now:\nimport * as cdk from &#039;@aws-cdk/core&#039;;\nimport { CfnInclude } from &#039;@aws-cdk/cloudformation-include&#039;;\nimport { CfnFunction } from &#039;@aws-cdk/aws-lambda&#039;;\nimport { CfnTable } from &#039;@aws-cdk/aws-dynamodb&#039;;\n \nexport class CdkInfraStack extends cdk.Stack {\n  readonly lambdaLogicalNames = [\n    &#039;CreateLambdaFunction&#039;,\n    &#039;DeleteLambdaFunction&#039;,\n    &#039;GetLambdaFunction&#039;,\n    &#039;UpdateLambdaFunction&#039;,\n    &#039;ListLambdaFunction&#039;,\n  ];\n  constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n    const cfnInclude = new CfnInclude(this, &#039;Template&#039;, {\n      templateFile: &#039;resources/template.json&#039;,\n    });\n \n    const dynamoDB = cfnInclude.getResource(&#039;TodosDynamoDbTable&#039;) as CfnTable;\n    if (!dynamoDB.tableName) {\n      throw new Error(&quot;DynamoDB has no name&quot;);\n    }\n    const dynamodbTable: string = dynamoDB.tableName;\n \n    const cfnFunctions = this.lambdaLogicalNames.map(\n      (logicalName) =&gt; cfnInclude.getResource(logicalName) as CfnFunction\n    );\n \n    cfnFunctions.forEach((f) =&gt; f.environment = {\n      variables: {\n        &#039;DYNAMODB_TABLE: dynamodbTable,\n      }\n    });\n  }\n}\nIf we try the curl command again, we still get the same error though. While\nCloudWatch yields a message like this:\naws-node-rest-api-with-dynamodb-dev-create is not authorized to perform: dynamodb:PutItem on resource\n\nRight, we didn’t gave it the correct permissions. Let’s change this! The easiest\nway to give a Lambda the necessary permissions is via the grant* methods. The\nproblem with these grant* methods is, they are only available on higher\nconstructs, but not for the Cfn variants. But this isn’t a big deal. We can\njust retrieve the higher constructs via the static from* methods, they\nprovide. It looks like this for the DynamoDB table:\nconst table = Table.fromTableArn(this, &#039;HigherTable&#039;, dynamoDB.attrArn);\nFor the Lambda Functions we can’t do the same. Because the grant* methods\nmodify the Lambda Execution Role, we need to provide it. If CDK is loading a\nFunction via fromFunctionArn it doesn’t load the role as well and therefore\ncan’t modify it. Instead we need to load the higher functions with the\nfromFunctionAttributes method. There we pass the role as well as the ARN. Now\nwe can apply the same principles, we already discovered to get the role and pass\nit to fromFunctionAttributes. First we install the iam module:\nnpm install @aws-cdk/aws-iam;\nAnd this is the according code:\n \nconst cfnRole = cfnInclude.getResource(&#039;IamRoleLambdaExecution&#039;) as CfnRole;\nconst role = Role.fromRoleArn(this, &#039;HigherRole&#039;, cfnRole.attrArn);\nconst functions = cfnFunctions.map((f) =&gt; Function.fromFunctionAttributes(\n  this,\n  &#039;HigherFunction&#039; + f.functionName,\n  {\n    functionArn: f.attrArn,\n    role: role\n  }\n));\nAnd finally we can grant read and write access to our functions:\nfunctions.forEach((f) =&gt; table.grantReadWriteData(f));\nExecuting the curl command again finally yields success:\n{&quot;id&quot;:&quot;8b585470-40a7-11eb-9910-c977be42124e&quot;,&quot;text&quot;:&quot;Migrate to CDK&quot;,&quot;checked&quot;:false,&quot;createdAt&quot;:1608237390647,&quot;updatedAt&quot;:1608237390647}\n\nApply Code Updates to Lambdas\nYou may already noticed that there is still an open issue, which we didn’t solve\nso far. Serverless also packaged our Code into a zip file and pushed it to S3 to\nupdate our Lambdas with the latest changes. If we would do a Code change now, it\nwouldn’t deploy these changes. Let’s do an example. Currently an empty JSON\nobject is returned when we delete a todo item. Let’s return a meaningful\ndeletion message instead. We change the code for todos/delete.js like this:\n...\n \n// create a response\nconst response = {\n  statusCode: 200,\n  body: JSON.stringify(&#039;TODO Item successfully deleted!&#039;),\n};\ncdk deploy only gives us this output\naws-node-rest-api-with-dynamodb-dev: deploying...\n\n ✅  aws-node-rest-api-with-dynamodb-dev (no changes)\n\n\nWhich already says that nothing changed. If we delete the item, we still get the\nempty json object back:\n$ curl -XDELETE ztr40k0bf7.execute-api.eu-central-1.amazonaws.com/dev/todos/8b585470-40a7-11eb-9910-c977be42124e\n{}\nThankfully CDK also provides a solution to this problem. But let’s have a look\nat the deployment bucket of the Serverless Framework first. We see that\nServerless is zipping up the whole project directory and uploads the zip file into the S3\nBucket under the following path:\nserverless/aws-node-rest-api-with-dynamodb/dev/&lt;some-date-time-string&gt;/.\nThere is a new CDK module called aws-s3-assets which has an experimental\nConstruct called Asset. With this construct we can achieve the same\ngoal. Basically we can give the path to our code directory and CDK is packaging\nthat directory into a S3 Bucket before the actual deployment. Afterwards we can\naccess information like the bucket name or object name. With these information\nwe can override the code attribute of our lambdas, so they get updated with\nthe newly uploaded code:\nconst asset = new Asset(this, &#039;LambdaCode&#039;, {\n  path: &#039;../code&#039;,\n});\n \ncfnFunctions.forEach((f) =&gt;\n  f.code = {\n    s3Bucket: asset.s3BucketName,\n    s3Key: asset.s3ObjectKey,\n  }\n);\nIf we create a new todo item and delete it afterwards, we get the new message\nand can be assured, that the newest version of our code was uploaded.\n$ curl -XPOST\nztr40k0bf7.execute-api.eu-central-1.amazonaws.com/dev/todos --data &#039;{&quot;text&quot;: &quot;Migrate to CDK&quot;}&#039;\n{&quot;id&quot;:&quot;10150e20-40ac-11eb-9edf-0dd37a8498a2&quot;,&quot;text&quot;:&quot;Migrate to   CDK&quot;,&quot;checked&quot;:false,&quot;createdAt&quot;:1608239331330,&quot;updatedAt&quot;:1608239331330}\n \n$ curl -XDELETE ztr40k0bf7.execute-api.eu-central-1.amazonaws.com/dev/todos/10150e20-40ac-11eb-9edf-0dd37a8498a2\n&quot;TODO Item successfully deleted!&quot;\nHere is the final state of our lib/cdk-infra-stack.ts class:\nimport * as cdk from &#039;@aws-cdk/core&#039;;\nimport { CfnInclude } from &#039;@aws-cdk/cloudformation-include&#039;;\nimport { CfnFunction, Function } from &#039;@aws-cdk/aws-lambda&#039;;\nimport { CfnTable, Table } from &#039;@aws-cdk/aws-dynamodb&#039;;\nimport { CfnRole, Role } from &#039;@aws-cdk/aws-iam&#039;;\nimport { Asset } from &#039;@aws-cdk/aws-s3-assets&#039;;\n \nexport class CdkInfraStack extends cdk.Stack {\n  readonly lambdaLogicalNames = [\n    &#039;CreateLambdaFunction&#039;,\n    &#039;DeleteLambdaFunction&#039;,\n    &#039;GetLambdaFunction&#039;,\n    &#039;UpdateLambdaFunction&#039;,\n    &#039;ListLambdaFunction&#039;,\n  ];\n  constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n    const cfnInclude = new CfnInclude(this, &#039;Template&#039;, {\n      templateFile: &#039;resources/template.json&#039;,\n    });\n \n    const dynamoDB = cfnInclude.getResource(&#039;TodosDynamoDbTable&#039;) as CfnTable;\n    if (!dynamoDB.tableName) {\n      throw new Error(&quot;DynamoDB has no name&quot;);\n    }\n    const dynamodbTable: string = dynamoDB.tableName;\n \n    const cfnFunctions = this.lambdaLogicalNames.map(\n      (logicalName) =&gt; cfnInclude.getResource(logicalName) as CfnFunction\n    );\n \n    cfnFunctions.forEach((f) =&gt; f.environment = {\n      variables: {\n        &#039;DYNAMODB_TABLE&#039;: dynamodbTable,\n      }\n    });\n \n    const table = Table.fromTableArn(this, &#039;HigherTable&#039;, dynamoDB.attrArn);\n \n    const cfnRole = cfnInclude.getResource(&#039;IamRoleLambdaExecution&#039;) as CfnRole;\n    const role = Role.fromRoleArn(this, &#039;HigherRole&#039;, cfnRole.attrArn);\n    const functions = cfnFunctions.map((f) =&gt; Function.fromFunctionAttributes(\n      this,\n      &#039;HigherFunction&#039; + f.functionName,\n      {\n        functionArn: f.attrArn,\n        role: role\n      }\n    ));\n \n    functions.forEach((f) =&gt; table.grantReadWriteData(f));\n \n    const asset = new Asset(this, &#039;LambdaCode&#039;, {\n      path: &#039;../code&#039;,\n    });\n \n    cfnFunctions.forEach((f) =&gt;\n      f.code = {\n        s3Bucket: asset.s3BucketName,\n        s3Key: asset.s3ObjectKey,\n    });\n  }\n}\nSummary\nThanks to the new cloudformation-include module we were able to migrate the\nexisting stack to CDK in no time. Afterwards it’s now possible to work with all\nthe resources deployed in the stack. You can use them as a reference for new\nresources or extend/modify the existing ones. Lastly the updating of the\ncode is very tricky, but aws-s3-assets helps in this case.\nIf you want to go even further and rewrite parts of your infrastructure in CDK,\nlike the API Gateway for example, you can do so. For this you can remove the\nrespective part from resources/template.json and create the higher construct\nin CDK directly:\nimport { RestApi } from &#039;@aws-cdk/aws-apigateway&#039;;\n...\nconst restApi = new RestApi(this, &#039;&lt;RestApiLogicalId&gt;&#039;, {...});\nconst cfnRestApi = restApi.node.defaultChild as CfnRestApi;\ncfnRestApi.overrideLogicalId(&#039;&lt;pass the logical id of the existing rest api here&gt;&#039;);\nThe important part is, that you pass the logical id of the existing resource\ninto the Cfn variant. If you now do cdk diff you see the differences between\nthe RestApi in CDK and what’s deployed. You can now incrementally resolve all\nthe differences until there are no left. Then you can deploy again and migrated\nthe resource successfully from CloudFormation into CDK code. However we don’t go\ninto detail here, because it doesn’t bring any functional benefits. However it’s\nan option if you have the time and patience and really want to get rid of the\nCloudFormation Template completely."},"posts/AWS/VPC-Peering-with-MongoDB-Atlas-and-AWS-in-Terraform":{"slug":"posts/AWS/VPC-Peering-with-MongoDB-Atlas-and-AWS-in-Terraform","filePath":"posts/AWS/VPC Peering with MongoDB Atlas and AWS in Terraform.md","title":"VPC Peering with MongoDB Atlas and AWS in Terraform","links":[],"tags":["mongodb","atlas","terraform","AWS"],"content":"Lately I was doing a lot of “Infrastructure as Code” (IaC) in terraform at\nwork. There was one application, which needed a MongoDB as the primary database.\nSadly there is no managed MongoDB service in AWS so far. So there were two\ninitial options for me:\n\nHost a self managed MongoDB in an EC2 instance\nUse Amazons DynamoDB\n\nThe main goal of the move into the cloud was to get away from all the\nadministration tasks and have everything managed by the cloud hosters, so we\nhave more time to get our actual problems solved. That’s why option one wasn’t\nreally something we wantet to do. Option two wasn’t satisfactorily either,\nbecause we would go deep into the AWS rabbit hole. We wouldn’t have the chance\nto test the application locally and always needed a connection to an actual\nDynamoDB instance.\nUpdate 10.02.2019: Actually there is a real alternative now called AWS\nDocumentDB. I didn’t look into it so far, so I don’t feel qualified to give any\nopinion on DocumentDB. But based on the MongoDB CEO MongoDB Atlas still has\nit’s right to\nexist.\nLuckily there was a third option called MongoDB\nAtlas. This is MongoDB Inc’s own take at\nDaaS (Database as a Service). They instanciate a replica set of three or\nmore instances on one of the most common cloud providers (AWS, Google Cloud\nPlatform or Azure) for you. Additionally you get backups, auto scaling, alerts\nand many more features. MongoDB Atlas is also following an API first approach\nlike AWS, so the creation of MongoDB resources can be automated. Sadly there is\nno support for MongoDB Atlas in Terraform so far. Luckily Akshay Karle already\ntook care of this problem and wrote a third party plugin for\nTerraform.\nIn this blog post we create an infrastructure setup which consists of 2 AWS\nVPCs.  In the first VPC our application will be hosted. This application will\nbe able to communicate with the MongoDB replica set, hosted in another VPC by\nAtlas.  Because we will host our MongoDB cluster in the same region, we can\nbenefit of VPC peering. This means, that the application and MongoDB can\ncommunicate directly via local IPs between the two VPCs and no traffic goes out\nto the internet and back into the other VPC. This is more secure (Because you\ndon’t need to care about securing the connection, because it’s local anyway)\nand also a lot of faster than sending every request and response over the\ninternet. We will code everything in Terraform, so we are able to create our\nwhole infrastructure with one terraform apply and are done with it. So let’s\nget started. Here is a rough overview of the most important pieces:\n\nAs you can see we have our 2 VPCs. The left AWS VPC will be avaible in the CIDR\nblock 172.16.0.0/16. The right VPC is the one created by Mongo Atlas and will\nbe available under 10.0.0.0/21. Both VPCs are connected via VPC-Peering. Also\nthe CIDR block of the AWS VPC is whitelisted in MongoDB Atlas. On top we have a\nroute table, to route traffic between the two VPCs and to the internet via an\ninternet gateway.\nThe following guide will be threefold. In the first part we create the route\ntable, internet gateway and AWS VPC with it’s subnet. In part two the MongoDB\npart will be build. In the third part we will deploy a small application inside\nof an EC2 instance, which is able to interact with the MongoDB. The whole code\nfor this guide can be found\nhere.\nPart 1: The AWS VPC\nIn this part we will be creating the left side of our overview diagram. So\nbasically this part:\n\nIn my example project I have created a subfolder called terraform where all\nthe infrastructure code can be found. First of all we load the AWS Provider.\nThis is done in the main.tf:\nprovider &quot;aws&quot; {\n  region  = &quot;eu-central-1&quot;\n  version = &quot;1.54&quot;\n}\nNext we create the actual VPC. This is done in the vpc.tf file:\nresource &quot;aws_vpc&quot; &quot;this&quot; {\n  cidr_block = &quot;172.16.0.0/16&quot;\n \n  enable_dns_support   = true\n  enable_dns_hostnames = true\n \n  tags = {\n    Name = &quot;vpc&quot;\n  }\n}\nAs already stated in the beginning, we use the 172.16.0.0/16 CIDR block for the\nAWS VPC. We also enable DNS support and DNS hostnames inside of the VPC. With\nthose two options we basically enable DNS discovery in the local VPC scope,\nwhich allows us to resolve the MongoDB cluster DNS to it’s private IP.\nNow we create a subnet inside of the VPC:\n \nresource &quot;aws_subnet&quot; &quot;this&quot; {\n  cidr_block = &quot;176.16.0.0/16&quot;\n  vpc_id     = &quot;${aws_vpc.this.id}&quot;\n \n  map_public_ip_on_launch = true\n \n  availability_zone = &quot;eu-central-1a&quot;\n \n  tags = {\n    Name = &quot;subnet1&quot;\n  }\n}\nHere we create one subnet, which takes all of the VPCs available addresses and\nis hosted in the availability zone eu-central-1a. Furthermore we want to give\nlaunched instances a public IP on startup to be able to download updates for\nthe EC2 instance.\nNext we put the internet gateway in front of the VPC, which is straight\nforward.\nresource &quot;aws_internet_gateway&quot; &quot;this&quot; {\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  tags = {\n    Name = &quot;internet-gateway&quot;\n  }\n}\nFinally we create the route table with a public route and associate our subnet\nto that route:\nresource &quot;aws_route_table&quot; &quot;this&quot; {\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  tags = {\n    Name = &quot;route-table-public&quot;\n  }\n}\n \nresource &quot;aws_route&quot; &quot;this&quot; {\n  route_table_id         = &quot;${aws_route_table.this.id}&quot;\n  destination_cidr_block = &quot;0.0.0.0/0&quot;\n  gateway_id             = &quot;${aws_internet_gateway.this.id}&quot;\n}\n \nresource &quot;aws_route_table_association&quot; &quot;this&quot; {\n  route_table_id = &quot;${aws_route_table.this.id}&quot;\n  subnet_id      = &quot;${aws_subnet.this.id}&quot;\n}\nBasically what we are doing here is to allow to route traffic from our VPC to\nevery address in the internet. This is mandatory, because we want to get\nresponses from our web applications inside of the VPC.\nThat’s it for the first part. You are now able to run terraform init &amp;&amp; terraform apply to deploy the AWS VPC.\nPart 2: MongoDB Atlas\nIn this part we care about the right half of the diagram and create our MongoDB\nCluster at Atlas, make a whitelist entry for our AWS VPC and create VPC Peering\nbetween the two VPCs:\n\nYou can find the part 2 code in the atlas.tf file. We get\nstarted again, by adding a new provider to the main.tf:\nprovider &quot;mongodbatlas&quot; {\n  username = &quot;${var.username}&quot;\n  api_key  = &quot;${var.api_key}&quot;\n}\nTo access your MongoDB Atlas account you need to pass your username and an API\nkey. If you define them as variables like me, you can create a new file called\nvariables.tf with the following content:\nvariable &quot;username&quot; {\n  type        = &quot;string&quot;\n  description = &quot;The Username for the MongoDB Atlas Login&quot;\n}\n \nvariable &quot;api_key&quot; {\n  type        = &quot;string&quot;\n  description = &quot;The API Key for the MongoDB Atlas Login&quot;\n}\nthis defines the variables used in the main.tf file. Next we create a\nterraform.tfvars in which we actually set the values of those variables:\nusername = &quot;&lt;your_username&gt;&quot;\napi_key = &quot;&lt;your_api_key&gt;&quot;\nBecause we save trustworthy information in those variables, we don’t actually\nset them in a normal *.tf file. These files are checked into version control,\nso we don’t want to write those informations down in those files. Instead we\nuse  *.tfvars files, which aren’t checked into version control.\nAs said in the beginning, mongodb atlas isn’t supported by terraform\nofficially. So we need to install the third party provider. To install the\nprovider you need to have go installed. With go\ninstalled you can get the package and symlink the executable to the plugin\nfolder of terraform:\ngo get github.com/akshaykarle/terraform-provider-mongodbatlas\nln -s $GOPATH/bin/terraform-provider-mongodbatlas \\\n      ~/.terraform.d/plugins/\nIf you do a terraform init again, you are able to initialize the mongodbatlas\nprovider as well.\nWith everything set up, we can start creating the MongoDB Atlas cluster.\nThe first thing needed is a project (former known as groups). Projects are a\nsort of grouping to isolate different environments from each other or to\nconfigure different alert settings. For a full description head over to the\nofficial\ndocumentation. We\ncreate a new file called atlas.tf for all our MongoDB Atlas resources. There\nwe create a project first:\nresource &quot;mongodbatlas_project&quot; &quot;this&quot; {\n  org_id = &quot;${var.org_id}&quot;\n  name   = &quot;example-project&quot;\n}\nTo create a project resource we need an organisation id, which can be found in\nthe settings tab:\n\nBecause we use it as a variable in the atlas.tf file, we need to add it to\nour terraform.tfvars\n...\norg_id = &quot;&lt;your-organisation-id&gt;&quot;\nand variables.tf\nvariable &quot;org_id&quot; {\n  type        = &quot;string&quot;\n  description = &quot;The organisation id of the mongodb Login&quot;\n}\nThe next thing we need is a container. The container is the network of the\ncloud provider. In the example of AWS it would create a VPC.\nresource &quot;mongodbatlas_container&quot; &quot;this&quot; {\n  group            = &quot;${mongodbatlas_project.this.id}&quot;\n  atlas_cidr_block = &quot;10.0.0.0/21&quot;\n  provider_name    = &quot;AWS&quot;\n  region           = &quot;EU_CENTRAL_1&quot;\n}\nPay attention to the region names, because they are different than in\nAWS.\nNow we can create the cluster:\nresource &quot;mongodbatlas_cluster&quot; &quot;this&quot; {\n  name                  = &quot;example&quot;\n  group                 = &quot;${mongodbatlas_project.this.id}&quot;\n  mongodb_major_version = &quot;4.0&quot;\n  provider_name         = &quot;AWS&quot;\n  region                = &quot;EU_CENTRAL_1&quot;\n  size                  = &quot;M10&quot;\n  disk_gb_enabled       = true\n  backup                = false\n  depends_on            = [&quot;mongodbatlas_container.this&quot;]\n}\nHere we create a MongoDB Cluster in Version 4.0 in AWS with the same region as\nthe container. M10 is the smallest non sandbox size available. It has 2GB of\nRAM, 10 GB of storage and 0.2 vCPUs. With disk_gb_enabled we allow the\ncluster to automatically scale up. Lastly the cluster should be created\nexplicitly after the container.\nNow we create a database user:\nresource &quot;mongodbatlas_database_user&quot; &quot;this&quot; {\n  username = &quot;application-user&quot;\n  password = &quot;application-pw&quot;\n  database = &quot;admin&quot;\n  group    = &quot;${mongodbatlas_project.this.id}&quot;\n \n  roles {\n    name     = &quot;readWrite\n    database = &quot;app&quot;\n  }\n}\nThis user will later be used by the application to access the MongoDB. He gets\na username, password and a will be authenticated in the admin database. Those\ninformation can also be exported to the terraform.tfvars file, but for this\nexample application I keep those information hardcoded in the atlas.tf file.\nThe admin database is the default value for MongoDB Atlas. The user gets\nrights to read and write to the app database.\nNext thing we do, is establishing the VPC peering connection:\nresource &quot;mongodbatlas_vpc_peering_connection&quot; &quot;this&quot; {\n  group                  = &quot;${mongodbatlas_project.this.id}&quot;\n  aws_account_id         = &quot;${var.aws_account_id}&quot;\n  vpc_id                 = &quot;${aws_vpc.this.id}&quot;\n  route_table_cidr_block = &quot;${aws_vpc.this.cidr_block}&quot;\n  container_id           = &quot;${mongodbatlas_container.this.id}&quot;\n}\n \nresource &quot;aws_vpc_peering_connection_accepter&quot; &quot;this&quot; {\n  vpc_peering_connection_id = &quot;${mongodbatlas_vpc_peering_connection.this.connection_id}&quot;\n  auto_accept               = true\n}\nFirst we create the MongoDB Atlas peering connection. The connection needs most\nof the stuff, we created before, like the AWS VPC to peer to and the container\nof our MongoDB cluster. Here we use another variable for the AWS account in\nwhich the destination VPC for the peering lies. This variable will be created\nanalogous to those, we created earlier. The second thing is an acceptor, which\nshould auto accept the peering requests for peerings with the MongoDB VPC.\nNow we can also create an entry in our route table for the new MongoDB Atlas\nVPC, which allows traffic to be routed between those two VPCs properly:\nresource &quot;aws_route&quot; &quot;this&quot; {\n  route_table_id            = &quot;${data.aws_route_table.this.id}&quot;\n  destination_cidr_block    = &quot;${mongodbatlas_container.this.atlas_cidr_block}&quot;\n  vpc_peering_connection_id = &quot;${mongodbatlas_vpc_peering_connection.this.connection_id}&quot;\n}\nThe last thing we have to do, is whitelist the AWS VPC CIDR Block in MongoDB\nAtlas, so that services inside of the AWS VPC are allowed to access the\ncluster.\nresource &quot;mongodbatlas_ip_whitelist&quot; &quot;this&quot; {\n  group      = &quot;${mongodbatlas_project.this.id}&quot;\n  cidr_block = &quot;${data.aws_vpc.this.cidr_block}&quot;\n  comment    = &quot;Whitelist for the AWS VPC&quot;\n}\nWith this in place we are now able to create our MongoDB Atlas cluster. Again\nyou can execute terraform apply to see the results.\nPart 3: Deploying the app\nIn the last part we keep it as simple as possible. We will create a single EC2\ninstance, which will be provisioned to install a docker container with a small\nPHP application. The important part here, is that we now can give the DSN (Data\nSource Name) of\nthe MongoDB to the app as an environment variable and the app is able to work\nwith the MongoDB without any further manual interventions.\nIn reality there is a lot more to running a scalable infrastructure, like load\nbalancing, autoscaling groups, launch templates and logging to name a few. But\ncovering these topics in this post would crush the scope of this article, which\nis already pretty long at this point.\nSo without further ado let’s get startet by creating a ec2.tf file. First of\nall we create an AMI\ndata &quot;aws_ami&quot; &quot;amazon_linux&quot; {\n  most_recent = true\n  owners      = [&quot;amazon&quot;]\n \n  filter {\n    name   = &quot;name&quot;\n    values = [&quot;amzn2-ami-hvm-2.0.20181024-x86_64-gp2&quot;]\n  }\n}\nThis defines an AMI (Amazon Machine Image) which is basically the operating\nsystem, the ec2 instance will be started with.\nIn this EC2 instance we will run a docker container. I created a repository in\namazons own ECR for my container image. So I add the repository as a data source:\ndata &quot;aws_ecr_repository&quot; &quot;example_app&quot; {\n  name = &quot;snowiow/example-app&quot;\n}\nNow we create a small shell script, which will be executed as soon as the\ninstance is started.\necho &quot;Update YUM&quot;\nsudo yum -y update\n \necho &quot;Install Docker&quot;\nsudo yum install -y docker\n \necho &quot;Start Docker&quot;\nsudo service docker start\n \necho &quot;Login to ECR (your Docker Registry)&quot;\n$(aws ecr get-login --no-include-email --region eu-central-1)\n \necho &quot;Start docker container&quot;\ndocker run \\\n  -p 80:80 \\\n  --env &quot;MONGO_DSN=${mongodb_dsn}&quot; \\\n  --env &quot;MONGO_DB=app&quot; \\\n  ${container_img_url}\nBasically this installs docker, downloads the image from the container\nrepository URL, where I uploaded it and executes it. As environment variables I\ngive the container the DSN of the MongoDB cluster, the container repository URL\nand the database, which will always be called app, so it is hardcoded here. The\nDSN and repository URL values are interpolated, so we need a way to fill in the\nactual values. We do this by creating a template file as a data source in\nterraform:\ndata &quot;template_file&quot; &quot;user_data&quot; {\n  template = &quot;${file(&quot;user_data.sh&quot;)}&quot;\n \n  vars {\n    mongodb_dsn    = &quot;mongodb://${mongodbatlas_database_user.this.username}:${mongodbatlas_database_user.this.password}@${substr(mongodbatlas_cluster.this.mongo_uri_with_options, 10, -1)}&quot;\n    docker_img_url = &quot;${data.aws_ecr_repository.example_app.repository_url}&quot;\n  }\n}\nHere we have to do some string manipulations to get the username and password\nin the DSN as well, otherwise the app wouldn’t be able to login to the cluster.\nThe last thing we need before creating our instances, is an IAM Role. The EC2\ninstance we launch will assume this role and will have the rights provided by\nthis role. First of all we need a policy document, which says, that the EC2\ninstance is allowed to assume a role\ndata &quot;aws_iam_policy_document&quot; &quot;assume&quot; {\n  statement {\n    sid     = &quot;AllowAssumeByEC2&quot;\n    effect  = &quot;Allow&quot;\n    actions = [&quot;sts:AssumeRole&quot;]\n \n    principals {\n      type        = &quot;Service&quot;\n      identifiers = [&quot;ec2.amazonaws.com&quot;]\n    }\n  }\n}\nNow we can create the role itself\nresource &quot;aws_iam_role&quot; &quot;example_app&quot; {\n  name               = &quot;example-app-iam-role&quot;\n  assume_role_policy = &quot;${data.aws_iam_policy_document.assume.json}&quot;\n}\nBecause our EC2 instance needs to download a Docker Image from the ECR, we need\nto give the right to the role first. Therefore we create another policy\ndocument:\ndata &quot;aws_iam_policy_document&quot; &quot;ecr&quot; {\n  statement {\n    sid    = &quot;AllowECRAuthorization&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;ecr:GetAuthorizationToken&quot;,\n    ]\n \n    resources = [&quot;*&quot;]\n  }\n \n  statement {\n    sid    = &quot;AllowECRDownload&quot;\n    effect = &quot;Allow&quot;\n \n    actions = [\n      &quot;ecr:GetDownloadUrlForLayer&quot;,\n      &quot;ecr:BatchGetImage&quot;,\n      &quot;ecr:BatchCheckLayerAvailability&quot;,\n    ]\n \n    resources = [&quot;${data.aws_ecr_repository.example_app.arn}&quot;]\n  }\n}\nThis policy consists of two statements. The first one allows an action to\nauthorize ourselfs with the ECR, which we do in the user_data.sh script with\nthis statement:\n$(aws ecr get-login --no-include-email --region eu-central-1)\nAs a resource * was chosen, because the ecr:GetAuthorizationToken action is\nglobal and can’t be restricted to a specific resource.  The second statement\nallows the download of the image. Here we defined the ARN of the specific\nrepository, we want to pull from, because we don’t want to allow our EC2\ninstance to pull from every repository we have in our account.\nWith this policy document, we can create an actual policy from it, which will\nbe attached to our example-app-role:\nresource &quot;aws_iam_policy&quot; &quot;ecr&quot; {\n  name        = &quot;ExampleAppECRAccess&quot;\n  description = &quot;Gives right to get an ECR authorization token and pull images&quot;\n  policy      = &quot;${data.aws_iam_policy_document.ecr.json}&quot;\n}\n \nresource &quot;aws_iam_role_policy_attachment&quot; &quot;ecr&quot; {\n  role       = &quot;${aws_iam_role.example_app.name}&quot;\n  policy_arn = &quot;${aws_iam_policy.ecr.arn}&quot;\n}\nBecause we can’t attach a role directly to an EC2 instance, we need an instance\nprofile:\nresource &quot;aws_iam_instance_profile&quot; &quot;this&quot; {\n  name = &quot;example-app-instance-profile&quot;\n  role = &quot;${aws_iam_role.example_app.name}&quot;\n}\nFinally we create the EC2 instance itself and output the IP address, where it\nis reachable afterwards. We also need a security group for the instance, which\nbasically tells who will be able to access the instance and to who the instance\nis allowed to respond to. In this example we allow traffic from any IP over\nport 80, because this is the port where the website is hosted:\nresource &quot;aws_security_group&quot; &quot;this&quot; {\n  name   = &quot;sg&quot;\n  vpc_id = &quot;${aws_vpc.this.id}&quot;\n \n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = &quot;tcp&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n \n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = &quot;-1&quot;\n    cidr_blocks = [&quot;0.0.0.0/0&quot;]\n  }\n}\n \nresource &quot;aws_instance&quot; &quot;this&quot; {\n  ami                  = &quot;${data.aws_ami.amazon_linux.id}&quot;\n  instance_type        = &quot;t2.micro&quot;\n  subnet_id            = &quot;${aws_subnet.this.id}&quot;\n  user_data            = &quot;${data.template_file.user_data.rendered}&quot;\n  iam_instance_profile = &quot;${aws_iam_instance_profile.this.name}&quot;\n  security_groups      = [&quot;${aws_security_group.this.id}&quot;]\n}\n \noutput &quot;example_app&quot; {\n  value = &quot;${aws_instance.this.public_ip}&quot;\n}\nThis is it! If you execute terraform apply again, you will see the public IP\nof the EC2 instance. To see if the connection to the MongoDB is working we can\nplay around with the example application.\nThe example application is a small website with two routes. Both use GET\nparameters. The first route looks like this:\n/insert/{firstname}/{lastname}\n\nThis route will insert a user with a first and lastname into the MongoDB. For\nthe example of will willson we combine the returned IP of the terraform script\nwith the route like this\n&lt;ec2ip&gt;/insert/will/willson\n\nthe response would look like this:\n\nTo really see if the user was inserted into MongoDB, we use the second route,\nwhich looks like this:\n/{lastname}\n\nBasically if we insert the lastname as the only part of the route, the example\napplication searches for users with this lastname and will print them. For the\nexample of willson the URL would look like this:\n&lt;ec2ip&gt;/willson\n\nand we get this response\n\nThis application is pretty basic but works as a proof of concept of our\ninfrastructure.\nSo there you have it. A VPC peering connection between MongoDB Atlas and our\nown AWS VPC, which is applicable in a single command, thanks to terraform. I\ncreated this post because there are many useful resources scattered around\nabout this topic, but there is no single resource which combines them all\ntogether. So I hope this is of help for anybody who is trying to achieve\nsomething similar."},"posts/Emacs/Relative-Line-Numbers-with-Emacs-built-in-Features":{"slug":"posts/Emacs/Relative-Line-Numbers-with-Emacs-built-in-Features","filePath":"posts/Emacs/Relative Line Numbers with Emacs built in Features.md","title":"Relative Line Numbers with Emacs built in Features","links":[],"tags":["emacs"],"content":"I’ve been a vivid evil citizen in the world of Emacs for many years now. One of the must-have settings in my opinion is, when using VIM keys for movement: Relative Line Numbers. It helps tremendously with navigating multiple lines up and down, because you always see the amount of rows you need to move.\nThis sounds a bit cryptic, so let’s visualize this with an example. Here is a code view with absolute line numbers:\n\nLet’s say we are on line 1373 and want to change a type in line 1396. We could just mash j a bunch of times (23 to be exact) or we could input 23j once and be there right away. But with absolute line numbers it’s not always easy to find out the number of lines you need to move, without doing some math in your head.\nThis is where relative line numbers come into play:\n\nAgain we want to move 23 lines down. Do you see the difference? You only need to read the number where you want to go and input it before j (down) or k (up). No need to subtract two numbers from each other in your head anymore. When skimming through code you do this kind of movement several times in a matter of minutes. Every time you just need to look at the number instead of doing mental arithmetic. This adds up and will save you a lot of time over the day.\nNow that we know that this feature is useful, how do we get it in Emacs? Back in Emacs 23.1 linum-mode was introduced, which brought the possibility to have line numbers in a column to the left of the text. Before this Emacs showed you the line number you were in, in the mode-line at the bottom. But many modern editors made the number column left to the text a mainstream feature and so Emacs wanted to offer it as a feature as well.\nTo get relative line numbers with linum-mode you had to install a third party package called linum-relative, because the package itself never included it.\nFast forward almost exactly 14 years (Emacs 23.1 released on 2009-07-29) and linum-mode became obsolete with 29.1 (Release Date: 2023-07-30). The reason was that it was implemented in a hacky way and it made Emacs really slow when large files were open. Where do we go from here?\nIntroducing display-line-numbers-mode: This was a new built-in package shipped with Emacs 26 (Release Date: 2018-05-28) which should offer the same functionality as linum-mode without slowing down on large files. Basically linum-mode but fast. With Emacs 29, display-line-numbers-mode is not only an alternative to linum-mode, but its official successor.\nAnd the best thing: it supports relative line numbers out of the box. That’s what got me excited. Solving an issue with built-in features instead of having to install more packages. Here is what we need to do:\n(setq-default\n    display-line-numbers-type &#039;relative)\n(global-display-line-numbers-mode)\nand at the same time we can get rid of this:\n(global-linum-mode t)\n(use-package linum-relative\n  :custom\n  (linum-relative-backend &#039;display-line-numbers-mode)\n  :config\n  (linum-relative-global-mode))\nThis gives you the same exact functionality in Emacs 29 and above with the new built-in display-line-numbers-mode package as linum-mode + linum-relative gave you. Another great feature display-line-numbers-mode offers is to set display-line-numbers-type &#039;visual instead of &#039;relative. This gives you relative line numbers based on visual lines instead of buffer lines. This means if a line is so long that it continues on the next line of your buffer (which is your view where the file content is shown), &#039;visual counts this as two lines instead of one, which &#039;relative would do. This makes navigation even more precise, especially if you work on code or text with long lines a lot. That’s why I will probably also move to &#039;visual.\nI hope this quick tip is of help to you. I was first thinking of introducing a new category bits with smaller bite-sized experiences than the normal blog post size of my past postings. But looking at the text now and seeing what I ended up with, I will continue to have these also under the normal posts."},"posts/Linux/Build-your-own-Git-Server-with-a-Raspberry-Pi":{"slug":"posts/Linux/Build-your-own-Git-Server-with-a-Raspberry-Pi","filePath":"posts/Linux/Build your own Git Server with a Raspberry Pi.md","title":"Build your own Git Server with a Raspberry Pi","links":[],"tags":["linux"],"content":"In this post I want to go into a topic, which I did a few months ago. My goal\nwas to get some of my data away from big cloud hosting platforms back into my\nown control. The initial spark was a news about a french artist, who had all\nhis pictures on googles platform blogger. His blog was deleted out of no reason\nand his whole work was lost. You can read the whole story\nhere.\nScared of a similar scenario, I decided to setup my own cloud with seafile and\nmy own Git server. In this post I will cover how to put up a simple Git server\nwith a basic web fronted to view the repositories. I still use GitHub pretty\nextensively but all my little and non private projects go onto my own Git\nserver now. First things first, we need some hardware to get started:\n\nA Raspberry Pi 3 Model B (older models should be sufficient too, but the newer the more performance you got at your hand)\n5V Micro USB Plug\nA Micro SD Card (the bigger, the better)\nA Raspberry PI 3 Case\nAnother Computer to store an OS image on the SD Card (This guide shows the Linux way, but it should also be possible on Windows and OSX)\nAn ethernet cable to initially connect your Raspberry Pi to your router\n\nI also bought an external hard drive where all my Git repositories and cloud\nstuff is stored, but you can also buy a big SD-card and store the repositories\non it directly. In this guide I will go about the default SD Card way. If you\nwant to use an external hard drive, you need to format it to sda4 for example\nand mount it to the point, where you want to have it. All your Git repository\ndirectories should go there and the mount point will be your Git root.\nOtherwise the procedure is the same as given in this guide.\nOne last thing on the notations in this post. Because we need to execute many\ncommands from the command line, I will use a short notion for which commands\nhave to be executed as which user. If a line starts with the $ sign, the\ncommand is executable by a normal user. If you see a # in the beginning,\nyou need to either execute the command with the sudo utility or be logged in\nas the root user.\nInstalling the OS on the SD-Card\nI decided to use Arch Linux ARM for the Raspberry Pi, but you are free to use\nanything you want here. The reason why I chose Arch is because it’s a very\nminimal distribution. That said it only installs relevant software and from\nthere you are free to go. A warning up front: We will never get to see a GUI,\nlike in Raspbian or another Raspberry Pi distribution. On the plus side of\nthings is, you will have much more RAM and processor performance for the\nsoftware that really matters. My Raspberry Pi with Seafile and hosting the Git\nsite on an nginx server is using around 200Mb/1GB of RAM and about 2% of one of\nthe 4 processors on average. So there is plenty of stuff my Raspberry could do\nas well, without getting overwhelmed.\nBut let’s get back to the installation: First tricky thing here is to use the\nARM7 Version. Even through the Raspberry Pi is listed as ARM8 I got huge\nproblems to get my wireless up and running. So go and get the Arch Linux ARM7\nhere.\nAfterwards you need to follow the installation instructions\nhere.\nConfiguring the OS\nBy now you should have installed Arch Linux ARM on the SD-Card, plugged it in\nand you should be logged into your Raspberry Pi (Don’t forget to connect your\nRaspberry Pi to your router if you want to connect via SSH). There were still\nsome steps I needed to do to get started with my main project. Here is a short\nlist:\n\nSet your keyboard layout\nSetup the wifi connection\nSet your timezone, locale and hostname\n\nSetting up Git\nNow we can get started with our real project. First we need to install Git:\n# pacman -S git\n\nNext we create a new Git user, who will be responsible for the repositories:\n# adduser -m -g git -s /bin/bash git\n\nThe -m flag says that we want to create a user directory in /home.\n-g is the default group of our user and -s is the default shell of the\nuser. We need to be able to do stuff as the Git user, that’s why he gets a\nshell. That’s it. Now you are basically ready to do Git stuff with your\nRaspberry Pi.\nCreate a new Git Repository\nThe next thing we want to do is create a new Git repository. Before we do\nanything Git related, we want to suspend ourselves as the Git user, like this:\n# su git\n\nNow we are the Git user and can do stuff in his name. First we need a place,\nwhere we want to place our repositories. A good place is the home directory of\nthe Git user, but it can be anywhere else, where the Git user has write access.\nNow we create a new repository called new-project:\n$ mkdir /home/git/new-project.git &amp;&amp; cd /home/git/new-project.git &amp;&amp; git init --bare\n\nFirst we create a new directory with a .git ending. You don’t need the\nending, but it’s a best practice, to let server side repositories end with\n.git. Afterwards we go into that directory and initiate it as a Git\nrepository. The bare argument is somewhat optional again, but also a best\npractice. It just means this Git repository doesn’t have a working directory.\nThe working directory is where you put your files. On the same level is a\n.git directory with the history and other things in it. The bare argument\nnow says that you don’t need the working directory and the main directory can\nbecome the .git directory. So overall you can remember to use the bare\nargument on the server side of a repository and not on the client side.  #\nClone your repo Now we want to clone our newly created repository from the\nserver. You should already  know the ip of your Raspberry, when you’ve set it\nup via ssh. If you don’t know the IP yet, you can run the command ifconfig as\nroot, which should give you something like this as an output:\n#ifconfig\neth0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500\n        ether b8:27:eb:8c:97:af  txqueuelen 1000  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n \nlo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;\n        loop  txqueuelen 1  (Local Loopback)\n        RX packets 1004803  bytes 352750188 (336.4 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 1004803  bytes 352750188 (336.4 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n \nwlan0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.178.48  netmask 255.255.255.0  broadcast 192.168.178.255\n        inet6 fe80::ba27:ebff:fed9:c2fa  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether b8:27:eb:d9:c2:fa  txqueuelen 1000  (Ethernet)\n        RX packets 1727438  bytes 275330489 (262.5 MiB)\n        RX errors 0  dropped 1561  overruns 0  frame 0\n        TX packets 4660814  bytes 2524959215 (2.3 GiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nAs you can see in the output, I am connected via wifi. So the IP address of\ninterest is under inet in the wlan0 block. If you are connected via cable,\nthere should be an inet entry in the eth0 block. Now we know where to clone\nthe repository from like this:\n$ git clone git@&lt;your-inet-entry-here&gt;:/home/git/new-project.git\nIn my case the command would look like this:\n$ git clone git@192.168.178.48:/home/git/new-project.git\nPush your first change\nLet’s add a file with\n$ touch test.txt\n\nNow you should have some changes, which you can check with\n$ git status\n\nNow we do our first commit and add the test file to our commit.\n$ git commit -a -m &quot;added test file&quot; &amp;&amp; git push\n\nCongratulations! You first commit has landed on the server.\nInitiate an existing project as a Raspberry Git repository\nWe pretty much covered everything which you need to know to work with your new\nGit server. But there is one last common use case, which I want to present to\nyou. What if you were already working on a project and want to sync it with the\nserver? First you have to switch into that directory and make it a Git\nrepository like this.\n$ cd /path/to/existing-project &amp;&amp; git init\n\nSame thing on your Raspberry Pi. Create the directory with a .git ending and\ninit it with the bare argument (remember to do this as the Git user)\n$ cd /home/git/existing-project.git &amp;&amp; cd /home/git/existing-project.git &amp;&amp; git init --bare\n\nAfterwards your project files can be added to an initial commit like this (this\nhappens on your work station again)\n$ git commit -a -m &quot;my first commit&quot;\n\nSo far there is nothing special. But now we add our Raspberry Pi as the remote server\n$ git remote add origin git@&lt;your-inet-entry-here&gt;:/home/git/existing-project.git\n\nAgain in my case the command would look like this and you have to change it to\nyour case accordingly\n$ git remote add origin git@192.16.178.48:/home/git/existing-project.git\n\nNow you can push your fist changes and set the Raspberry Pi as the default origin like this\n$ git push --set-upstream origin master\n\nYou are also able to name origin anything you want. Origin is just a\nconvention which is used for the default upstream. This was the main part,\nwhich should get you up to speed with Git on a Raspberry Pi. In the next\nchapter I want to show you, how to set up a simplistic Git web frontend to\nwatch your repositories, commits and diffs in the web browser.\nCreate a Website to view your repositories\nThe feature we are are using is the built in gitweb. It’s a minimal web\nfrontend to view your code and you should already have seen sites like this\nhere and there. For example the project overview of my dotfiles repository\nlooks like this:\n\nFirst of all we need a running webserver, to host our Git site. We are going\nfor nginx in this tutorial, but it’s also possible to use\napache. To install nginx, we run\n# pacman -S nginx-mainline\n\nAfterwards we also need to install perl-cgi\n# pacman -S perl-cgi\n\nNow we start the nginx service via systemd\n# systemctl start nginx\n\nYou can check if the nginx server is running by looking up the ip address of\nyour Raspberry Pi in your browser. In my case this would be navigating\nto 192.168.178.48. You should see a success message, that the nginx server is\nrunning. Because gitweb comes preinstalled with Git, we can directly go into\nactivating the gitweb cgi script. Therefor we need to edit\nthe /etc/nginx/nginx.conf\nserver {\n    listen       80;\n    server_name  localhost;\n \n    #charset koi8-r;\n \n    #access_log  logs/host.access.log  main;\n \n \n    #error_page  404              /404.html;\n \n    # redirect server error pages to the static page /50x.html\n    #\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n \n    location /gitweb.cgi {\n            include fastcgi_params;\n            gzip off;\n            fastcgi_param   SCRIPT_FILENAME  /usr/share/gitweb/gitweb.cgi;\n            fastcgi_param   GITWEB_CONFIG  /etc/gitweb.conf;\n            fastcgi_pass    unix:/var/run/fcgiwrap.sock;\n    }\n \n    location / {\n        root /usr/share/gitweb;\n        index gitweb.cgi;\n    }\n    ...\n}\nAs you can see in the code snippet, you need to add the gitweb.cgi location\ninto the server entity and tell nginx where to find the fastcgi_params and\npass. You also need to override the default location to be gitweb.cgi. All\nchanges you need to do, are highlighted in yellow. Now we need to install\nfcgiwrap. This is a cgi server, which runs our gitweb cgi app.\n# pacman -S fcgiwrap\n\nThen we need to add a service config for fcgiwrap to be able to start it via\nsystemctl. Create the following\nfile _/usr/lib/systemd/system/fcgiwrap.service _with this content\n[Unit]\nDescription=Simple server for running CGI applications over FastCGI\nAfter=syslog.target network.target\n \n[Service]\nType=forking\nRestart=on-abort\nPIDFile=/var/run/fcgiwrap.pid\nExecStart=/usr/bin/spawn-fcgi -s /var/run/fcgiwrap.sock -P /var/run/fcgiwrap.pid -u http -g http -- /usr/sbin/fcgiwrap\nExecStop=/usr/bin/kill -15 $MAINPID\n \n[Install]\nWantedBy=multi-user.target\nNow we can enable and start the fcgiwrap server for nginx\n# systemctl enable nginx fcgiwrap\n# systemctl start nginx fcgiwrap\n\nHold with me, we are already done. The server is now able to run the gitweb cgi\napp on the root directory of the nginx server. The last thing, we need to do is\ngiving gitweb the root directory, where it can find our repositories and set\nthe URL according to the Git IP, which we used earlier. To configure these\nthings we open up /etc/gitweb.conf and set the content like this:\nour $git_temp = &quot;/tmp&quot;;\n\n# The directories where your projects are. Must not end with a slash.\nour $projectroot = &quot;/home/git&quot;;\n\n# Base URLs for links displayed in the web interface.\nour @git_base_url_list = qw(git://192.168.178.48 http://git@192.168.178.48);\n\n#Syntax Highlighting\n$feature{&#039;highlight&#039;}{&#039;default&#039;} = [1];\n\nThe tmp folder can stay as it is. The project root is the home directory of our\nGit user. You need to edit the Git URLs according to your Raspberry’s IP\naddress, if it differs to mine. The last option enables highlighting of the\nshown source code on the Git web site, which looks like this\n\nA final restart is required for the changes to take effect\n# systemctl restart nginx\n\nNow you can navigate the IP of your Raspberry Pi again, like when you were\nchecking if the nginx server was running. You should now see your very own\ngitweb frontend."},"posts/Linux/Increase-disk-space-of-a-vagrant-machine":{"slug":"posts/Linux/Increase-disk-space-of-a-vagrant-machine","filePath":"posts/Linux/Increase disk space of a vagrant machine.md","title":"Increase disk space of a vagrant machine","links":[],"tags":["vagrant","linux"],"content":"Lately I came across a rather big problem of vagrant: Increasing the disk\nspace, because the normal 40GB were in use. There are options to increase RAM\nor CPU count easily. You can change it like this:\nconfig.vm.provider &quot;virtualbox&quot; do |vb|\n    vb.memory = 4096\n    vb.cpus = 4\nend\nSo my thoughts were, that there must be a similar option to disk space as well,\nbut there wasn’t. After some research I found out, there was no easy solution\nto that. There are already some solutions though, but nothing worked for me\ncompletely. I’m currently mainly using kaorimatz ubuntu 16.04\niso. One point\nwhich wasn’t working for me is, that other machines used lvm2 as a volume\nmanager. Second problem was, that some people’s new increased hdd’s got\nattached automagically to the vagrant machine, even though it should give an\nUUID conflict. So I had to go for my own solution, which is a combination of\nvarious solutions out there. I hope this is of any use for someone else out\nthere, who was as frustrated as I was.\nThe Solution\nFirst things first: This solution is for machines, which use\nVirtualBox as their virtualisation platform. If you use VirtualBox, we can go\non. The first thing you have to do is turn your running vagrant machine off\n(vagrant halt or vagrant suspend). Next we need to find the actual VMDK\nfile, which represents the HDD of the machine. If you have installed VirtualBox\nwith the standard procedure, your machine should be here:\n/home/&lt;your username&gt;/VirtualBox\\ VMs/&lt;name of the vm&gt;/&lt;name-of-the-disk&gt;.vmdk\n\nVMDK is a type, which can’t be resized. So, after we switched into the\ndirectory, the first thing we will do is converting it to VDI\nVBoxManage clonehd &lt;name-of-the-disk&gt;.vmdk tmp-disk.vdi --format vdi\n\nNow we are able to increase it\nVBoxManage modifyhd tmp-disk.vdi --resize &lt;size-in-MB&gt;\n\nAs an example, if we want to have 60GB as the new size, the command would look\nlike this\nVBoxManage modifyhd tmp-disk.vdi --resize 61440\n\nNow we convert it back to VMDK\nVBoxManage clonehd tmp-disk.vdi resized-disk.vmdk --format vmdk\n\nThe next thing we need to do, is closing the SATA controller to the old disk\nVBoxManage storageattach &lt;name-of-the-vm&gt; --storagectl &quot;IDE Controller&quot; --port 0 --device 0 --medium none\n\nHere are two things, you need to take care of. First thing is\n. This is the same as the directory name, where the initial\nVMDK file is placed. The second thing is, that your storagectl could have\nanother name. You can find it out by opening the *.vbox file, which is in the\nsame directory as the initial VMDK file. This should be a normal XML file. Now\nlook out for this tag\n&lt;StorageControllers&gt;\n  &lt;StorageController name=&quot;IDE Controller&quot; type=&quot;PIIX4&quot; PortCount=&quot;2&quot; useHostIOCache=&quot;true&quot; Bootable=&quot;true&quot;&gt;\n    &lt;AttachedDevice type=&quot;HardDisk&quot; hotpluggable=&quot;false&quot; port=&quot;0&quot; device=&quot;0&quot;&gt;\n      &lt;Image uuid=&quot;{42b28bc6-a130-45be-9603-ee16779459d5}&quot;/&gt;\n    &lt;/AttachedDevice&gt;\n  &lt;/StorageController&gt;\n&lt;/StorageControllers&gt;\nAs you can see, we are interested in the name of the StorageController tag.\nIf there is something else than “IDE Controller”, you need to use that name for\nthe —storagectl flag instead. Now we need to close the old medium\nVBoxManage closemedium disk &lt;name-of-the-disk&gt;.vmdk\n\nWhen it is closed, we can attach our new resized VMDK\nVBoxManage storageattach &lt;name-of-the-vm&gt; --storagectl &quot;IDE Controller&quot; --port 0 --device 0 --type hdd --medium resized-disk.vmdk\n\nHere you need to watch out for the same things, as in the last storageattach\ncommand. Now you should be able to restart your vagrant machine (vagrant\nup).  Afterwards SSH into it (vagrant ssh). Now you can check your disk\nspace with\ndf -h\n\nIf there is your new size already, you are done. But if you are like me and\nstill have the old size, we need to do some more steps. Namely we need to\ncreate a partition with the new space and resize our filesystem to that space.\nThe following images are used from my example of increasing the disk from 40GB\nto 60GB, but there shouldn’t be a difference in doing the same for other sizes.\nFirst we need to search for our HDD\nsudo fdisk -l\n\nThere are some USB drives and the last entry should look like this\n\nHere we are interested in what comes after Disk. This should always be\n/dev/sda. If not, you continue using that name instead of /dev/sda. Next\nexecute\nsudo fdisk /dev/sda\n\nYou should have entered the fdisk prompt now, which looks like this\n\nNow type in p and Enter to execute. This should call the current partition\ntable\n\nHere /dev/sda1 should be the boot partition, /dev/sda2 is the swap\npartition and /dev/sda3 is the actual partition, which should be resized. If\nyours isn’t /dev/sda3, change the following commands to your partition name.\nFirst we delete the current /dev/sda3 partition by typing in d and Enter.\nNow you should get to choose which one. We want to delete the 3. So we type in\n3 + Enter.\n\nNow we create a new one with n + Enter. Next we choose p for a primary\npartition. The new partition number will be 3 again and the next two questions\nfor first and last sector will just be accepted with the default value by\npressing Enter.\n\nAs you can see we now have a new partition 3 with size 58.8 GB. To save the\nchanges, we have made so far, type in w + Enter. This will result in an error\nlike this\n\nAs the messages tells us, the changes can’t be applied until a restart. That’s\nwhy we do exactly that. We leave the machine via exit and execute:\nvagrant reload\n\nAfter a few moments you should be able to re-SSH into your vagrant machine. The\nlast thing we need to do is resize our file system to the new partition size.\nsudo resize2fs /dev/sda3\n\nIf you do\ndf -h\n\nagain, you should now see the new size you’ve always wanted."},"posts/Linux/Next-level-dotfiles-with-Ansible":{"slug":"posts/Linux/Next-level-dotfiles-with-Ansible","filePath":"posts/Linux/Next level dotfiles with Ansible.md","title":"Next level dotfiles with Ansible","links":[],"tags":["linux"],"content":"In the life of every linux enthusiast comes the point, where he has a lot of\nindividual configuration build up and wants to migrate these settings over to\nanother machine as seamlessly as possible. At this point a whole philosophy of\nhow to do this is coming into play. Today this philosophy is bundled under the\nname: dotfiles. Historically dotfiles are the config files, which lie\nin your home directory. Because they try to be invisible to the user, they start\nwith a dot (which is the linux convention for invisible files).\nI started the classical approach like many others and kept a git\nrepository with all the important dotfiles\nI want to share between my machines. A shell script lies in the root directory,\nwhich symlinks everything inside the dotfiles sub directory to the home\ndirectory. In the beginning this was great. All my important applications, like\nvim, were configured after executing the shell script. From that point on I\ncould change my configuration files, which were synced back to the git\nrepository. When I was at a new consistent state, which I wanted to share, I\npushed it to the repository. Afterwards I could pull the changes onto my other\nmachines. My main concern with this approach arised when more and more\napplication implemented the\nxdg-user-dir. Now\nmost of the dotfiles were located in my .config directory and the symlink\nlinked to ~/.config itself. Now I had to differentiate a lot between files I\nwant to track and those I don’t, before every commit. Long story short: I found\na new and better approach, which I want to present here:\nAnsible.\nYou might think now: “Ansible? Really? That’s total overkill for syncing\ndotfiles!” That’s true, but on the other hand it’s much more than just for\nsyncing my dotfiles. But first of all a small introduction for everyone, who\nnever heard about Ansible before: Ansible is a project written in Python, which\nhelps you create reproducible systems. The normal use case for Ansible is as a\nconfiguration manager. Ansible projects mostly consist of YAML files, in which\nyou define a want to state of a system. Then you apply it to a system and\nAnsible syncs everything between the system and the defined state of your YAML\nfiles. So in many ways it can be compared to Terraform, which I talked a lot\nabout in my previous posts. But I see Ansible on a level lower than\nTerraform. While Terraform is about the big picture representing your whole\ninfrastructure, consisting of many virtual Servers and other cloud services,\nAnsible takes care about each and every system in this infrastructure. So it is\nmore like docker, but you install and configure stuff on the system itself. Ok,\nthis should be enough of an intro to Ansible. Also because we don’t use it in\nit’s intended way here…\nLet’s dive into my totally over engineered dotfiles!\nGetting a base structure built up\nNormally you provision other systems from one operator system, mostly via SSH.\nBut we want to use Ansible on the current system. Sure it would also be cool,\nto provision a new system from some sort of main machine, but in reality it was\nnever necessary for me. So the first thing we do, is to tell Ansible, we want to\nexecute everything on localhost. Therefore we create a\nhosts file in our new\ndotfiles directory, with the following line of code:\n[local]\nlocalhost ansible_connection=local\nThis will tell Ansible which hosts to provision.\nNow we need to talk about two important concepts of Ansible. The first one are\nplaybooks. Playbooks are Ansible’s infrastructure as code so to say. They\ndemonstrate the want to be state of a system and are described in a descriptive\nmanner in form of YAML files. The Ansible docs describe playbooks as the\ninstruction manual for configuration, deployment and orchestration.\nThe second important concept are roles. Ansible roles are a form of structuring\nyour playbooks and make them reproducible. We mainly use roles to structure our\ndotfiles and to keep different application configurations separated. For example\nI reuse my roles in different forms for my Linux and OSX machine, because the\ninstallation process is different, but the symlinking of the dotfiles is the\nsame. Roles dictate a special directory structure. This way Ansible is able to\nmagically run those roles without setting too much manually. I will go into more\ndetail, when we create our first role. All roles are located in the roles sub\ndirectory.\nCreate your first role\nAs an example we install neovim, but this can be any software you want to always\nhave installed. For this, we create a nvim sub directory in the roles\ndirectory. In the nvim sub directory we create a tasks and files\ndirectory. The current directory structure should look like this:\n\nIn the tasks directory we create a file called main.yml. This file is the\nstarting point of a role and will be called automatically, similar as the main\nfunction in various programming languages. Here we add the following\ninstruction:\n- name: install neovim on linux\n  package:\n    name:\n      - neovim\n      - python-pynvim\n    state: present\n  become: yes\nBasically everything in playbooks is defined in these blocks. They follow a\ncommon structure starting with an optional name, which is printed during\nexecution. This is helpful to follow what’s currently done. Then the following\nkey specifies a module, which does some sort of operation. There are many\nprebuild\nmodules,\nwhich are shipped with Ansible. Of course you can also write your own\nmodule,\nbut the amount of choice is so overwhelming, that I never ran into this\nsituation.\nThe package\nmodule\ndoes the operation of downloading a package with your systems package manager\nfor example. If you execute this on an Arch Linux machine, a package is\ndownloaded via Pacman. If you execute this on Mac OSX, homebrew is used. There\nare also the respective modules for\nPacman\nand\nhomebrew,\nif you already know that you execute this playbook on this specific OS only.\nEvery module has some mandatory and optional parameters. See the package\nmodule\nfor example. These are set inside of the package block. The first parameter\nname takes one or more names of packages. The second parameter state tells\nif the packages defined under name should be present or absent. Also there is\nan optional parameter use, where you can define which package manager to use.\nIn our example we omit this parameter, which results in Ansible searching for\nthe correct one to use on the current system.\nNow you can also see why I was talking about the declarative nature of Ansible.\nWe don’t tell Ansible what and how something needs to be done. We just tell\nwhat we want to have. We want to have neovim and the python package neovim\npresent on the system. Ansible takes care about the rest. In a normal shell\nscript for example, we would need to explicitly tell, with which package manager\nwe want to download a specific package and only if this package isn’t already\ninstalled. Otherwise do nothing.\nThe last part of this snippet (become: yes) says that this block should be\nexecuted as root. This is outside of the package block, because it’s not\nspecific to this module, but can be appended to every block. In this case we\nneed to be root, because normally only root users are able to install new\nsoftware via the package manager.\nNow let’s execute this role. First we need a new YAML file, which we\nspecifically execute by Ansible. In this file we also define the roles, which\nshould be executed. Let’s create a file called roles.yml in the root directory\nwith the following content:\n- name: Setup local environment\n  hosts: localhost\n  roles:\n    - nvim\nAs always we give this block a name and set the desired host. This represents\nthe key of the hosts init file, we defined in the beginning. Ansible will take\nthe stuff under that key and execute the roles, which are defined afterwards,\nfor this host.\nNow we execute the Ansible playbook script via:\nansible-playbook --ask-become-pass -i hosts roles.yml\nThe —ask-become-pass flag needs to be given, because we have at least one\nblock, which needs root access. This causes Ansible to ask for your password in\nthe beginning. Ansible saves this password during execution and uses it every\ntime it finds the become: yes block. The -i flag specifies the inventory,\nwhere the hosts are defined. In our case we pass the hosts file. As the main\nargument we pass Ansible the roles.yml, which then executes the roles defined\nin that file. This should give you something like this output (if you already\nhave those packages installed) or a slightly different output, because Ansbible\nwill install them now. Also this should be executable by every Linux\ndistribution as long as the package names match.\n\nAs a quality of live addition I would create a shell script called\ninstall or something,\nwhich itself executes the ansible-playbook command above. The command itself\nis very daunting to type over and over again. I also use the shell script for\nexecuting the ansible-playbook with different flags on different systems and\npass environment variables, which are then used by Ansible.\nSymlink nvim dotfiles\nNow we want to symlink some dotfiles for neovim. In this example we will only\nsymlink the init.vim file. First place your init.vim into the files\ndirectory, which we created earlier in the nvim role directory. Then we add a\nnew block to the main.yml in the nvim role:\n- name: symlink init.vim\n  file:\n    src: &quot;{{ ansible_env.PWD }}/roles/nvim/files/init.vim&quot;\n    path: &quot;~/.config/nvim/init.vim&quot;\n    state: link\nBecause you now understand Ansible playbooks, you can easily read the above\nblock. First we have a name again, which tells us to symlink the init.vim file.\nThen we make use of the file module of Ansible. As the source we give it the\npath to our init.vim file. As the path we give it the destination. As the\ndesired state we want to have a link between the source and path.\nExecuting the playbook again, will now also create the symlink and nvim should\nnow have the settings you normally use. When you do changes to your config and\nthe ansible project is checked into git, you can now automatically sync these\nchanges with your other machines. On the other machines you just clone the\nrepository, execute ansible once and from then on all the files, which are\nsymlinked, are synced automatically. You only need to execute Ansible again, if\nyou add new software, which needs to be synced as well between your machines.\nOf course there are also loops to symlink more than one file and directory. You\ncan look it up in my current nvim\nplaybook.\nI also have a lot of other parts automated, like downloading a vim package\nmanager and downloading all the defined packages from my init.vim. I won’t\ncover everything here, because this should only be a small overview of how I\nhandle my system setup. If you are curios, you can check my ansible dotfiles\nrepository out and look around yourself. You\nnow should be able to read and understand everything in this repository because\nthe concepts, described before are repeated over and over again.\nHandle different kinds of systems\nIn this last part I want to present how I use one role for different systems.\nThe most common example I ran into was Pacman requiring root and homebrew\ndenying being executed as root.\nFor this case I created two files install_dawin.yml and install_linux.yml\nin which I defined the different instructions for installing neovim on Linux\nand on Mac OSX. My Linux version looks like the main.yml from before and my\nMac Version looks like this:\n- name: install neovim on darwin\n  homebrew:\n    name: neovim\n    state: present\n \n- name: install python-neovim on darwin\n  pip:\n    name: neovim\nAs you can see I need to get python-neovim from pip this time, because it isn’t\navailable in homebrew. But even big differences like these are easy to handle\nwith splitting some blocks in OS specific files.\nThe main.yml of the role got changed to this:\n- import_tasks: install_darwin.yml\n  when: ansible_facts[&#039;os_family&#039;] == &quot;Darwin&quot;\n- import_tasks: install_linux.yml\n  when: ansible_facts[&#039;os_family&#039;] == &quot;ArchLinux&quot;\n- import_tasks: config.yml\nHere I import the install tasks based on the OS family. Ansible collects some\nfacts about the system, it’s running on in a dictionary. This can be accessed\nlike in the code snippet above. Afterwards I import the config task, which\nexecutes everything to configure neovim to my liking. The configuration is\nindependent of the underlying OS and can be executed in any case.\nConclusion\nFirst of all these are the basics of some of my system setup. There is a lot\nmore stuff to discover. But with the things I have shown you, it\nshould be easy to pick up for your dotfiles as well.\nAlso big thanks to Greg Hurrel\nfrom whom I got the base idea of using Ansible for my system setup. Also his\ndotfiles are a lot more sophisticated than\nmine. Definitely take a look at those as well. I use it time and time again to\nlook up stuff and use them in my dotfiles. However I think it’s important to\nstart like described in this post, to really create your own spin of an Ansible\ndotfiles repository. It’s totally okay to copy over things, but I think you\nshouldn’t start with mine or his dotfiles as a basis, because this will force\nyou in a corner, which maybe isn’t the way you wanted to originally design your\ndotfiles repository.\nI download Ansible from the package manager of my OS, clone the repository and\nexecute the install shell script. I decided not to pull in Ansible as a third\nparty dependency in the repository because:\n\nIt’s basically available on any systems package manager anyway and therefore\neasy to install\nYou depend again on other python dependencies, which are needed by Ansible\n\nEspecially reason 2 is very problematic, because if you use the universal way of\ninstalling python packages via pip, you get problems with your systems package\nmanager. If other installed packages have the same dependencies as Ansible does,\nyour Package manager won’t notice that the dependency was already installed\nthere via pip and will fail while trying to override the files. Trying to solve\nthis issue by using virtual environments brought even more problems to the\ntable, because I needed even more dependencies. That’s why I went with just\ndoing\nsudo pacman -S ansible\nor\nbrew install ansible\ninstead of pulling the whole dependency hell into my dotfiles repository.\nThe last thing I want to mention is that this is only one way of managing system\nsetup/dotfiles. If you just want to symlink your stuff and nothing else, most\nprobably then this isn’t worth the effort. However if you also want to automate\nother stuff of your system setup and aren’t comfortable with one of the many\noptions out there, you should definitely give Ansible a chance. It’s very easy\nto pick up and thanks to the documentation it’s easy to discover new things\nfast. Also creating something like this (which has a practical use case for\nalmost all of us) gives you a good practical entry point in how to use Ansible\nand gives you the knowledge to dig deeper and do even more with it. Ansible\ntoday is still a very needed skill in the DevOps space.\nThanks for reading. I’m happy to hear your approaches for your system setup or\nhow you handle specific things with Ansible."},"posts/Linux/Setup-pass-on-multiple-devices":{"slug":"posts/Linux/Setup-pass-on-multiple-devices","filePath":"posts/Linux/Setup pass on multiple devices.md","title":"Setup pass on multiple devices","links":[],"tags":["linux"],"content":"I recently started using a wonderfully simple password manager called\npass. I wanted to check it out for a long time,\nbut now I finally got some time and incentive to finally do so. Even though the\nweb- and manpage are very helpful, I had to search quite a bit around to make it\nwork between my phone and Laptop, so I wanted to streamline the process in this\npost. We will setup pass on an Arch Linux machine first and then sync our existing\npassword store to a MacBook and an Android Phone. So without further ado let’s\nget started!\nInstalling pass\nInstalling pass should be as easy as:\nyour-package-manager install pass\nfor example in Arch Linux it’s just:\nsudo pacman -S pass\npass should be available in every major package manager.\nBefore we actually initiate the password store, we need a gpg key which is used\nto encrypt our passwords. To do this, we proceed as follows:\ngpg --full-gen-key\nAfterwards you get a couple of questions, which you can mostly answer with the\ndefaults. The only exception I made, was to use the biggest keysize\npossible. Here is the full dialogue:\nPlease select what kind of key you want:\n   (1) RSA and RSA (default)\n   (2) DSA and Elgamal\n   (3) DSA (sign only)\n   (4) RSA (sign only)\n  (14) Existing key from card\n(ins)Your selection? 1\nRSA keys may be between 1024 and 4096 bits long.\n(ins)What keysize do you want? (3072) 4096\nRequested keysize is 4096 bits\nPlease specify how long the key should be valid.\n         0 = key does not expire\n      &lt;n&gt;  = key expires in n days\n      &lt;n&gt;w = key expires in n weeks\n      &lt;n&gt;m = key expires in n months\n      &lt;n&gt;y = key expires in n years\n(ins)Key is valid for? (0) 0\nKey does not expire at all\n(ins)Is this correct? (y/N) y\n \nGnuPG needs to construct a user ID to identify your key.\n \n(ins)Real name: Your Name\n(ins)Email address: your.name@address.com\n(ins)Comment:\nYou selected this USER-ID:\n    &quot;Your Name &lt;your.name@address.com&gt;&quot;\n \n(ins)Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O\nIf everything worked out fine, you should now see your gpg key with:\ngpg --list-keys\nOutput:\n/home/snow/.gnupg/pubring.kbx\n--------------------------------\npub   rsa4096 2021-04-11 [SC]\n      0abcE883def59ghi422Dsggo1226C853FF98C17\nuid           [ultimate] Marcel Patzwahl &lt;my.name@address.com&gt;\nsub   rsa4096 2021-04-11 [E]\nNow we are able to create our password-store with:\npass init 0abcE883def59ghi422Dsggo1226C853FF98C17\nHere we explicitly pass the gpg key id to pass (this post will probably contain\na lot of these puns). This way it’s set as the default key and we don’t get\nasked about the gpg key each time we encrypt/decrypt a password. If you forget\nabout passing the key id initially, you can also do this on an already existing\npassword-store. Then the gpg key is just applied to the existing store.\nFinally we want to activate git for our password-store:\npass git init\nNow our password-store is a git repository and every change will be a git\ncommit (examples follow later). git will be our primary tool to sync pass\nbetween multiple devices and it actually works pretty well with the automatic\ncommits pass does for us. But before we go into the synchronization, let’s get\nto know the basic usage of pass a little better.\nBasic Usage of pass\nEven though this isn’t an intro into the pass cli itself, we want to go through some\nexamples just to get a feel for this tool.\nCreate an existing password\npass insert test\nThis command will ask you twice about an existing password. This password is\nthen stored as a file in the .password-store directory, which is encrypted via\nour gpg key we created earlier.\nWe can also store our passwords in a directory tree structure if we split the\nnames with a slash:\npass insert mail/test.com\nThis creates a mail directory in the .password-store directory and puts a\ntest.com file in it. This file again is encrypted with our gpg key.\nCreate a password with more metadata\nOften we don’t want to only store a password, but also some additional\ninformation like the username and the URL to which the password belongs. This\nis also pretty straightforward. We just pass the -m flag for a multiline\npassword. Let’s try this by overriding the last password we added earlier:\npass insert -m mail/test.com\nAn entry already exists for mail/test.com. Overwrite it? [y/N] y\nEnter contents of mail/test.com and press Ctrl+D when finished:\nmy-secret-password\nurl: test.com\nusername: snow\nFirst we accept to override the existing password and the we are able to write\nmultiple lines. The first line is always only the password, followed by an\narbitrary number of key: value pairs, splitted by a colon. When you are done\npress CTRL+D to save and exit. As we will see later, these additional fields\ncan be used by different pass clients, to directly open a url or also insert the\nusername in a form of a website.\nGenerate a new password\nYou can also generate new passwords like this:\npass generate generated-password\nOutput:\nThe generated password for generated-password is:\n6y&lt;a\\+57Eh&amp;@L@@O6ax|MK&quot;;{\nYou can also pass -n to the generate command to generate a password without\nsymbols and add a number after the name to set the length of the generated\npassword.\nShowing and copying passwords\nTo show the test password we created earlier, you call:\npass show test\nor only:\npass test\nTo also copy the password into your clipboard you pass the -c flag.\nShow all passwords in the password store\nThis is:\npass list\nor just:\npass\nOutput:\nPassword Store\n├── generated-password\n├── mail\n│   └── test.com\n└── test\nAs you can see, this prints a nice tree of all the passwords we have created in\nthis section.\npass git commands\npass is tightly integrated with git. You could already see this when executing\none of the commands before. pass is always automatically creating a\ncommit with a meaningful message for every operation we did. For example:\n[master f71e270] Add generated password for generated-password.\n 1 file changed, 0 insertions(+), 0 deletions(-)\nStarting with pass git we are able to execute basically any git command on the\n.password-store directory. We will use this knowledge to setup a remote repository for\nour password store to be able to sync our passwords between different machines.\nClosing words on pass usage\nEven though we didn’t cover every possible pass command here, I hope you got a\ngrasp of how easy and straightforward it is to interact with your passwords via\nthe pass CLI. It also doesn’t get much harder from here. If you are searching\nfor another functionality, you can view the pass man pages via:\nman pass\nThe manpage describes all the commands and arguments you can pass to them in a\nvery intuitive fashion. You should also already know how pass works behind the\ncurtain by now, just by interacting with it for a bit. It’s really just a\nminimal CLI tool, which leverages other unix technologies already present like\ngnupg (gpg), git and files. pass really deserves the name “the standard unix\npassword manager” because it shows very nicely what the unix philosophy is all\nabout.\nIn the coming sections we come to the actual core of this blog post: How do we\nmanage our password-store on multiple devices? We start by creating a remote\nrepository, which will act as our single source of truth with which all our\nclients will sync their passwords.\nSync the password store with a remote repository\nFirst we need to decide where we want to host our remote repository. I decided\nto this on my Raspberry Pi. See this earlier blog post of mine, where I describe\nin detail, how to setup a Git Server with your Raspberry\nPi\nBut you could even host your password store in a public GitHub repository if you\nlike. People would see the structure of your password store, but wouldn’t be\nable to see the actual passwords, because they can’t encrypt them without your\ngpg key. However I prefer to keep it completely private, but however you decide,\nyou just need the remote url of the repository to follow along.\nWhen you have the remote url you can add it to your existing password store via:\npass git remote add origin git@address.com:snow/pass.git\nAfterwards you should see the remote with:\npass git remote -v\norigin\tgit@address.com:snow/pass.git (fetch)\norigin\tgit@address.com:snow/pass.git (push)\nIf it looks the same, but with your remote address, you are now able to push\nyour current state to the remote:\npass git push origin master\nNow that we have stored the password store to a remote repository, we can start\ncloning it onto our other devices. Let’s start with the MacBook.\nSyncing to another Machine\nSince Mac OSX and Linux both built onto Unix, the process is quite\nsimilar. Just replace brew with your operating system’s package manager and you should\nbe good to go. You should have git access already configured on your second\nmachine, because this won’t be covered explicitly in this post.\nFirst you need to export and transfer your existing GPG secret key to your new\nmachine, because you want to be able to encrypt and decrypt passwords there as\nwell. To get the secret key id we first list our secret keys:\ngpg --list-secret-keys\nThen we take our key id and export it like this:\ngpg --export-secret-key &lt;your-secret-key-id&gt; &gt; secret.gpg\nNow we need to take the generated file secret.gpg and share it with our second\nmachine. This can be done in various ways, like copying it over via SSH. I\nshared it via Syncthing with my other device.\nWhen you have the secret key on your second device you can import it via:\ngpg --import &lt;path-to-file&gt;\nA little side note: Always make sure you use gpg2, since pass is using gpg2 by\ndefault. If you installed gpg via brew on Mac OSX, you use gpg2 by\ndefault. The same goes for Arch Linux. But this is not the case for every distro\non earth. So make sure gpg and gpg2 both direct to gpg2. Otherwise you\ncould get into trouble doing things with pass, when gpg keys were created with\ngpg one for example.\nNow we want to list our secret keys again and see if it was imported successfully:\ngpg --list-secret-keys\nIn the output you will see something like this in the uid row:\nuid           [ unknown] &lt;Your UID&gt;\nThe unknown means, the key isn’t trusted yet. To trust the key fully, we need to\ndo the following:\ngpg --edit-key &lt;your-key-id&gt;\nOutput:\ngpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\nSecret key is available.\n\nsec  rsa4096/...\n     created: 2021-04-07  expires: never       usage: SC\n     trust: unknown       validity: unknown\nssb  rsa4096/...\n     created: 2021-04-07  expires: never       usage: E\n[ unknown] (1). ...\n(ins)gpg&gt; \n\nIn the GPG shell, which just opened just type and send trust, choose the\nmaximum trust level (5), save and afterwards you can quit. Here is the\ncomplete dialogue:\n(ins)gpg&gt; trust\nsec  rsa4096/...\n     created: 2021-04-07  expires: never       usage: SC\n     trust: unknown       validity: unknown\nssb  rsa4096/...\n     created: 2021-04-07  expires: never       usage: E\n[ unknown] (1). ...\n\nPlease decide how far you trust this user to correctly verify other users&#039; keys\n(by looking at passports, checking fingerprints from different sources, etc.)\n\n  1 = I don&#039;t know or won&#039;t say\n  2 = I do NOT trust\n  3 = I trust marginally\n  4 = I trust fully\n  5 = I trust ultimately\n  m = back to the main menu\n\n(ins)Your decision? 5\n(ins)Do you really want to set this key to ultimate trust? (y/N) y\n\nsec  rsa4096/...\n     created: 2021-04-07  expires: never       usage: SC\n     trust: ultimate      validity: unknown\nssb  rsa4096/...\n     created: 2021-04-07  expires: never       usage: E\n[ unknown] (1). ...\nPlease note that the shown key validity is not necessarily correct\nunless you restart the program.\n\n(ins)gpg&gt; quit\n\nIf you list your secret keys again, you should see the box [ultimate] in the\nuid row instead of [unknown].\nNext we install pass via:\nbrew install pass\nand we clone the repository to the .password-store directory in our $HOME:\ngit clone git@address.com:/pass.git ~/.password-store\nand we are done already. You can list your password store with pass, get one\nof our earlier created passwords with pass test and so on, like it was on our\noriginal machine.\nSince we started with a git repository on our second machine right away, every\nchange gets already committed automatically. Remember to push your changes to\nthe remote when you modify your password store and pull again from your other\ndevices. You can also automate the pushing and pulling by setting a CRON job\nthat pushes and pulls e.g. once per hour. This works surprisingly well\nalready, because normally you don’t work on all your devices in parallel. Even\nif you should run into merge conflicts, you can use git’s capabilities to either\nrebase onto the HEAD or do a merge commit and be in sync again.\nSync to a phone\nTo add new devices, you only need to setup these few steps:\n\nsetup the gpg key\nclone your password-store repository\ninstall the pass cli\n\nThis is the same for setting up pass on a phone. However, how you do each step is\nslightly different. This is why we cover this explicitly as well now for the\nexample of an Android Phone.\nFirst step again is getting the secret key file, we created earlier on your\nphone. You can do this via USB file transfer or like I did it: with Syncthing.\nAfterwards we need an app, which can deal with gpg keys. The app I use for this\nis called OpenKeychain. Download it from F-Droid or the Play Store and open\nit. You should see the following welcome screen:\n\nSelect OPEN KEY FROM FILE and click on the directory symbol on the newly\nopened dialogue to select the gpg key file:\n\nNow your GPG key is also setup on your phone. Next we install the Password\nStore app and open it:\n\ntitle=“password-store opening screen” /&gt;\nWe select CLONE REMOTE REPO and give it the url of our remote repo and the way\nwe want to authenticate. If you are normally using SSH Keys, you can choose SSH\nand either generate a new key and share it or import one you created before. By\ngenerating and sharing, you can share the public ssh key with the server you try\nto clone the git repository from and will immediately be able to sync with the\nrepository.\nAfter you cloned the repository, you need to allow the Password Store to access\nOpenKeychain and afterwards you are able to encrypt and decrypt passwords here\nas well.\nA little tip at the end: if you swipe down on the password list inside of your\nPassword Store app, it syncs with your remote repository and the state on your\nphone is up to date again.\nWhy Password Store?\nNow that we have our multi setup completed we can now collect the fruits of our\nlabor. One of pass’ biggest pros is: you have so many great integrations to use\nwith your new password manager, it’s outstanding!\nYou can directly get passwords into your clipboard via different launchers like\ndmenu on linux or alfred on Mac OS. You already saw the Android App, which makes\nPassword Store usage on phones a breeze. There is a browser integration for\nFirefox and Chrome. You can have different GUI apps, like you are used to from\nother password managers. If you set additional fields like URLs or your\nusernames, many of the tools can make use of them. For example the browser\nintegrations know where to suggest what, based on the url field you’ve set and\nalso offer username completion. There is a plugin for different editors like\nEmacs. You can extend pass with different additions to also support OTPs for\nexample.\nYou are also able to integrate it with any scripts you write, because it’s just\na neat little CLI tool. One example I use, is to get my mail password from pass\nfor offlineimap to sync my mail with my machine. The way how to get the password\ncan be scripted in python. There I just open an external process call to the\npass CLI to retrieve the password. See my\ndotfiles\nfor more details on how to do it.\nAlso when you use Emacs, you can make use of the built-in plugin\nauth-source-pass, which automatically gets secrets from pass for things you\nwant to do within Emacs and need a password for, like sending mails, joining IRC,\nlogin to GitHub to use Forge and so on.\nWith the default settings you are asked for your GPG password every time, but\nyou can also cache it for a longer time. For this you need to set the following\nfields in the ~/.gnupg/gpg_agent.conf:\nmax-cache-ttl 86400\ndefault-cache-ttl 86400\nThe values are in seconds and in my case, cache the password for one day. You\ncan find more infos on GPG in the arch\nwiki.\nAnd the final plus is: you learn a bit about GPG itself during the process,\nwhich finds usage also in many other areas besides of pass. For example you can\nuse your GPG key for other neat things like: signing releases/commits in git,\nsign or encrypt your mails and many things more.\nI hope this post helped you getting rid of some initial difficulties with\nsetting up pass on multiple devices. The bit of effort you spend on the initial\nsetup is quite worth it, because pass is so simple and mainly makes use of\nalready established technologies. This attribute makes it easily integrable into\nalmost any process you need secrets for and makes the handling with passwords\nnot only easy, but also secure."},"posts/PHP/A-basic-routing-approach-with-PHP":{"slug":"posts/PHP/A-basic-routing-approach-with-PHP","filePath":"posts/PHP/A basic routing approach with PHP.md","title":"A basic Routing approach with PHP","links":[],"tags":["mvc","php","route","routing"],"content":"Nice and readable URLs are the way to go in modern web applications. More and\nmore people are abandoning the old style URLs, containing question marks and \nequal signs, in favor of the slash separation for actions and parameters.  Most\nframeworks are already supporting this new kind of URLs and encapsulate the\nlogic inside of a routing class or module. Everyone is using it and everything\nis good so far. But even if these new URLs are all over the place, there is\nvery little information on the net about how it is actually implemented.\nBecause I’m currently working on a minimalistic PHP MVC Framework with a\nfriend, I came across this problem. Beside of the\nsource code of the big players in PHP Frameworks I found a small and easy to\nuse snippet to get routes working pretty fast. In this post I want to present\nthe key PHP feature, which allows us to realize it and how to build upon it.\nEverything starts of with the PHP super constant\n$_SERVER.  We are\ninterested about the ‘REQUEST_URI’  key. If we type in the address of our\nwebserver, followed by and /index.php and this file contains the following\nline of code.\n&lt;?php\necho $_SERVER[&#039;REQUEST_URI&#039;];\nwe probably would get back /index.php. This is exactly what we wanted to\narchive, because now we are able to read what the user typed into the address\nbar of his browser and now we can react to it. But with this approach comes a\nproblem. If we would build our Router upon this feature, we are forced to place\na file at each possible path, which we want to cover, containing the request of\nthe super constant and reroute to the appropriate controller and action. A good\nway to solve this problem would be, that whatever the user types into the\naddress bar, it would open up the same file everytime, like our index.php.\nSadly there is no way to solve this in PHP. So this has to be done on the\nwebserver level itself. Most webservers (like Apache)  support configurations\nwritten down to a .htaccess file, which is placed in the root directory of\nthe webserver. We write the following content to the file.\nRewriteEngine On\nRewriteRule ^(.*) - [E=BASE:%1]\nRewriteRule ^(.*)$ %{ENV:BASE}index.php [NC,L]\nThe first directive tells the webserver to turn on the rewrite engine.\nAfterwards we give him the rules to link everything to the index.php in the\nroot directory. Now we can type in /test/dir/index for example and the\nindex.php in the root directory is called, but with this output\ninstead: /test/dir/index. Based on this behavior we can build our Router. For\na minimalistic example we create a Router.php file which contains the\nfollowing class.\n&lt;?php\n \nclass Router\n{\n    public function route($route)\n    {\n        echo $route;\n    }\n}\n \nand we change our index.php.\n&lt;?php\n \nrequire_once &#039;Router.php&#039;;\n \n$r = new Router();\n$r-&gt;route($_SERVER[&#039;REQUEST_URI&#039;]);\nSo far we gained nothing except a new layer of abstraction. But now we are able\nto actually give our routing some behavior, similar to modern frameworks, which\nare based on controllers and actions. As an example we take the following\npath /default/print_hello. In theory this should instantiate a\nDefaultController class and call the print_helloAction method. As a place\nfor our controllers we create a new directory called controllers and add a\nDefaultController.php file.\n&lt;?php\n \nclass DefaultController\n{\n    public function indexAction()\n    {\n        echo &#039;Index&#039;;\n    }\n \n    public function print_helloAction()\n    {\n        echo &#039;Hello&#039;;\n    }\n}\nThe last part of this post will be about how our Router class can actually\ninstantiate the DefaultController and call the method based on the given\nroute. One way to do it is to cut down the string at the slashes. Afterwards we\ngot the controller and action name which we are now able to instantiate and\ncall. The code for the route function could look like this.\n&lt;?php\n \nclass Router\n{\n    public function route($route)\n    {\n        $parts = explode(&#039;/&#039;, $route);\n        //Index 0 is empty because the routes\n        //always start with a preceding /\n        $controller = array_key_exists(1, $parts) ? $parts[1] : &#039;&#039;;\n        $action     = array_key_exists(2, $parts) ? $parts[2] : &#039;&#039;;\n \n        if (strlen($controller)) {\n            //upper case the first letter of the controller name and\n            //append the Controller string to it\n            $controller = ucfirst($controller) . &#039;Controller&#039;;\n            //Include the PHP file for the Controller\n            require_once &#039;controllers/&#039; . $controller . &#039;.php&#039;;\n            //instantiate the controller\n            $object = new $controller;\n            //call the action\n            if (strlen($action)) {\n                return call_user_func([$object, $action . &#039;Action&#039;]);\n            }\n            //call indexAction if no action is given\n            return call_user_func([$object, &#039;indexAction&#039;]);\n        }\n    }\n}\nIf we type in /default/print_hello we get back Hello. If no action is\ngiven, we automatically try to route to the index action. So /default would\nprint\nIndex. call_user_func\nalso supports parameters. So it is very easy to extend the given Router with\nparameter handling. Also there should be more handling of special cases, like\nif no controller is given in the URL. It’s also simple to extend the given\nmodel to use real view files for showing a page, instead of just doing an echo.\nNevertheless serves this basic concept  as a starting point and it should be\nstraightforward to build upon it."},"posts/PHP/Create-your-own-PHP-dev-env-in-Vagrant":{"slug":"posts/PHP/Create-your-own-PHP-dev-env-in-Vagrant","filePath":"posts/PHP/Create your own PHP dev-env in Vagrant.md","title":"Creating your own PHP dev-env in Vagrant","links":["posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-2","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-3","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-1","posts/creating-your-own-php-dev-env-in-vagrant-bonus-2"],"tags":["dev-env","php","vagrant"],"content":"Table of Contents\n\nPart 2\nPart 3\nBonus 1\nBonus 2\nFull Code Base on GitHub\n\nIn this series of posts we will be creating a development environment(dev-env)\nfor PHP with the help of vagrant. Vagrant is a\nthin wrapper around different virtualization\nsoftware projects like virtual\nbox, which we will be using in this tutorial. The\nso called wrapper of vagrant is a configuration file, written in the ruby\nprogramming language. But don’t worry, you don’t\nneed to be an expert ruby programmer to setup a dev-env in vagrant. Everything\nwe use it for, are some variable assignments. In this configuration file we can\ntell the virtualization software, which operating system and software to\ninstall and how to configure everything. Our ultimate goal here is, if the\nenvironment is started, everything is setup already. Finally we want to get a\nfully configured LAMP stack with a running apache webserver, a mysql database\nand PHP. As always there are many different ways to glory. For example does\nvagrant offer different ready to use recipes via\nchef. There is also support for\nPuppet, a\nunified configuration language for different systems. We won’t use any of these\nplugins in this series. Everything we will work with, are some bash scripts and\nthe vagrant file. I have chosen this path, because I want to keep the full\ncontrol over everything. On the other hand it’s also more work, but I think\nit’s worth it. Before we go into the details, lets talk about the advantages of\na vagrant based development environments.\nBenefits\nThe most obvious benefit is that we will have one file, which handles the\ncomplete configuration. As soon as it is written, we can use it over and over\nagain. We can upload the file to git and download it on other machines to get\nthe same dev-env everywhere we work. We can share it with other programmers in\na team, so everyone is working in the same environment. Sentences like “…but\nit works on my machine” lie in the past now. Best case scenario would be to\nreplicate the environment of the production system. In this case, you are able\nto catch any environmental issues locally, before they go live. The second\nbenefit is a PHP specific one. Languages like python have built-in features to\nrun different versions of the language for different projects and also manage\ntheir respective\npackages(virtalenv). In PHP it’s\npretty difficult to comfortably run different versions on one system. There are\nhowever some approaches like phpbrew to\nhandle this problem. Vagrant is another option. Here you just start the vagrant\nmachine with the respective version of PHP installed. The final benefit is that\nyou interact with everything inside of vagrant from your physical machine. You\ncan use your preferred IDE to code your app. You can reach the webserver via\nyour normal webbrowser. You can view the database over a ssh connection and so\non. In this domain your physical machine is called the host machine and the\noperating system inside vagrant is the guest. So lets list up our final goals,\nwe want to archieve at the end of this tutorial.\nGoals\n\nHaving a one line command to fire up the whole dev-env(vagrant up)\nAccess the served sites via your preferred web browser on your host system\nSetup Virtual hosts to access the site via a pretty looking URL\nAccess the database over a ssh connection with workbench or PHPMyAdmin on your hosts web browser\nHaving a shared fdirectory between host and guest system, to be able to code on your host system and let the changes take effect immediately\n\nInstallation\nYou need to install two things to get started. The first thing is vagrant of\ncourse. You can download it from their official\nhomepage. I personally installed\nvagrant with my package manager on arch linux. This isn’t recommended by\nhashicorp, but I never had problems so far. The second thing you will need is\nVirtualBox, which will be our virtualization software of choice. Again you can\neither download it from their official\nwebsite or install it via your\npackage manager. Those are all the things you need to get yourself into the\ngetting started section.\nGetting started\nWe want to create a vagrant subdirectory in a project directory. Our goal in this tutorial is to have vagrant as a subdirectory of the project folder, where our PHP web app lives. So we can upload anything to git together. Our “project” will just be a phpinfo file, but it can be substituted by any project, you are currently working on.\ncd path-to-project &amp;&amp; mkdir vagrant\nAfterwards we switch into that directory and initiate a new vagrant project.\ncd vagrant &amp;&amp; vagrant init\n\nThis will create a file called Vagrantfile in your newly created directory.\nThis is the configuration we talked about in the beginning of this post. If you\nopen it, you see some ruby code, which consists of some assignments and many\ncomments. We won’t cover anything in detail here, because we want to focus on\nour mission and get our goals done. If you want to know more about the other\nstuff in the Vagrantfile, you can head to the official\ndocumentation of vagrant,\nwhich is very well written. So let’s get into the first two lines of code.\nVagrant.configure(2) do |config|\n  config.vm.box = &quot;base&quot;\nThe first line stays as it is. It tells vagrant on which version this config is\nbased. The second line says what operating system image should be used. These\nare called boxes in the vagrant domain. We will use a basic ubuntu 14.04\ninstallation as our basis. That’s why we change the second line to this:\nconfig.vm.box = &quot;ubuntu/trusty64&quot;\nFor a full list of available boxes have a look into\nthis. Here you can find all\ndifferent kinds of boxes. Starting off with very basic ones, up to fully\nconfigured environments. The next line we are interested in is the following:\n# config.vm.synced_folder &quot;../data&quot;, &quot;/vagrant_data&quot;\nThis line tells vagrant which directory of the host system should be mapped to\na directory inside the guest system. These are called synced folders, because\nany operation happening in this directory is instantly visible on both systems.\nAs you may noticed, this will be the location, where our code base will be\nplaced. For the moment, we will edit this line of code to the following:\nconfig.vm.synced_folder &quot;../&quot;, &quot;/vagrant/projects/&quot;\nThe first argument is the directory on the host system. The second argument is\nthe path inside the guest system. It will be created automatically on startup\nof the vagrant machine. So we don’t need to do anything there for the\nmoment. The last thing we uncomment for the moment is this one\n# config.vm.network &quot;private_network&quot;, ip: &quot;192.168.33.10&quot;\nWe don’t change anything except for deleting the hash tag at the beginning. As\nsoon as this line is active, vagrant starts up a private network connection,\nwhich is only accessible by the host system. This will also be the IP address\nwhere our web application will be available later on. You can change the IP to\nanything you want, but remember to change the later occurrences, where we work\nwith this particular IP as well. This is the end of our intro. In the following\nchapter we will write our first shell scripts to setup the apache webserver.\nGoto Part 2"},"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-1":{"slug":"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-1","filePath":"posts/PHP/Creating your own PHP dev-env in Vagrant Bonus 1.md","title":"Creating your own PHP dev-env in Vagrant: Bonus 1","links":["posts/PHP/Create-your-own-PHP-dev-env-in-Vagrant","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-2","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-3","posts/creating-your-own-php-dev-env-in-vagrant-bonus-2","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-2"],"tags":["apache","php","vagrant"],"content":"Table of Contents\n\nPart 1\nPart 2\nPart 3\nBonus 2\nFull Code Base on GitHub\n\nIn the previous posts we’ve successfully set up a development environment for\nPHP. In the following posts I will present some bonus things, which you can do,\nto optimize your work with vagrant. In this post I will show you how to set a\nvirtual host in apache inside your guest system. Because IP addresses can be\nforgotten quite easily, it’s much more handy to have a short named address\nunder which you can access your web app. In this project we will create a\nvirtual host for our main project directory called mysite.dev. So let’s get\nstarted! First we create a config file inside our conf directory with the\nfollowing content:\n&lt;VirtualHost *:80&gt;\n    DocumentRoot &quot;/var/www/html&quot;\n    ServerName mysite.dev\n    ServerAlias www.mysite.dev\n    ErrorLog &quot;/var/log/apache2/mysite.dev-error_log&quot;\n    CustomLog &quot;/var/log/apache2/mysite.dev-access_log&quot; common\n    &lt;Directory &quot;/var/www/html&quot;&gt;\n       Order allow,deny\n       Allow from all\n       AllowOverride All\n   &lt;/Directory&gt;\n&lt;/VirtualHost&gt;\nThis is the virtual host definition. The first line says that it’s listening on\nport 80, which is the default port. The DocumentRoot is the path to\nour webspace, which was symlinked to the vagrant synced directory in an earlier\npost. ServerName is the address, how you can access the page and the\nServerAlias is the alternative if the ServerName isn’t accessible. The\nfollowing two lines are the paths to the access and error log of our virtual\nhost. If anything bad happens, those two files are the place to start\ntroubleshooting. The following lines are some directory options about what the\nweb app is allowed to do and what not. Next we need a new shell script, which\nwe call mysite.sh. Everything required to setup your project(composer update,\nmigrations etc.) will be defined in this script. In this tutorial we won’t do\nall this, but only activating our virtual host address.\nif [ -f /vagrant/tmp/mysite.conf ]; then\n    chown $USER:$USER /var/www/html\n    mv /vagrant/tmp/mysite.conf /etc/apache2/sites-available/mysite.conf\n    a2ensite mysite.conf\nelse\n    &gt;&amp;2 echo &quot;Error: mysite.conf not found&quot;\nfi\nMake changes effective\nservice apache2 restart\n\nMost of this should be familiar to you from the previous chapters. First we\ncheck if the file exists in our temp directory. If it does, we create the\ndirectory for our project and set the owner of this directory. Afterwards we\nmove the file to the sites-available directory of apache and enable it. To\nmake sure all settings are active, we restart the apache demon. As you already\nknow, we need to get the mysite.conf into the tmp directory. That’s why we\nadd another file provisioner to our Vagrantfile.\nconfig.vm.provision &quot;file&quot;, source: &quot;conf/mysite.conf&quot;, destination: &quot;/vagrant/tmp/mysite.conf&quot;\nTo activate the new provision you either need to execute\nvagrant reload --provision\n\nOr vagrant destroy and vagrant up again. The last step is to make the\nvirtual host known to your host system. For this, you need to add a line into\nyour hosts file. On unix like systems you can find the file in the /etc/\ndirectory. The file should begin with the following\n#\n# /etc/hosts: static lookup table for host names\n#\n \n#&lt;ip-address&gt;\t&lt;hostname.domain.org&gt;\t&lt;hostname&gt;\nAfter this intro we add the following line\n192.168.33.10 mysite.dev\nWhich maps the IP address of our private network hosted by vagrant to a domain\nname. Ok let’s check it out. Type in mysite.dev into the address bar of your\nbrowser. You should now see the php info page from the last tutorial. In the\nnext post I want to cover the database access. I will show two ways to you, how\nyou can interact with your database from your host system. The first way will\nbe with MySQL Workbench via an SSH connection. The second is by installing\nPHPMyAdmin on the guest system and access it via the hosts web browser.  Bonus 2"},"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-2":{"slug":"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-2","filePath":"posts/PHP/Creating your own PHP dev-env in Vagrant Bonus 2.md","title":"Creating your own PHP dev-env in Vagrant: Bonus 2","links":["posts/PHP/Create-your-own-PHP-dev-env-in-Vagrant","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-2","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-3","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-1"],"tags":["mysql","php","phpmyadmin","vagrant","workbench"],"content":"Table of Contents\n\nPart 1\nPart 2\nPart 3\nBonus 1\nFull Code Base on GitHub\n\nSlowly but surely we get to the end of this series. In this last post, I will\nwrite about database access from the host system, while your database is\nrunning on the guest machine. Therefore I will present two strategies to do so.\nSure you also can\nvagrant ssh\n\nvia the terminal and access the database on the CLI. But viewing and fast\nediting of some tables is much more comfortable from a GUI. In this post I will\ngo over two MySQL applications: MySQL Workbench and PHPMyAdmin. While the\nWorkbench will be installed on your host system and access the database via\nssh, PHPMyAdmin will be installed on the guest system, on the existing apache\nweb server. Here we are able to access it over the hosts browser like our web\napp.\nMySQL Workbench\nThe MySQL Workbench can be installed via your package manager\nor from their website. After the\ninstallation we start up the vagrant machine and open the workbench. You should\nsee this area.\n\nClick on the ”+” next to the MySQL Connections. Now a new dialog should open up.\n\nChoose “Standard TCP/IP over SSH”. Afterwards fill out SSH Hostname and SSH\nusername, like I did. At the top you can choose whatever name you like for the\nConnection Name. Then press Ok. Afterwards you should see your newly created\nConnection.\n\nDouble-Click it and you will be asked for the SSH password. Since we didn’t\nchanged it yet, it’s still vagrant. Then you will be asked for the database\npassword. If you followed this tutorial closely, this will be my_pw.\nAfterwards you should be logged into your database successfully and you can\nstart interacting with it.\nPHPMyAdmin\nHere we process similar, like we did in the earlier posts. First\nwe create a new shell script inside of our sh directory. We start off with\nthe following content\ndebconf-set-selections &lt;&lt;&lt; &quot;phpmyadmin phpmyadmin/reconfigure-webserver multiselect apache2&quot;\ndebconf-set-selections &lt;&lt;&lt; &quot;phpmyadmin phpmyadmin/dbconfig-install boolean true&quot;\ndebconf-set-selections &lt;&lt;&lt; &quot;phpmyadmin phpmyadmin/mysql/admin-pass password my_pw&quot; \ndebconf-set-selections &lt;&lt;&lt; &quot;phpmyadmin phpmyadmin/mysql/app-pass password my_pw&quot;\ndebconf-set-selections &lt;&lt;&lt; &quot;phpmyadmin phpmyadmin/app-password-confirm password my_pw&quot;\nThis should look similar to you. We already did it, while installing MySQL.\nBecause PHPMyAdmin is working with an interactive prompt as well, we pass the\ndifferent parameters in before. Out of simplicity we go with my_pw as our\npassword again. Now we can install it\napt-get install -y phpmyadmin\nphp5enmod mcrypt\nOn PHP5 we need to enable the mcrypt library, otherwise we get an annoying red\nbox inside of our PHPMyAdmin Frontend. The last part of our config looks like\nthis.\nif [ -f /vagrant/tmp/phpmyadmin.conf ]; then\n    mv /vagrant/tmp/phpmyadmin.conf /etc/apache2/sites-available/phpmyadmin.conf\n    a2ensite phpmyadmin.conf\nelse\n    &gt;&amp;2 echo &quot;Error: phpmyadmin.conf not found&quot;\nfi\n \nservice apache2 restart\nWe put a virtual host configuration for the PHPMyAdmin page into the right\nplace, like we did with our web app in the last post. Finally we restart the\napache server. Our phpmyadmin.conf looks like this\n&lt;VirtualHost *:80&gt;\n    ServerAdmin webmaster@dummy-host.com\n    DocumentRoot &quot;/usr/share/phpmyadmin&quot;\n    ServerName phpmyadmin.dev\n    ServerAlias www.phpmyadmin.dev\n    ErrorLog &quot;/var/log/apache2/phpmyadmin.dev-error_log&quot;\n    CustomLog &quot;/var/log/apache2/phpmyadmin.dev-access_log&quot; common\n    &lt;Directory &quot;/usr/share/phpmyadmin&quot;&gt;\n       Options Indexes FollowSymLinks Includes MultiViews\n       Order allow,deny\n       Allow from all\n       AllowOverride All\n   &lt;/Directory&gt;\n&lt;/VirtualHost&gt;\nThis is almost the same, like we did in the last post, except for the new\npaths. Additionally we set some more directory options for PHPMyAdmin to make\nit’s routing possible. Next thing to do is add these scripts as partitioners to\nour Vagrantfile.\nconfig.vm.provision &quot;shell&quot;, path: &quot;sh/phpmyadmin.sh&quot;\nconfig.vm.provision &quot;file&quot;, source: &quot;conf/phpmyadmin.conf&quot;, destination: &quot;/vagrant/tmp/phpmyadmin.conf&quot;\nNow we are only missing an entry in our hosts file, to make the new domain\naddress known to our host machine. So we go to our hosts file again and add\nthe following line\n192.168.33.10 \tphpmyadmin.dev\nThat’s it! Recreate or re-provision your vagrant machine and go to\nphpmyadmin.dev on your hosts web browser. You should see this page\n\nType in the credentials you have typed in earlier and you should be able to be\nin the web frontend of your database."},"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-2":{"slug":"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-2","filePath":"posts/PHP/Creating your own PHP dev-env in Vagrant Part 2.md","title":"Creating your own PHP dev-env in Vagrant: Part 2","links":["posts/PHP/Create-your-own-PHP-dev-env-in-Vagrant","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-3","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-1","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-2"],"tags":["apache","bash","php","vagrant"],"content":"Table of Contents\n\nPart 1\nPart 3\nBonus 1\nBonus 2\nFull Code Base on GitHub\n\nIn the last post we installed vagrant and enabled the vagrant settings we need\nin the Vagrantfile. We have set a base image of ubuntu 14.04, made a synced\nfolder and enabled a private network connection between host and guest system.\nNow it’s time to write our first shell scripts, which will configure our guest\nsystem to serve as a web server. Vagrant comes with a neat feature called\nprovisioning. This can be shell scripts, which will be executed or files, which\nwill be uploaded onto the guest system. Of course there are more provisioners\nto explore. For a full reference head over\nhere. In this tutorial we will\nfocus on these two provisioners. Before we setup our apache webserver, let’s\nstart with a simple script, which should run before. This script will update\nthe ubuntu system and should be a great intro into provisioning. First we\ncreate a new directory inside of our vagrant directory, called sh. This\ndirectory will contain all of our shell scripts, which we will write during\nthis tutorial. Everything could be in a single shell script, but I like to\nsplit the shell scripts into specific categories. The first will be called\nupdate.sh and contains the the following content:\n#!/usr/bin/env bash\n \napt-get update\nThe first line will be in every script we write. It’s a dynamic binding to tell\nthe operating system where to find the program, which should execute this\nscript. Afterwards we update the system by executing apt-get update. As you\ncan see, we don’t need to issue sudo rights. Everything inside these shell\nscripts will be run as a super user automagically. Now we need to get vagrant\nto provision this file. That’s why we add the following line after the\ndefinition of our base system.\nconfig.vm.box = &quot;ubuntu/trusty64&quot;\nconfig.vm.provision &quot;shell&quot;, path: &quot;sh/update.sh&quot;\nThis statement is pretty straight forward. We tell vagrant that we want to\nprovision something, which is of the type shell and the path to the shell\nscript is sh/update.sh. If we would start our system now, it would do a\nsystem update at the beginning. Now that we are working on the latest state of\nthe system, we can start adding new software. It’s time to create the apache\nwebserver. We add a new filed called apache.sh to the sh directory, which we\ncreated earlier. We start off in the same way, by downloading the software via\napt-get:\napt-get install -y apache2\nFollowed by this:\nif ! [ -L /var/www/html ]; then\n    rm -rf /var/www/html\n    ln -fs /vagrant/projects /var/www/html\nfi\nHere we delete any symlinks on the /var/www/html location, if they exist and\ncreate a new one afterwards. The symlink will be between our projects\ndirectory and the apache web server. So everything which changes in one place,\nwill automatically change in the other location as well. It’s like the synced\nfolders, but it’s an inter system feature of linux. The next part is\noptional. I copied the whole apache.conf out of it’s base installation and\nkeep it on my host system to change some things. On system startup I copy it\nback onto the guest and replace the base config with it.\nif [ -f /vagrant/tmp/apache2.conf ]; then\n    mv /vagrant/tmp/apache2.conf /etc/apache2/apache2.conf\nelse\n    &gt;&amp;2 echo &quot;Error: apache2.conf not found&quot;\nfi\nThis is a check, if a file is in the vagrant/tmp directory, called\napache2.conf. If there is one, it will be moved over to the position of the\noriginal file. Otherwise an error is printed to the error channel of the\nconsole. But how do we get the file to the tmp directory? Remember that shell\nprovision isn’t the only thing? Because now file provisioning comes into play.\nconfig.vm.provision &quot;file&quot;, source: &quot;conf/apache2.conf&quot;, destination: &quot;/vagrant/tmp/apache2.conf&quot;\nWe add this line under the shell provision. This copies the file from the\nconf directory to the tmp directory of the guest system. Why we need to do\nthis extras step, you may ask? Why not copy it directly to the original\nlocation? Well, it’s pretty simple: In the provision context you don’t have any\nroot rights. But later in the shell script you do. That’s why we provide a\nplace for our files, where the shell scripts have access to later. The next\nthing I do, is changing the rights on the webspace and the log directory, to be\nable to create and read files without any problems.\nchmod -R 755 /var/www\nchmod -R 755 /var/log\nThe last thing I do is activating mod rewrite, which allows web applications to\nrewrite the routing of requests. Many frameworks rely heavily on this feature.\na2enmod rewrite\nAnd this is our full apache2.sh script:\n#!/usr/bin/env bash\n \napt-get install -y apache2\n \n#Symlink the apache webspace to the shared folder of the host and guest syste,\nif ! [ -L /var/www/html ]; then\n    rm -rf /var/www/html\n    ln -fs /vagrant/projects /var/www/html\nfi\n \n#Copy the apache2.conf to the right location\nif [ -f /vagrant/tmp/apache2.conf ]; then\n    mv /vagrant/tmp/apache2.conf /etc/apache2/apache2.conf\nelse\n    &gt;&amp;2 echo &quot;Error: apache2.conf not found&quot;\nfi\n \n#grant permission to webserver\nchmod -R 777 /var/www\nchmod -R 777 /var/log\n \n#enable mod_rewrite\na2enmod rewrite\nIn the next part we will reach forward to install php and mysql. You already\nlearned the main functionalities, which we need to get our dev-env running.\nFrom now on it gets far more repetitive, but some small challenges are still\nwaiting on our way. Goto Part 3"},"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-3":{"slug":"posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-3","filePath":"posts/PHP/Creating your own PHP dev-env in Vagrant Part 3.md","title":"Creating your own PHP dev-env in Vagrant: Part 3","links":["posts/PHP/Create-your-own-PHP-dev-env-in-Vagrant","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Part-2","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-1","posts/PHP/Creating-your-own-PHP-dev-env-in-Vagrant-Bonus-2"],"tags":["mysql","php","vagrant"],"content":"Table of Contents\n\nPart 1\nPart 2\nBonus 1\nBonus 2\nFull Code Base on GitHub\n\nIn the previous post we’ve set up the apache web server successfully. Now it’s\ntime to add MySQL and PHP to finish the LAMP stack. Let’s start with MySQL\nfirst and add a mysql.sh file to the sh directory. We write the following\nlines into that file.\ndebconf-set-selections &lt;&lt;&lt; &quot;mysql-server mysql-server/root_password password my_pw&quot;\ndebconf-set-selections &lt;&lt;&lt; &quot;mysql-server mysql-server/root_password_again password my_pw&quot;\napt-get install -y mysql-server libapache2-mod-auth-mysql\nThe first two lines set values for the upcoming installation dialog of MySQL.\nThe installation dialog will ask us for a password and to repeat it. Because we\nwant to setup MySQL automatically without any interaction, we preset those\nfields with our password of choice. In this case my_pw. Afterwards we\ninstall MySQL and an extra package for apache. Afterwards we copy over the\nconfig file of MySQL onto the guest system.\nif [ -f /vagrant/tmp/my.cnf ]; then\n    mv /vagrant/tmp/my.cnf /etc/mysql/my.cnf\nelse\n    &gt;&amp;2 echo &quot;Error: my.cnf not found&quot;\nfi\nWe do it exactly the same way, as we did with the apache.conf. First we place\nthe my.cnf in our conf directory. Afterwards we add a file provisioner to\nour Vagrantfile.\nconfig.vm.provision &quot;file&quot;, source: &quot;conf/my.cnf&quot;, destination: &quot;/vagrant/tmp/my.cnf&quot;\nThis time it’s important to replace a custom config file with the original,\nbecause in our modified version we switch the following line from:\nbind-address = 127.0.0.1\n\nto\nbind-address = 0.0.0.0\n\nBefore it was only possible to access the database on the guest machine itself.\nWe changed this, so we can connect from the host machine as well. Finally we\nneed to restart the MySQL demon.\nservice mysql restart\nWith which we end up in our final mysql.sh file, which looks like this:\n#!/usr/bin/env bash\n \ndebconf-set-selections &lt;&lt;&lt; &quot;mysql-server mysql-server/root_password password my_pw&quot;\ndebconf-set-selections &lt;&lt;&lt; &quot;mysql-server mysql-server/root_password_again password my_pw&quot;\napt-get install -y mysql-server libapache2-mod-auth-mysql php5-mysql\n \nif [ -f /vagrant/tmp/my.cnf ]; then\n    mv /vagrant/tmp/my.cnf /etc/mysql/my.cnf\nelse\n    &gt;&amp;2 echo &quot;Error: my.cnf not found&quot;\nfi\n \nservice mysql restart\nFinal thing we need to do is adding the shell script as a shell provisioner to our Vagrantfile.\nconfig.vm.provision &quot;shell&quot;, path: &quot;sh/mysql.sh&quot;\nNow let’s install PHP as well. This is pretty straight forward. Our php.sh looks like this:\n#!/usr/bin/env bash\napt-get install -y php5 libapache2-mod-php5 php5-mcrypt php5-curl php5-mysql\nAfterwards we also add it as a provisioner\nconfig.vm.provision &quot;shell&quot;, path: &quot;sh/php.sh&quot;\nWhich results in the following Vagrantfile\nVagrant.configure(2) do |config|\n  config.vm.box = &quot;ubuntu/trusty64&quot;\n \n  config.vm.provision &quot;shell&quot;, path: &quot;sh/update.sh&quot;\n  config.vm.provision &quot;shell&quot;, path: &quot;sh/apache.sh&quot;\n  config.vm.provision &quot;shell&quot;, path: &quot;sh/mysql.sh&quot;\n  config.vm.provision &quot;shell&quot;, path: &quot;sh/php.sh&quot;\n \n  config.vm.provision &quot;file&quot;, source: &quot;conf/apache2.conf&quot;, destination: &quot;/vagrant/tmp/apache2.conf&quot;\n  config.vm.provision &quot;file&quot;, source: &quot;conf/my.cnf&quot;, destination: &quot;/vagrant/tmp/my.cnf&quot;\n \n  config.vm.network &quot;private_network&quot;, ip: &quot;192.168.33.10&quot;\n \n  config.vm.synced_folder &quot;../&quot;, &quot;/vagrant/projects/&quot;\nend\nThis is it. Our LAMP stack is complete. Now we want to check out our\nenvironment. Switch into the directory of the Vagrantfile and execute the\nfollowing command\nvagrant up\n\nA lot of stuff is happening now and for the first time it will take some time\nto fire up the vagrant machine. If vagrant booted up successfully, we add\nan index.php in the directory above our Vagrantfile. This is our project\ndirectory. I won’t show anything big here, just a proof of concept\n&lt;?php\n \nphpinfo();\nIf you browse to 192.168.33.10 on your host system, you should see an info\npage about the running PHP version, similar to this\n\nCongratulations! You’ve done it! To suspend your vagrant machine just type the\nfollowing in the same spot, where you did the vagrant up\nvagrant suspend\n\nThis saves the state of your machine and will be the way to shut it down in\ndaily business. If you want to completely reset your machine and install\neverything again, you execute\nvagrant destroy\n\nTo start your vagrant machine, just do vagrant up again. You could start from\nhere and put your own PHP project in the parent directory of the Vagrantfile\nand start programming. But if you want to know some nifty stuff, like how to\naccess your website via an address like mysite.dev instead of 192.168.33.10\nor how you can access your database with the mysql-workbench software on\nyour host machine, you should definitely check out the next posts in this\nseries. Goto Bonus 1"},"posts/Vim/The-Power-of-Vim-Plugins-Vim-Plug":{"slug":"posts/Vim/The-Power-of-Vim-Plugins-Vim-Plug","filePath":"posts/Vim/The Power of Vim Plugins Vim-Plug.md","title":"The power of Vim Plugins: Vim-Plug","links":[],"tags":[],"content":"A few weeks after my\nVundle post, I stumbled\nupon a plugin manager called vim-plug.\nIt sounded very promising and I checked it out. Until today I didn’t go back to\nVundle. This is almost half a year ago and I think it’s time to write something\nup about this amazing plugin manager. When you enter the GitHub page you see a\nbunch of pros, why this plugin manager is so awesome. Two points stood out for\nme though. Point one is the utilization of asynchronous downloads and\ninstallation, which makes the whole process of installing and updating a whole\nlot faster. If you installed your vim with the +python/+python3 or +ruby flag,\nyou are able to use this feature. The alternative is to install\nneovim, which comes with asynchronous batteries included.\nThe second point is “On-demand loading for faster startup time”. This line\nsays, you are able to give every plugin some options, when they should be\nloaded up. For example, if you program in more than one language, say PHP and\nC++. Now you got some language specific plugins for example\nclang-format, which formats your\nC++ Code. Having the plugin loaded up, only makes sense if you are in a C++\nfile, right? With the help of vim-plug you are now able to delay the startup of\nclang-format until you open a C++ file for the first time. This isn’t the only\nlazy load option. We will cover this feature in depth in a later section. As I\nlooked through my plugin list I got about one half of my plugins which didn’t\nneed to be loaded on startup. After setting these options, my startup time\nincreased quite a bit. If you are convinced either by me or because of the rest\nof pro list, let’s get started with the installation.\nInstallation\nThe installation is pretty straight forward. The GitHub page\ngives you 3 different commands. If you use normal vim on Unix, you need to\nexecute the following line:\ncurl -fLo ~/.vim/autoload/plug.vim --create-dirs\nIf you use neovim and you don’t have your default vim location symlinked, you\nneed to change the destination path:\ncurl -fLo ~/.config/nvim/autoload/plug.vim --create-dirs\nNow that you have vim-plug installed you need a section at the beginning of\nyour .vimrc, which looks like this:\ncall plug#begin(&#039;~/.vim/plugged&#039;)\n \ncall plug#end()\nWith this template you are ready to go for the next steps.\nConfiguration\nAdding plugins from GitHub is as easy as the following line.\nYou need to place these between the 2 lines, which we added at the end of the\nlast section.\nPlug &#039;scrooloose/nerdtree&#039;\n\nIf you are using another source than GitHub, you can put the full git path\nbetween the quotes. Pay attention to the ending of the URL:\nPlug &#039;github.com/scrooloose/nerdtree.git&#039;\n\nThats it! Now you can install that Plugin via the :PlugInstall command.\nAnother tiny benefit is, that you save 2 characters in each command. You just\ntype :PlugInstall for vim-plug instead of :PluginInstall in Vundle. The\nsame goes for updating existing packages, which is :PlugUpdate instead of\n:PluginUpdate. This is only a small thing, but I really like this, since I’ve\nswitched. The second important feature I use very often is to remove packages.\nFor this you just remove the line\nPlug &#039;scrooloose/nerdtree&#039;\n\nAfterwards you call :PlugClean. Now you get a prompt with the plugins, which\nshould get deleted. You can accept with [y]es or [n]o. To skip this prompt,\nyou can also type in the command :PlugClean!. Now we are coming to one of the\nmain advantages: The on-demand loading. As described in the beginning, this\ndelays the loading of the plugin until something specific happens. This is\ngreat because vim doesn’t load up all plugins on startup, which improves your\nstartup time quite a bit. Let’s get to our first example, where I want to show\nyou the on keyword. The on keyword loads a plugin, when a specific\ncommand was executed. Those options will be written as JSON after the plugin\npath. For example, we want to load NERDTree when :NERDTreeToggle was\nexecuted for the first time.\nPlug &#039;scrooloose/nerdtree&#039;, {&#039;on&#039;: &#039;NERDTreeToggle&#039;}\n\nAs you can see, we split up the line by a comma. Afterwards we write down a\nJSON string, where on is the key and the command(without the colon) is the\nvalue. That’s everything! Now NERDTree load will be delayed until this command\nis executed. The second keyword I use pretty often is for. This keyword\nloads plugins for specific filetypes. Let’s take the C++ plugin clang-format\nfrom the intro, which formats C++ code according to some sort of standard. This\nplugin is only useful to us, if we work on C++ files. That’s why we want to\ndelay the loading of this plugin until we work on a C++ file for the first time\nPlug &#039;rhysd/vim-clang-format&#039;, {&#039;for&#039;: &#039;cpp&#039;}\n\nAnd here we go. Clang-fromat will be lazy loaded when we open a C++ file for\nthe first time. The last keyword which is pretty useful is do. It’s used\nfor plugins, which need another third party component to be compiled. One\nexample is YouCompleteMe, an\nautocompletion engine for vim. If you want to have C++ semantic completion\nsupport, you need to compile another component, which is used by this plugin.\nThe compilation is triggered via a python script, which downloads clang and\ncompiles the component afterwards. Normally you do this manually after each\nupdate, but you are also able to tell vim-plug to execute it after the plugin\nupdate automatically. To make it happen add the following line to your\n.vimrc.\nPlug &#039;Valloric/YouCompleteMe&#039;, { &#039;do&#039;: &#039;./install.py&#039; }\n\nNow everytime YouCompleteMe is updated, the third party component will be\nrecompiled automatically and you don’t have to deal with it anymore. Of course\nyou can combine the keywords in a single JSON document.\nPlug &#039;Valloric/YouCompleteMe&#039;, { &#039;do&#039;: &#039;./install.py&#039;, &#039;for&#039;: &#039;cpp&#039; }\n\nThose are the things I mainly use, but there is much more to discover. For a\nfull reference head over to the GitHub page of\nvim-plug."},"posts/Vim/The-power-of-Vim-Plugins-CtrlP":{"slug":"posts/Vim/The-power-of-Vim-Plugins-CtrlP","filePath":"posts/Vim/The power of Vim Plugins CtrlP.md","title":"The power of Vim Plugins: CtrlP","links":[],"tags":[],"content":"In this article we will discover one of the greatest and most widely used plugins in the vim world: CtrlP. It covers a similar area like NERDTree, because it helps you to find and open files very quickly. The magic behind this plugin is it’s fuzzy search engine. You can type in any region of a file path and the fuzzy search matches it to possible files and directory parts. The more matches there are, the higher it is ranked in the results. This means, that you don’t need to type in the beginning of a file path, but it can be any arbitrary part of the path. Of course you want to type in those parts that make a file unique to match it as fast as possible. With some practice it is a highly efficient way of navigate to files. Of course NERDTree and other tree views have their right to exist, but CtrlP is far more useful in most situations. In my humble opinion the big hype around this feature started with Textmate. It shipped this feature right with the editor itself. There was no need to install additional stuff to get the fuzzy search. In Textmate the shortcut for this feature was Ctrl-P. So this is why the corresponding vim plugin has the name CtrlP. But there is no official statements in the docs about it. In the process many other editors adopted this feature in form of a built-in feature (atom) or in form of a plugin (emacs[helm]). Most search requests still give you kien’s CtrlP GitHub Repository. Sad news is he isn’t actively maintaining it anymore. Good news is there is an active fork where you should download it from. After installing CtrlP, you should be able to call it with :CtrlP. A new horizontal split view should have opened at the bottom now.\n\nIf you type in some letters the result list should get modified. Also the parts of the file paths are highlighted, which match your typed in string. If you press  the split will be closed again. Like always, we don’t want to type in the whole command over and over again if we want to execute CtrlP. So let’s configure a proper shortcut for opening CtrlP.\nlet g:ctrlp_map = &#039;&lt;C-p&gt;&#039; \nlet g:ctrlp_cmd = &#039;CtrlP&#039;\n\nWhat a surprise! We mapped CtrlP to &lt;C-p&gt;. Now you should be able to open CtrlP much faster. Maybe you recognized it during the first test already: the CtrlP split isn’t modal. If you try to go into normal mode, to navigate through the results, the CtrlP view closes immediately. But this doesn’t mean, that you need to blow the dust from your arrow keys. You can still navigate up and down with j and k by holding down Ctrl at the same time. You are also not bound to to the enter key, when opening files, even though this is the fastest way to open a file in the current buffer. If you try to open a file with &lt;C-o&gt; you have the chance to choose how you want to open it. You can choose between [t]ab, [h]orizontal, [v]ertical and [r]eplace. The first letter stands for the key, which needs to be pressed to apply the command. Replace doesn’t make much sense in my opinion, because pressing Enter(&lt;CR&gt;) is much faster. Of course there are shortcuts for the other kinds of opening as well. &lt;C-t&gt; opens the file in a new tab, &lt;C-v&gt; opens it in a vertical split and &lt;C-x&gt; does the job in a horizontal split.\nModes\nIn the previous chapter we learned about the basic navigation features, but there is much more to explore. If we open CtrlP we see the whole path for each file. This is the filepath mode. In addition to this mode there are three other ones. Let’s call them: major modes. There is a buffer mode, which shows a list of all open buffers of our current Vim session. The third mode is called MRU(most recently used) files. The name says it all. You are able to cycle through these modes by pressing &lt;C-f&gt; (forward) or &lt;C-b&gt; (backward).\n\nThe highlighted mode is always the active mode, as you can see in the image. In this case we are in the buffer mode. If you cycle back with &lt;C-b&gt; you would access the file mode and CtrlP would look like this\n\nAs you can see now, files is the highlighted mode. On the right is the buffer mode, in which we were in before. You can access it again by pressing &lt;C-f&gt;. So you can imagine the modes as a merry-go-round. The mode on the right is always accessed by cycling forward and the mode on the left of the current mode is accessed by cycling back. Thanks to thomass for putting this right in the comments section. I’m using the Buffer list very often, so I created an additional shortcut for it like this:\nnnoremap &lt;leader&gt;b :CtrlPBuffer&lt;CR&gt;\n\nThe same thing can be done for MRU. The command is called :CtrlPMRUFiles. Next to these major modes, there are two additional minor modes which can be turned on or off for every one of these three major modes. These minor modes are also saved for the future. This means, if you close CtrlP and open it up later, those modes are still active. The first is the file only mode, which can be activated by pressing &lt;C-d&gt;. If this mode is active your typed in characters are only matched against the filename itself, not the rest of the path. I usually prefer the file only search in smaller projects and path in bigger ones. The reason is, that I can have redundant file names more often and this way I can distinguish better between them by typing in distinct parts of the path. To distinguish between file and path mode, there is a small d at the beginning of the CtrlP prompt, when in file mode.\n\nThe other minor mode is the regex mode, which is called by pressing &lt;C-r&gt;. It’s indicated by a small r at the beginning of the prompt.\n\nI don’t really use it at all, but I’m sure it’s very helpful in some scenarios. Both of these modes are active until you turn them off by pressing the same key combination again. Additionally both of these minor modes can be active at the same time.\n\nExecute command on open\nThis is also a neat feature. For example, if you got an error message in a program you are currently coding on. It’s telling you the file and line, where it occurred. Now you are able to type in the file name and write a : afterwards, followed by the line number and CtrlP opens the file and executes the line command afterwards. This is almost the only command I use, but you can give CtrlP every valid Vim command like :q, which opens the file and quits the buffer right afterwards. This doesn’t make sense at all, but it shows, that almost every command is possible.\nFurther Configuration\nThis is already a good chunk of stuff we get from CtrlP. But with some more configuration, we can get even more from it. Maybe you’ve noticed, there are also files listed up, that you don’t want to open ever. There are two ways to ignore those in a CtrlP listing. The first option is, you populate the Vim wildignore setting with the file endings you don’t want to open. These will also be ignored by Vim itself. Furthermore other plugins make use of this setting, too. So if you don’t want to open these filetypes at all in Vim, wildignore is the way to go for you. To add it your .vimrc, you type in the following:\nset wildignore += *.swp,*.zip,*.exe,*/tmp/*\n\nThere you have a comma separated list with some *.ending definitions. The star says, that any arbitrary name can occur before a dot and the file ending. If you add a slash, this setting is also able to ignore complete directories. For example the last argument in the list ignores all kinds of tmp directories. The 2 stars say, that the tmp directory can lie anywhere in the file system and anything that lies in this tmp folder will be ignored as well. The second way is the CtrlP only way. To ignore specific file types in CtrlP only, you add the following to your .vimrc.\nlet g:ctrlp_custom_ignore = {\n    \\ &#039;dir&#039;: &#039;\\\\v\\[\\\\/\\]\\\\.(git|hg|svn)$&#039;,\n    \\ &#039;file&#039;: &#039;\\\\v\\\\.(exe|so|dll)$&#039;,\n\\ }\n\nThis example is taken from the official documentation. As you can see, this list is separated by directories and files. This snippet would ignore git, hg and svn directories in your project as well as files ending with exe, so and dll. We don’t go into more detail about what the regex notions mean, but if you want to to add or remove anything you can do so by adding them inside the brackets. Be sure to have the pipe between the arguments. One more important configuration is to tell CtrlP where to search for files. Add this to your .vimrc\nlet g:ctrlp_working_path_mode = &#039;ar&#039;\n\nThis tells CtrlP where to start the search and where to search. There are some characters with a special meaning, which can be combined to set this configuration. The a tells that CtrlP should search in the current directory unless it’s not a sub directory of where vim was called. This normally occurs if you start vim from the console. The r flag tells CtrlP that it should iterate through parent directories for files until it finds a Version Control System Directory like the .git or .hg directory. This flag is very useful if you often work on projects, which were checked out from git or another VCS, because they act like a natural delimiter for your project files, which you want to find inside of CtrlP. There are also some more options, which I haven’t set. The c option says, that only files of the current directory are listed, w is the opposite of a. It starts the search from the cwd up to the sub directory of your current file, while a does it the other way around. An empty string or 0 would disable this feature completely. This is it. The post was getting longer as I thought initially. But I got one more neat snippet, which you can use to make CtrlP search even faster:\nlet g:ctrlp_use_caching = 0\nif executable(&#039;ag&#039;)\n    set grepprg=ag\\ --nogroup\\ --nocolor\n\n    let g:ctrlp_user_command = &#039;ag %s -l --nocolor -g &quot;&quot;&#039;\nelse\n    let g:ctrlp_user_command = [\n        \\ &#039;.git&#039;,\n        \\ &#039;cd %s &amp;&amp; git ls-files . -co --exclude-standard&#039;,\n        \\ &#039;find %s -type f&#039;\n    \\ ]\n    let g:ctrlp_prompt_mappings = {\n    \\ &#039;AcceptSelection(&quot;e&quot;)&#039;: [&#039;&lt;space&gt;&#039;, &#039;&lt;cr&gt;&#039;, &#039;&lt;2-LeftMouse&gt;&#039;],\n    \\ }\nendif\n\nI found this on one of my favorite vim blog posts of all time. To use it, you need to have Ag (a.k.a the silver searcher) installed, which is a tool that can search for patterns in many files and directories. We will come to the Vim Plugin for Ag later. For now you can download Ag with your package manager and enjoy the advantages of this snippet. If you don’t want to install Ag right now, it’s also ok if you have git installed, which you already did, if you followed the first post of this series. Overall the Ag and Git search engines are much faster than the normal grep, which is used by CtrlP. For me it made a big difference in huge projects. This is by far no complete reference about CtrlP. If you need further information you can type the command :h CtrlP into Vim."},"posts/Vim/The-power-of-Vim-Plugins-NERDTree":{"slug":"posts/Vim/The-power-of-Vim-Plugins-NERDTree","filePath":"posts/Vim/The power of Vim Plugins NERDTree.md","title":"The power of Vim Plugins: NERDTree","links":[],"tags":["nerdtree","plugins","vim"],"content":"Hey everyone, this time we will cover a plugin called\nNERDTree. This is the kind of\naddition, which shows the file system with all it’s files and sub folders. It\nis a great tool to keep an overview, when working on a project or a big set of\nfiles. Because I’m coming from a Sublime background, it was one of the first\nthings I’ve been missing. I was and am still one of those people, who interact\nwith the tree view quite a lot. I also prefer it over tabs and other\nalternatives, but that is just my personal preference. With\nVundle installed from the last\npost, it is pretty easy\nto get started with NERDTree. Just add the following line to your plugin list\ninside your .vimrc\nPlugin &#039;scrooloose/nerdtree&#039;\nRe-source your .vimrc and call :PluginInstall. Restart vim, just to be\nsave everything is properly set. Now call :NERDTreeToggle and the tree view\nshould pop up on the left side of your vim instance.\n\nNERDTree is just another split view, in which you aren’t able to write\nanything. It has it’s own shortcuts, to which we come later.\nConfiguration\nThe first thing you probably think of, is why would I call :NERDTreeToggle\nevery time I start vim or open a new buffer. To end this struggle, just add the\nfollowing line into your .vimrc\nautocmd VimEnter * NERDTree\nThere is one more thing I’m not comfortable with as well. And this is the\ncursor being in NERDTree after startup. To position the cursor in the code file\nto the right of NERDTree at the beginning, add the following line to the\n.vimrc\nautocmd VimEnter * wincmd p\nIf you are working with tabs on a regular basis, you will encounter, that\nNERDTree isn’t opened in new tabs as well. To mirror our existing NERDTree to\nother tabs, we need an additional command in our .vimrc\nautocmd BufWinEnter * NERDTreeMirror\n\nAn alternative approach is the\nNERDTree-Tabs plugin, which\nenables this feature by default, without any configuration effort. The last\nthing I have configured is a shortcut to focus NERDTree. Because I’m working\nwith split views pretty much at the moment, it is very inconvenient to walk\nthrough all the views, until I’m back into NERDTree.\nmap &lt;F5&gt; :NERDTreeFocus&lt;CR&gt;\nThis maps the control key F5 to the NERDTree focus action. If you prefer\nanother key or a combination you are free to use whatever you are comfortable\nwith.\nUsage\nWith the last part of the configuration, we already stepped into the usage.\nBut what are we able to do, if we called :NERDTreeFocus? The short answer\nis: pretty much everything you can do with a tree view in Sublime and\nother editors/IDEs. In this tutorial I only go through the functions I use\nmostly, but there are plenty more helpful shortcuts and functionalities. For a\nfull reference you can always type in ? while in NERDTree and you get the full\nlist of shortcuts. Press ? again to come back to the tree view.  # Navigation\nOk, lets start of easy. j and k are also used for navigating up and\ndown like in any other buffer. To jump to the parent directory, you can\npress p and if you want to go to your root directory you press P. To\nopen a directory press o. This will show all files and sub directories. To\nclose the directory, simply press o again.  # Setting the current node and\nbookmark The number one reason we want to use NERDTree is an overview of our\ncurrent project or set of files, which is always a sub node in our file system.\nAt the moment I’m navigating through the console to the root directory of my\nproject and open vim there. But there is also a NERDTree way of doing this. You\nnavigate to the root directory of your project in NERDTRee and press cd\nafterwards. This means “change directory” and it doesn’t have any visual\neffect. But now every command you type into command-line mode is relative to\nyour chosen root directory and not to the system root. Next thing you may want\nto do is to only show this directory and hide the rest of your file system. By\npressing C you set the underlying directory as the top most visible node.\nEvery time you want to work on your project you need to navigate there. This\ngets very exhaustive, but NERDTree has a neat feature called bookmarks. You can\nbookmark directories and files and you are instantly able to open them from a\nbookmarks list, without navigating there first. To bookmark a node, like our\nproject root, we type in :Bookmark, while NERDTree is focused and the project\nroot is chosen. To open our bookmarks we can press B and now we can\ntype cd, C and o on the chosen bookmark and we are in our project\nwithout navigating there first.\n\nIf you want to get rid of the bookmark, you just navigate to the bookmark\nand press D.\nOpening files\nNow we come to the most important part, actually opening a file. There are\nmany different ways of opening. A normal open in the buffer to the right\nis done by pressing o. But you are also able to open a file in a new\ntab[t], in a vertical split[s] or horizontal split[i]. As a\nsite node: o is always overwriting the buffer you were in last. So for\nexample, if you opened some vertical splits and you are in the most right\nsplit, you focus NERDTree and open another file by pressing o, the most\nright buffer will be overwritten.\nModifying nodes in the file system\nIn the beginning I just used terminal commands like touch/mkdir/mv. With the\nhelp of :! I was able to do any CLI command from within vim. Afterwards I\nneeded to refresh the file system by pressing R. But I discovered a much\neasier way of doing things like these, quite recently. Just go to a node, which\nyou want to modify and press m. Now you get an overview of allowed\noperations.\n\nThese are adding, moving/renaming, copying and removing files or directories.\nJust choose the shortcut in front of the option and you will be put to the next\ninstruction buffer. Removing needs to be confirmed by a ‘yes’. Adding, moving\nand copying requires the new path. As an example: if you want to add a new\ndirectory you press m on the given node, followed by a and then type in\na directory name, followed by a /, otherwise a file would be created. If\nyou are ok with the name, you confirm by pressing Enter. Every modification\ndone from this menu will update your tree view automatically, so there is no\nneed to refresh manually. I hope, this overview will get you started with\nNERDTree properly. If you have any questions you can ask them in the comment\nsection below."},"posts/Vim/The-power-of-Vim-Plugins-Vundle":{"slug":"posts/Vim/The-power-of-Vim-Plugins-Vundle","filePath":"posts/Vim/The power of Vim Plugins Vundle.md","title":"The power of Vim Plugins: Vundle","links":[],"tags":["vim"],"content":"To start of this series properly, we need a\nplugin manager, to handle all our plugins and keep them up to date.\nVundle is one common option, but\nthere are some more to choose from. Two more big players in this business are\nNeoBundle and\nPathogen. I didn’t looked too deep\ninto the last two plugin managers. To be honest, I never tried out something\nelse than Vundle and I think the reason is that I never felt uncomfortable or\nmissed something. So I never felt in need of getting another plugin manager and\nin this post I want to show you why. First of all you need to have git\ninstalled. You can download git with the package manager of your distribution,\nif it isn’t installed already. You can easily check this by starting up your\nconsole and type in the following:\ngit --version \n\nIf git version &lt;some number&gt; appears you are good to go, but if something\nlike unknown command pops up, you are in a lot of trouble… Not really. You\njust need to install git with your package manager and everything will be fine.\nFor the most common Linux distributions, the command looks like one of these:\napt-get install git\n\nyum install git\n\npacman -S git\n\nWhen git is installed you download Vundle by executing the following command in\nyour console:\ngit clone github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim\n\nThis command downloads the Vundle code into bundle in your personal vim\ndirectory. The bundle directory will be the place where all your plugins are\nsaved. If you’re done, you should keep git on your computer, because vundle is\nworking with git commands internally. If it doesn’t find git, you aren’t able\nto use Vundle at all. Now we need to add some stuff to the .vimrc. The .vimrc\nis a file, where all your configurations in relation to Vim are placed. If you\ndidn’t edit anything there yet, have a look at your home directory (Make sure\nto also show invisible files). If there is no such file, you can create it now\nand it will be automatically loaded, when vim is starting the next time. Now we\nneed to initialize Vundle. To do so we place a code snippet at the beginning of\nthe .vimrc(It is very important to place the following snippet in front of your\nexisting configuration, if you already have written down some configurations).\nset nocompatible              &quot; be iMproved, required\nfiletype off                  &quot; required\n \n&quot; set the runtime path to include Vundle and initialize\nset rtp+=~/.vim/bundle/Vundle.vim\ncall vundle#begin()\n \n&quot; let Vundle manage Vundle, required\n \nPlugin &#039;VundleVim/Vundle.vim&#039;\n&quot;New plugins go here\n \ncall vundle#end()\nTo test if Vundle is working, we add a new plugin. As an example we install\nNERDTree, which will be covered in the next post of this series. Under the\ncomment “New plugins go here the plugin will be added. Because it is hosted\non GitHub we can write the following into the .vimrc\nPlugin &#039;scrooloose/nerdtree&#039;\n\nTo take this change into effect, you either need to restart Vim or re-source\nyour .vimrc. I’ve got the following line saved into my .vimrc, which\nautomatically re-sources my .vimrc everytime it is saved.\nautocmd! bufwritepost .vimrc source %\n\nAfter restarting/resourcing the .vimrc you enter the following command\n:PluginInstall\n\nA new split view is opened, which shows your plugins and either a dot or a star\nin front. The dot says that nothing was done, because the plugin is already\ninstalled and the star says that the new plugin was successfully installed. If\nanything goes wrong, you get the possibility to open up the log and look up the\nerror. To remove a plugin, you need to remove the corresponding line out of\nyour .vimrc. For example if we want to get rid of the NERDTree Plugin we remove\nthe line, which we just added and save the .vimrc (Don’t forget to\nre-source/restart). Afterwards type in the following command into Vim:\n:PluginClean\n\nA new split view opens with the plugins which will be removed. You can accept\nit by typing y or abort by typing _n. _ The final command I use on a regular\nbasis is to update my existing plugins.\n:PluginUpdate\n\nAgain a new split view opens and an arrow is moving through your installed\nplugins and marks them with a dot or a star, similar to the :PluginInstall\ncommand. The symbols also have the same meaning. Dots say that nothing needed\nto be done and the star tells you if a update was available and installed. If\nyou want to search for new plugins you can use the :PluginSearch command like\nthis:\n:PluginSearch nerdtree\n\nBut I often got problems doing it this way. So the normal way I search for new\nplugins is using my search engine of trust. I also prefer downloading every\nplugin from Github to keep this clean overview of Plugin\n‘creator/plugin_name’ in my .vimrc. But maybe a plugin isn’t available on\nGitHub(which was never the case for me), the second best source would be Vim\nScripts. To add a Vim Scripts link, you just need to\nadd the name of the plugin, which is even cleaner:\nPlugin &#039;nerdtree&#039;\n\nAnother option is to include a plugin from a non GitHub Hoster, who also uses\ngit, like Bitbucket. To add a plugin you need to start writing _git: _followed\nby the complete URL of the plugin. As an imaginary example, if NERDTree would\nbe hosted on Bitbucket, it would look like this:\nPlugin &#039;git:bitbucket.com/scrooloose/nerdtree&#039;\n\nI hope I could help with this brief overview of Vundle. If you need further\ninformation either head to the GitHub page of the Vundle\nproject or type the following command\ninto Vim:\n:h vundle\n"},"posts/Vim/The-power-of-Vim-Plugins-netrw":{"slug":"posts/Vim/The-power-of-Vim-Plugins-netrw","filePath":"posts/Vim/The power of Vim Plugins netrw.md","title":"The power of Vim Plugins: netrw","links":[],"tags":["netrw","plugins","vim"],"content":"This time we look into an alternative for NERDTree, which is called netrw.\nThanks to aguerosantiale, who put\nmy attention onto it. Both plugins serve the same purpose, but they behave\nfundamentaly different in archiving this. So the first question you will\nprobably ask is: “why use netrw, if I’m already familiar with NERDTree?“. The\nbiggest plus for netrw is, that you don’t have to download anything. If you\nset nocampatible in your .vimrc you are able to use it. So if you are on a\ndifferent machine, for example connected via ssh, you are able to use netrw.\nThe second point is, that vim is very reactive by nature, but if you have too\nmany plugins running at the same time, you could destroy this attribute. So\nit’s always good to use something, what is already there. Not everyone will\nlike netrw, because it isn’t the approach you know from Sublime Text or others.\nNetrw is a so called split explorer, while tree views like NERDTree are called\nproject drawer. To learn about the difference, we will talk about the project\ndrawer approach first. Project drawer open a new split view on the\nleft(sometimes right) and stays opened during your complete coding session.\nEvery time you click on an item in the project drawer, it opens the files in a\nseparate buffer.\n\nVim wasn’t supposed to work along with a feature like this. That’s why we had\nto do some tweaks in the last\npost, to make it work.\nEspecially if you are working with splits, you first must know in which split\nNERDTree opens your file. If you got that down, you need to learn, how to\nnavigate between them fast. If you want to open a file in a split, which is on\nthe opposite to the project drawer, you have to go a long way… That’s why we\ncreated a keybinding to toggle NERDTree instantly. Then we choose a file and\ngot thrown in the opened buffer immediately. To sum it up: that aren’t d many\nkey presses to get work done with NERDTree. So overall they provide a very good\nsolution for bringin in a principle, which wasn’t supposed to be in vim\ninitially. But what is different with the split explorer? A split explorer\nopens up in the same buffer. You then are able to choose a new file and this\nfile opens up in the same buffer again. It’s like sitting in front of a book\nand having the table of contents opened. You look up the page you want to see\nand switch to the according page. If you read the page, you can go back to the\ntable of contents and choose a new page and switch to it and so on and so\nforth. You are also able to open the explorer in another split view and choose\nwhich file to open from there.\n\nSo what’s the benefit from this? First of all: you look at the project\ndrawer very rarely(only if you want to change a file). So let’s say 5% of your\ntime coding is spent with the project explorer and the rest of the time you\nignore it. But 95% of the time it is still open and takes away space from your\nmonitor. So why not keep it closed as long as you are working on a file until\nyou need to switch? Another benefit is, that you always know where your buffer\nopens up. If you want to know more about project drawers and split explorers,\nhave a look at oil and\nvinegar,\nit’s great in-depth article about this topic. If you are a NERDTree user and\nyou are critical with the previous principle, you can test the split explorer\nby adding the following line to your .vimrc\nlet NERDTreeHijackNetrw=1\n\nClose your normal NERDTree with the :NERDTreeToggle command and type in :e\n. or create a mapping in your .vimrc. This is mine for example:\nnnoremap &lt;leader&gt;e :e .&lt;CR&gt;\n\nOf course you can take whatever binding you are comfortable with. Now NERDTree\nshould be opened in the buffer you are currently in and if you open a file,\nNERDTree will hide and your chosen file appears instead. You can experiment\nwith this setup now. If you feel more comfortable with this setup, like me, you\ncan also do the switch to netrw. You need to learn some new shortcuts for\nnetrw, but they are pretty intuitive in my opinion. If you don’t want to go\nthat way, you can stay with this NERDTree approach aswell and if you don’t feel\ncomfortable with the split explorer approach at all you can keep using NERDTree\nas it is. If you decided for netrw, I will get you going in the upcoming couple\nof paragraphs.\nConfiguration\nIf you removed NERDTree and all the according configuration\nfrom your .vimrc you can add the following line to your .vimrc.\nlet g:netrw_liststyle=3\n\nThis will open up netrw in tree like view and not a list. So it’s easier to\nrecognize which are contents of which subfolders. You can now press the\nshortcut defined above and you should see netrw. You are also able to open up\nnetrw in a vertical or horizontal split. I’ve created the following\nkeybindings:\n&quot;Open netrw in a vertical split\nnnoremap &lt;Leader&gt;v :vs .&lt;CR&gt;\n&quot;Open netrw in a horizontal split\nnnoremap &lt;Leader&gt;s :sp .&lt;CR&gt;\n\nI use these, when I want to compare two files or look something up from another\nfile. The banner at the top is pretty helpful in the beginning, but it only\ntook away space when I knew all the information. At this point I added the\nfollowing to my .vimrc\nlet g:netrw_banner=0\n\nThis will hide the banner and only shows the tree itself.\nNavigation\nLike in NERDTree, you are able to navigate with the j and k\nkeys. You can go to the top with gg and to the bottom with G. You are also\nable to search in the explorer buffer, to get to a specific entry fast(Those\noptions are also available in NERDTree, but I didn’t mentioned them in the last\npost). But jump up and down a directory(NERDTree: p and P) don’t exist from\nwhat I’ve found out so far. Files and folders are opened by pressing\n(Enter).\nBookmarking\nYou are able to bookmark directories, but no files. By pressing\nmb you create a bookmark. With qb you can open a quick fix window, which\nshows the history and bookmarked directories, but you aren’t able to choose\nanything here. With gb you should be able to jump to the last bookmark.\nBookmarking and everything project related, like changing the current work\ndirectory and minimizing the tree to the current directory are the biggest\ndownside with netrw, because they often seem to not work properly or are\nmissing completely.\nOpening files\nLike already mentioned, files can be opened by pressing\n(Enter). Then they open up in the same view. You can also use o to open\na file in a horizontal split or v to open in a vertical split. Additionally\nwe’ve set shortcuts in the beginning to open up a split explorer in another\nsplit.\nModifying Nodes in the file system\nIn NERDTree we needed to call a menu\nfirst and choose our operation afterwards, but in netrw there is a keybinding\nfor each of these operations, without the need to open a menu first. New files\ncan be created by pressing %. You need to type in the name afterwards and the\nfile will be opened automatically. By pressing d you create a new directory.\nD deletes the current node under your cursor, whatever it is a directory or\nfile. The last operation in this bulk is renaming, which is called with R.\nAll operations will be done relative from your current cursor position. But\nalso this has a downside. Creating new files don’t show up immediately.\nSometimes even a refresh didn’t help(C-l). So I had to close the sub\ndirectory and open it again, until it showed up.\nSummary\nI really like the split explorer approach, but for me netrw shows his\nweaknesses, if I want to do anything advanced. I think netrw is the perfect\nchoice, if you only rely on navigation and opening files. If you want to do\nmore, netrw falters. NERDTree also isn’t perfect to use with this approach,\nbecause every time it is opened, it is reset. This means that every opened\ndirectory is closed. So you have to navigate the whole way to a file again.\nThats why NERDTree is only a good choice, if you want to stay with the project\ndrawer approach. If you decided to stay with netrw and need further help, you\ncan type in\n:h netrw\n\nThis will show you the whole documentation of netrw, containing all it’s\nfeatures and configuration possibilities. During the writing of this article I\nalso am experimenting with vimfiler, which seems to combine the good things of\nboth plugins and adds a whole bunch of configuration possibilities on top of\nit. I will probably cover this plugin in a later post, if it fulfills my needs."},"posts/Vim/The-power-of-Vim-Plugins":{"slug":"posts/Vim/The-power-of-Vim-Plugins","filePath":"posts/Vim/The power of Vim Plugins.md","title":"The power of Vim Plugins","links":[],"tags":["intro","plugins","vim"],"content":"In this post I want to introduce a new series, which will cover some of the\nmost useful Vim plugins available. There are many guides and tutorials out\nthere about how Vim works and how great its modal editing is. So I won’t cover\nthese things in this series. I personally learned a lot from the book\nPractical Vim by Drew Neil. He also\npublished a lot of video tutorials, called VimCasts.\nBefore I started of with Vim, I was using Sublime Text and as I started using\nVim, I missed some essential knowledge about how to install, configure and use\nplugins. Part of the problem was, that everyone is telling you: “When you start\nof with Vim, learn it without any plugins first, so you get used to the modal\nediting.” Well this isn’t completely correct, because those are two different\npairs of shoes. Of course it’s more to learn at once, but many people don’t\nwant to learn Vim completely and then start installing plugins until they find\nout, that something they need very bad, isn’t available in Vim. So they wasted\nabout 2 months of their life until they can go on with the next tool on their\nlist. Luckily there is not much missing, to transform Vim into a good\nprogramming tool, in my opinion. For me it became my one and only code\nediting/writing tool for work, studying and hobby projects. By configuring\nplugins, people will discover another great thing: the .vimrc. It’s a file\nwhere all the plugins are linked and where Vim is configured, to really become\nyour personal editor. And because it’s just one file, it’s easy to put into a\nVersion Control System like GIT. This gives you the ability to have the same\nlook and feel of Vim on different machines in no time. It’s also the first time\nyou get in touch with VimL(VimScript). It’s a script language, which is used to\nconfigure, write plugins and interact with Vim in command mode. So it can be\nseen the same way as Python for Sublime and Coffeescript for Atom. To give you\na slight overview about what is coming next, I will list up a short\nsummary(non-chronologically) of what I want to cover in the next posts.\n\nVundle(The plugin manager of Vim)\nNERDTree(A tree like view, which shows the hierarchy of files and sub folders)\n CtrlP(A fuzzy search for files. Similar to Sublimes/Atoms Ctrl-P)\nYouCompleteMe(A heavy auto completion engine)\nVimAirline(A status bar which contains all the important infos about a file)\nThemes(This will be more of a general topic about how to install and use themes)\nTagbar(A Symbol View like list for code files)\nUltiSnips(Snippets for Vim)\nAg(A project wide search for pieces of texts)\n"}}